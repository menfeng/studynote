

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scrapy 1.2.0dev2 documentation</title>
  

  
  

  

  
  
    

  

  
  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="top" title="None" href="index.html#document-index"/> 

  
  <script src="_static/js/modernizr.min.js"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://doc.scrapy.org/en/latest/" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'index'
</script>

<script type="text/javascript" src="_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html#document-index" class="icon icon-home"> Scrapy
          

          
          </a>

          
            
            
            
              <div class="version">
                master
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">First steps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/overview">Scrapy at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/install">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/tutorial">Scrapy Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/examples">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/commands">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spiders">Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/selectors">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/items">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/loaders">Item Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/shell">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/item-pipeline">Item Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/feed-exports">Feed exports</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/request-response">Requests and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/link-extractors">Link Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/settings">Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exceptions">Exceptions</a></li>
</ul>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/logging">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/stats">Stats Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/email">Sending e-mail</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/telnetconsole">Telnet Console</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/webservice">Web Service</a></li>
</ul>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/debug">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/contracts">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/practices">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/broad-crawls">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firefox">Using Firefox for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/firebug">Using Firebug for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/leaks">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/media-pipeline">Downloading and processing files and images</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/ubuntu">Ubuntu packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/deploy">Deploying Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/autothrottle">AutoThrottle extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/jobs">Jobs: pausing and resuming crawls</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/architecture">Architecture overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/downloader-middleware">Downloader Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spider-middleware">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/api">Core API</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/signals">Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exporters">Item Exporters</a></li>
</ul>
<p class="caption"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-news">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-versioning">Versioning and API Stability</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html#document-index">Scrapy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html#document-index">Docs</a> &raquo;</li>
      
    <li>Scrapy 1.2.0dev2 documentation</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="https://github.com/scrapy/scrapy/blob/master/docs/index.rst" class="fa fa-github"> Edit on GitHub</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scrapy-version-documentation">
<span id="topics-index"></span><h1>Scrapy 1.2 documentation<a class="headerlink" href="#scrapy-version-documentation" title="Permalink to this headline">¶</a></h1>
<p>This documentation contains everything you need to know about Scrapy.</p>
<div class="section" id="getting-help">
<h2>Getting help<a class="headerlink" href="#getting-help" title="Permalink to this headline">¶</a></h2>
<p>Having trouble? We&#8217;d like to help!</p>
<ul class="simple">
<li>Try the <a class="reference internal" href="index.html#document-faq"><em>FAQ</em></a> &#8211; it&#8217;s got answers to some common questions.</li>
<li>Looking for specific information? Try the <a class="reference internal" href="genindex.html"><span>Index</span></a> or <a class="reference internal" href="py-modindex.html"><span>Module Index</span></a>.</li>
<li>Search for information in the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">archives of the scrapy-users mailing list</a>, or
<a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">post a question</a>.</li>
<li>Ask a question in the <a class="reference external" href="irc://irc.freenode.net/scrapy">#scrapy IRC channel</a>.</li>
<li>Report bugs with Scrapy in our <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>.</li>
</ul>
</div>
<div class="section" id="first-steps">
<h2>First steps<a class="headerlink" href="#first-steps" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id1">
<span id="document-intro/overview"></span><div class="section" id="scrapy-at-a-glance">
<span id="intro-overview"></span><h3>Scrapy at a glance<a class="headerlink" href="#scrapy-at-a-glance" title="Permalink to this headline">¶</a></h3>
<p>Scrapy is an application framework for crawling web sites and extracting
structured data which can be used for a wide range of useful applications, like
data mining, information processing or historical archival.</p>
<p>Even though Scrapy was originally designed for <a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web scraping</a>, it can also be
used to extract data using APIs (such as <a class="reference external" href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html">Amazon Associates Web Services</a>) or
as a general purpose web crawler.</p>
<div class="section" id="walk-through-of-an-example-spider">
<h4>Walk-through of an example spider<a class="headerlink" href="#walk-through-of-an-example-spider" title="Permalink to this headline">¶</a></h4>
<p>In order to show you what Scrapy brings to the table, we&#8217;ll walk you through an
example of a Scrapy Spider using the simplest way to run a spider.</p>
<p>So, here&#8217;s the code for a spider that follows the links to the top
voted questions on StackOverflow and scrapes some data from each page:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">StackOverflowSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;stackoverflow&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://stackoverflow.com/questions?sort=votes&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.question-summary h3 a::attr(href)&#39;</span><span class="p">):</span>
            <span class="n">full_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">full_url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_question</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_question</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;h1 a::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(),</span>
            <span class="s1">&#39;votes&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.question .vote-count-post::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(),</span>
            <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.question .post-text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(),</span>
            <span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.question .post-tag::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">(),</span>
            <span class="s1">&#39;link&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>Put this in a file, name it to something like <code class="docutils literal"><span class="pre">stackoverflow_spider.py</span></code>
and run the spider using the <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy runspider stackoverflow_spider.py -o top-stackoverflow-questions.json
</pre></div>
</div>
<p>When this finishes you will have in the <code class="docutils literal"><span class="pre">top-stackoverflow-questions.json</span></code> file
a list of the most upvoted questions in StackOverflow in JSON format, containing the
title, link, number of upvotes, a list of the tags and the question content in HTML,
looking like this (reformatted for easier reading):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[{
    &quot;body&quot;: &quot;... LONG HTML HERE ...&quot;,
    &quot;link&quot;: &quot;http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array&quot;,
    &quot;tags&quot;: [&quot;java&quot;, &quot;c++&quot;, &quot;performance&quot;, &quot;optimization&quot;],
    &quot;title&quot;: &quot;Why is processing a sorted array faster than an unsorted array?&quot;,
    &quot;votes&quot;: &quot;9924&quot;
},
{
    &quot;body&quot;: &quot;... LONG HTML HERE ...&quot;,
    &quot;link&quot;: &quot;http://stackoverflow.com/questions/1260748/how-do-i-remove-a-git-submodule&quot;,
    &quot;tags&quot;: [&quot;git&quot;, &quot;git-submodules&quot;],
    &quot;title&quot;: &quot;How do I remove a Git submodule?&quot;,
    &quot;votes&quot;: &quot;1764&quot;
},
...]
</pre></div>
</div>
<div class="section" id="what-just-happened">
<h5>What just happened?<a class="headerlink" href="#what-just-happened" title="Permalink to this headline">¶</a></h5>
<p>When you ran the command <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">somefile.py</span></code>, Scrapy looked for a
Spider definition inside it and ran it through its crawler engine.</p>
<p>The crawl started by making requests to the URLs defined in the <code class="docutils literal"><span class="pre">start_urls</span></code>
attribute (in this case, only the URL for StackOverflow top questions page)
and called the default callback method <code class="docutils literal"><span class="pre">parse</span></code>, passing the response object as
an argument. In the <code class="docutils literal"><span class="pre">parse</span></code> callback we extract the links to the
question pages using a CSS Selector with a custom extension that allows to get
the value for an attribute. Then we yield a few more requests to be sent,
registering the method <code class="docutils literal"><span class="pre">parse_question</span></code> as the callback to be called for each
of them as they finish.</p>
<p>Here you notice one of the main advantages about Scrapy: requests are
<a class="reference internal" href="index.html#topics-architecture"><span>scheduled and processed asynchronously</span></a>.  This
means that Scrapy doesn&#8217;t need to wait for a request to be finished and
processed, it can send another request or do other things in the meantime. This
also means that other requests can keep going even if some request fails or an
error happens while handling it.</p>
<p>While this enables you to do very fast crawls (sending multiple concurrent
requests at the same time, in a fault-tolerant way) Scrapy also gives you
control over the politeness of the crawl through <a class="reference internal" href="index.html#topics-settings-ref"><span>a few settings</span></a>. You can do things like setting a download delay between
each request, limiting amount of concurrent requests per domain or per IP, and
even <a class="reference internal" href="index.html#topics-autothrottle"><span>using an auto-throttling extension</span></a> that tries
to figure out these automatically.</p>
<p>Finally, the <code class="docutils literal"><span class="pre">parse_question</span></code> callback scrapes the question data for each
page yielding a dict, which Scrapy then collects and writes to a JSON file as
requested in the command line.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is using <a class="reference internal" href="index.html#topics-feed-exports"><span>feed exports</span></a> to generate the
JSON file, you can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>, for example).  You can also write an
<a class="reference internal" href="index.html#topics-item-pipeline"><span>item pipeline</span></a> to store the items in a database.</p>
</div>
</div>
</div>
<div class="section" id="what-else">
<span id="topics-whatelse"></span><h4>What else?<a class="headerlink" href="#what-else" title="Permalink to this headline">¶</a></h4>
<p>You&#8217;ve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:</p>
<ul>
<li><p class="first">Built-in support for <a class="reference internal" href="index.html#topics-selectors"><span>selecting and extracting</span></a> data
from HTML/XML sources using extended CSS selectors and XPath expressions,
with helper methods to extract using regular expressions.</p>
</li>
<li><p class="first">An <a class="reference internal" href="index.html#topics-shell"><span>interactive shell console</span></a> (IPython aware) for trying
out the CSS and XPath expressions to scrape data, very useful when writing or
debugging your spiders.</p>
</li>
<li><p class="first">Built-in support for <a class="reference internal" href="index.html#topics-feed-exports"><span>generating feed exports</span></a> in
multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,
S3, local filesystem)</p>
</li>
<li><p class="first">Robust encoding support and auto-detection, for dealing with foreign,
non-standard and broken encoding declarations.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#extending-scrapy"><span>Strong extensibility support</span></a>, allowing you to plug
in your own functionality using <a class="reference internal" href="index.html#topics-signals"><span>signals</span></a> and a
well-defined API (middlewares, <a class="reference internal" href="index.html#topics-extensions"><span>extensions</span></a>, and
<a class="reference internal" href="index.html#topics-item-pipeline"><span>pipelines</span></a>).</p>
</li>
<li><dl class="first docutils">
<dt>Wide range of built-in extensions and middlewares for handling:</dt>
<dd><ul class="first last simple">
<li>cookies and session handling</li>
<li>HTTP features like compression, authentication, caching</li>
<li>user-agent spoofing</li>
<li>robots.txt</li>
<li>crawl depth restriction</li>
<li>and more</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">A <a class="reference internal" href="index.html#topics-telnetconsole"><span>Telnet console</span></a> for hooking into a Python
console running inside your Scrapy process, to introspect and debug your
crawler</p>
</li>
<li><p class="first">Plus other goodies like reusable spiders to crawl sites from <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a> and
XML/CSV feeds, a media pipeline for <a class="reference internal" href="index.html#topics-media-pipeline"><span>automatically downloading images</span></a> (or any other media) associated with the scraped
items, a caching DNS resolver, and much more!</p>
</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h4>What&#8217;s next?<a class="headerlink" href="#what-s-next" title="Permalink to this headline">¶</a></h4>
<p>The next steps for you are to <a class="reference internal" href="index.html#intro-install"><span>install Scrapy</span></a>,
<a class="reference internal" href="index.html#intro-tutorial"><span>follow through the tutorial</span></a> to learn how to organize
your code in Scrapy projects and <a class="reference external" href="http://scrapy.org/community/">join the community</a>. Thanks for your
interest!</p>
</div>
</div>
<span id="document-intro/install"></span><div class="section" id="installation-guide">
<span id="intro-install"></span><h3>Installation guide<a class="headerlink" href="#installation-guide" title="Permalink to this headline">¶</a></h3>
<div class="section" id="installing-scrapy">
<h4>Installing Scrapy<a class="headerlink" href="#installing-scrapy" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Check <a class="reference internal" href="#intro-install-platform-notes"><span>Platform specific installation notes</span></a> first.</p>
</div>
<p>The installation steps assume that you have the following things installed:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.python.org/">Python</a> 2.7 or above 3.3</li>
<li><a class="reference external" href="https://pip.pypa.io/en/latest/installing/">pip</a> and <a class="reference external" href="https://pypi.python.org/pypi/setuptools">setuptools</a> Python packages. Nowadays <a class="reference external" href="https://pip.pypa.io/en/latest/installing/">pip</a> requires and
installs <a class="reference external" href="https://pypi.python.org/pypi/setuptools">setuptools</a> if not installed. Python 2.7.9 and later include
<a class="reference external" href="https://pip.pypa.io/en/latest/installing/">pip</a> by default, so you may have it already.</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a>. Most Linux distributions ships prepackaged versions of lxml.
Otherwise refer to <a class="reference external" href="http://lxml.de/installation.html">http://lxml.de/installation.html</a></li>
<li><a class="reference external" href="https://pypi.python.org/pypi/pyOpenSSL">OpenSSL</a>. This comes preinstalled in all operating systems, except Windows
where the Python installer ships it bundled.</li>
</ul>
<p>You can install Scrapy using pip (which is the canonical way to install Python
packages). To install using <code class="docutils literal"><span class="pre">pip</span></code> run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>pip install Scrapy
</pre></div>
</div>
</div>
<div class="section" id="platform-specific-installation-notes">
<span id="intro-install-platform-notes"></span><h4>Platform specific installation notes<a class="headerlink" href="#platform-specific-installation-notes" title="Permalink to this headline">¶</a></h4>
<div class="section" id="anaconda">
<h5>Anaconda<a class="headerlink" href="#anaconda" title="Permalink to this headline">¶</a></h5>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For Windows users, or if you have issues installing through <cite>pip</cite>, this is
the recommended way to install Scrapy.</p>
</div>
<p>If you already have installed <a class="reference external" href="http://docs.continuum.io/anaconda/index">Anaconda</a> or <a class="reference external" href="http://conda.pydata.org/docs/install/quick.html">Miniconda</a>, the company
<a class="reference external" href="http://scrapinghub.com">Scrapinghub</a> maintains official conda packages for Linux, Windows and OS X.</p>
<p>To install Scrapy using <code class="docutils literal"><span class="pre">conda</span></code>, run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>conda install -c scrapinghub scrapy
</pre></div>
</div>
</div>
<div class="section" id="windows">
<h5>Windows<a class="headerlink" href="#windows" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">Install Python 2.7 from <a class="reference external" href="https://www.python.org/downloads/">https://www.python.org/downloads/</a></p>
<p>You need to adjust <code class="docutils literal"><span class="pre">PATH</span></code> environment variable to include paths to
the Python executable and additional scripts. The following paths need to be
added to <code class="docutils literal"><span class="pre">PATH</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>C:\Python27\;C:\Python27\Scripts\;
</pre></div>
</div>
<p>To update the <code class="docutils literal"><span class="pre">PATH</span></code> open a Command prompt and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>c:\python27\python.exe c:\python27\tools\scripts\win_add2path.py
</pre></div>
</div>
<p>Close the command prompt window and reopen it so changes take effect, run the
following command and check it shows the expected Python version:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">--</span><span class="n">version</span>
</pre></div>
</div>
</li>
<li><p class="first">Install <cite>pywin32</cite> from <a class="reference external" href="http://sourceforge.net/projects/pywin32/">http://sourceforge.net/projects/pywin32/</a></p>
<p>Be sure you download the architecture (win32 or amd64) that matches your system</p>
</li>
<li><p class="first"><em>(Only required for Python&lt;2.7.9)</em> Install <a class="reference external" href="https://pip.pypa.io/en/latest/installing/">pip</a> from
<a class="reference external" href="https://pip.pypa.io/en/latest/installing/">https://pip.pypa.io/en/latest/installing/</a></p>
<p>Now open a Command prompt to check <code class="docutils literal"><span class="pre">pip</span></code> is installed correctly:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="o">--</span><span class="n">version</span>
</pre></div>
</div>
</li>
<li><p class="first">At this point Python 2.7 and <code class="docutils literal"><span class="pre">pip</span></code> package manager must be working, let&#8217;s
install Scrapy:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>pip install Scrapy
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Python 3 is not supported on Windows. This is because Scrapy core requirement Twisted does not support
Python 3 on Windows.</p>
</div>
</div>
<div class="section" id="ubuntu-9-10-or-above">
<h5>Ubuntu 9.10 or above<a class="headerlink" href="#ubuntu-9-10-or-above" title="Permalink to this headline">¶</a></h5>
<p><strong>Don&#8217;t</strong> use the <code class="docutils literal"><span class="pre">python-scrapy</span></code> package provided by Ubuntu, they are
typically too old and slow to catch up with latest Scrapy.</p>
<p>Instead, use the official <a class="reference internal" href="index.html#topics-ubuntu"><span>Ubuntu Packages</span></a>, which already
solve all dependencies for you and are continuously updated with the latest bug
fixes.</p>
<p>If you prefer to build the python dependencies locally instead of relying on
system packages you&#8217;ll need to install their required non-python dependencies
first:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
</pre></div>
</div>
<p>You can install Scrapy with <code class="docutils literal"><span class="pre">pip</span></code> after that:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>pip install Scrapy
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The same non-python dependencies can be used to install Scrapy in Debian
Wheezy (7.0) and above.</p>
</div>
</div>
<div class="section" id="archlinux">
<h5>Archlinux<a class="headerlink" href="#archlinux" title="Permalink to this headline">¶</a></h5>
<p>You can follow the generic instructions or install Scrapy from <cite>AUR Scrapy package</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>yaourt -S scrapy
</pre></div>
</div>
</div>
<div class="section" id="mac-os-x">
<h5>Mac OS X<a class="headerlink" href="#mac-os-x" title="Permalink to this headline">¶</a></h5>
<p>Building Scrapy&#8217;s dependencies requires the presence of a C compiler and
development headers. On OS X this is typically provided by Apple’s Xcode
development tools. To install the Xcode command line tools open a terminal
window and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">xcode</span><span class="o">-</span><span class="n">select</span> <span class="o">--</span><span class="n">install</span>
</pre></div>
</div>
<p>There&#8217;s a <a class="reference external" href="https://github.com/pypa/pip/issues/2468">known issue</a> that
prevents <code class="docutils literal"><span class="pre">pip</span></code> from updating system packages. This has to be addressed to
successfully install Scrapy and its dependencies. Here are some proposed
solutions:</p>
<ul>
<li><p class="first"><em>(Recommended)</em> <strong>Don&#8217;t</strong> use system python, install a new, updated version
that doesn&#8217;t conflict with the rest of your system. Here&#8217;s how to do it using
the <a class="reference external" href="http://brew.sh/">homebrew</a> package manager:</p>
<ul>
<li><p class="first">Install <a class="reference external" href="http://brew.sh/">homebrew</a> following the instructions in <a class="reference external" href="http://brew.sh/">http://brew.sh/</a></p>
</li>
<li><p class="first">Update your <code class="docutils literal"><span class="pre">PATH</span></code> variable to state that homebrew packages should be
used before system packages (Change <code class="docutils literal"><span class="pre">.bashrc</span></code> to <code class="docutils literal"><span class="pre">.zshrc</span></code> accordantly
if you&#8217;re using <a class="reference external" href="http://www.zsh.org/">zsh</a> as default shell):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>echo &quot;export PATH=/usr/local/bin:/usr/local/sbin:$PATH&quot; &gt;&gt; ~/.bashrc
</pre></div>
</div>
</li>
<li><p class="first">Reload <code class="docutils literal"><span class="pre">.bashrc</span></code> to ensure the changes have taken place:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>source ~/.bashrc
</pre></div>
</div>
</li>
<li><p class="first">Install python:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>brew install python
</pre></div>
</div>
</li>
<li><p class="first">Latest versions of python have <code class="docutils literal"><span class="pre">pip</span></code> bundled with them so you won&#8217;t need
to install it separately. If this is not the case, upgrade python:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>brew update; brew upgrade python
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first"><em>(Optional)</em> Install Scrapy inside an isolated python environment.</p>
<p>This method is a workaround for the above OS X issue, but it&#8217;s an overall
good practice for managing dependencies and can complement the first method.</p>
<p><a class="reference external" href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> is a tool you can use to create virtual environments in python.
We recommended reading a tutorial like
<a class="reference external" href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a> to get started.</p>
</li>
</ul>
<p>After any of these workarounds you should be able to install Scrapy:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>pip install Scrapy
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-intro/tutorial"></span><div class="section" id="scrapy-tutorial">
<span id="intro-tutorial"></span><h3>Scrapy Tutorial<a class="headerlink" href="#scrapy-tutorial" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we&#8217;ll assume that Scrapy is already installed on your system.
If that&#8217;s not the case, see <a class="reference internal" href="index.html#intro-install"><span>Installation guide</span></a>.</p>
<p>We are going to use <a class="reference external" href="https://www.dmoz.org/">Open directory project (dmoz)</a> as
our example domain to scrape.</p>
<p>This tutorial will walk you through these tasks:</p>
<ol class="arabic simple">
<li>Creating a new Scrapy project</li>
<li>Defining the Items you will extract</li>
<li>Writing a <a class="reference internal" href="index.html#topics-spiders"><span>spider</span></a> to crawl a site and extract
<a class="reference internal" href="index.html#topics-items"><span>Items</span></a></li>
<li>Writing an <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a> to store the
extracted Items</li>
</ol>
<p>Scrapy is written in <a class="reference external" href="https://www.python.org/">Python</a>. If you&#8217;re new to the language you might want to
start by getting an idea of what the language is like, to get the most out of
Scrapy.  If you&#8217;re already familiar with other languages, and want to learn
Python quickly, we recommend <a class="reference external" href="http://learnpythonthehardway.org/book/">Learn Python The Hard Way</a>.  If you&#8217;re new to programming
and want to start with Python, take a look at <a class="reference external" href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this list of Python resources
for non-programmers</a>.</p>
<div class="section" id="creating-a-project">
<h4>Creating a project<a class="headerlink" href="#creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Before you start scraping, you will have to set up a new Scrapy project. Enter a
directory where you&#8217;d like to store your code and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy startproject tutorial
</pre></div>
</div>
<p>This will create a <code class="docutils literal"><span class="pre">tutorial</span></code> directory with the following contents:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project&#39;s Python module, you&#39;ll import your code from here
        __init__.py

        items.py          # project items file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you&#39;ll later put your spiders
            __init__.py
            ...
</pre></div>
</div>
</div>
<div class="section" id="defining-our-item">
<h4>Defining our Item<a class="headerlink" href="#defining-our-item" title="Permalink to this headline">¶</a></h4>
<p><cite>Items</cite> are containers that will be loaded with the scraped data; they work
like simple Python dicts. While you can use plain Python dicts with Scrapy,
<cite>Items</cite> provide additional protection against populating undeclared fields,
preventing typos. They can also be used with <a class="reference internal" href="index.html#topics-loaders"><span>Item Loaders</span></a>, a mechanism with helpers to conveniently populate <cite>Items</cite>.</p>
<p>They are declared by creating a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Item</span></code></a> class and defining
its attributes as <a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Field</span></code></a> objects, much like in an ORM
(don&#8217;t worry if you&#8217;re not familiar with ORMs, you will see that this is an
easy task).</p>
<p>We begin by modeling the item that we will use to hold the site&#8217;s data obtained
from dmoz.org. As we want to capture the name, url and description of the
sites, we define fields for each of these three attributes. To do that, we edit
<code class="docutils literal"><span class="pre">items.py</span></code>, found in the <code class="docutils literal"><span class="pre">tutorial</span></code> directory. Our Item class looks like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>This may seem complicated at first, but defining an item class allows you to use other handy
components and helpers within Scrapy.</p>
</div>
<div class="section" id="our-first-spider">
<h4>Our first Spider<a class="headerlink" href="#our-first-spider" title="Permalink to this headline">¶</a></h4>
<p>Spiders are classes that you define and Scrapy uses to scrape information from a
domain (or group of domains).</p>
<p>They define an initial list of URLs to download, how to follow links, and how
to parse the contents of pages to extract <a class="reference internal" href="index.html#topics-items"><span>items</span></a>.</p>
<p>To create a Spider, you must subclass <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Spider</span></code></a> and define some attributes:</p>
<ul>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal"><span class="pre">name</span></code></a>: identifies the Spider. It must be
unique, that is, you can&#8217;t set the same name for different Spiders.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a>: a list of URLs where the
Spider will begin to crawl from.  The first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a>: a method of the spider, which will
be called with the downloaded <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object of each
start URL. The response is passed to the method as the first and only
argument.</p>
<p>This method is responsible for parsing the response data and extracting
scraped data (as scraped items) and more URLs to follow.</p>
<p>The <a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> method is in charge of processing
the response and returning scraped data (as <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>
objects) and more URLs to follow (as <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects).</p>
</li>
</ul>
<p>This is the code for our first Spider; save it in a file named
<code class="docutils literal"><span class="pre">dmoz_spider.py</span></code> under the <code class="docutils literal"><span class="pre">tutorial/spiders</span></code> directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;.html&#39;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="crawling">
<h5>Crawling<a class="headerlink" href="#crawling" title="Permalink to this headline">¶</a></h5>
<p>To put our spider to work, go to the project&#8217;s top level directory and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy crawl dmoz
</pre></div>
</div>
<p>This command runs the spider with name <code class="docutils literal"><span class="pre">dmoz</span></code> that we&#8217;ve just added, that
will send some requests for the <code class="docutils literal"><span class="pre">dmoz.org</span></code> domain. You will get an output
similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)
2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...
2014-01-23 18:13:07-0400 [scrapy] INFO: Spider opened
2014-01-23 18:13:08-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&gt; (referer: None)
2014-01-23 18:13:09-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
2014-01-23 18:13:09-0400 [scrapy] INFO: Closing spider (finished)
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">At the end you can see a log line for each URL defined in <code class="docutils literal"><span class="pre">start_urls</span></code>.
Because these URLs are the starting ones, they have no referrers, which is
shown at the end of the log line, where it says <code class="docutils literal"><span class="pre">(referer:</span> <span class="pre">None)</span></code>.</p>
</div>
<p>Now, check the files in the current directory. You should notice two new files
have been created: <em>Books.html</em> and <em>Resources.html</em>, with the content for the respective
URLs, as our <code class="docutils literal"><span class="pre">parse</span></code> method instructs.</p>
<div class="section" id="what-just-happened-under-the-hood">
<h6>What just happened under the hood?<a class="headerlink" href="#what-just-happened-under-the-hood" title="Permalink to this headline">¶</a></h6>
<p>Scrapy creates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Request</span></code></a> objects
for each URL in the <code class="docutils literal"><span class="pre">start_urls</span></code> attribute of the Spider, and assigns
them the <code class="docutils literal"><span class="pre">parse</span></code> method of the spider as their callback function.</p>
<p>These Requests are scheduled, then executed, and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></code></a>
objects are returned and then fed back to the spider, through the
<a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> method.</p>
</div>
</div>
<div class="section" id="extracting-items">
<h5>Extracting Items<a class="headerlink" href="#extracting-items" title="Permalink to this headline">¶</a></h5>
<div class="section" id="introduction-to-selectors">
<h6>Introduction to Selectors<a class="headerlink" href="#introduction-to-selectors" title="Permalink to this headline">¶</a></h6>
<p>There are several ways to extract data from web pages. Scrapy uses a mechanism
based on <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> expressions called <a class="reference internal" href="index.html#topics-selectors"><span>Scrapy Selectors</span></a>.  For more information about selectors and other extraction
mechanisms see the <a class="reference internal" href="index.html#topics-selectors"><span>Selectors documentation</span></a>.</p>
<p>Here are some examples of XPath expressions and their meanings:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">/html/head/title</span></code>: selects the <code class="docutils literal"><span class="pre">&lt;title&gt;</span></code> element, inside the <code class="docutils literal"><span class="pre">&lt;head&gt;</span></code>
element of an HTML document</li>
<li><code class="docutils literal"><span class="pre">/html/head/title/text()</span></code>: selects the text inside the aforementioned
<code class="docutils literal"><span class="pre">&lt;title&gt;</span></code> element.</li>
<li><code class="docutils literal"><span class="pre">//td</span></code>: selects all the <code class="docutils literal"><span class="pre">&lt;td&gt;</span></code> elements</li>
<li><code class="docutils literal"><span class="pre">//div[&#64;class=&quot;mine&quot;]</span></code>: selects all <code class="docutils literal"><span class="pre">div</span></code> elements which contain an
attribute <code class="docutils literal"><span class="pre">class=&quot;mine&quot;</span></code></li>
</ul>
<p>These are just a couple of simple examples of what you can do with XPath, but
XPath expressions are indeed much more powerful. To learn more about XPath, we
recommend <a class="reference external" href="http://zvon.org/comp/r/tut-XPath_1.html">this tutorial to learn XPath through examples</a>, and <a class="reference external" href="http://plasmasturm.org/log/xpath101/">this tutorial to learn &#8220;how
to think in XPath&#8221;</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>CSS vs XPath:</strong> you can go a long way extracting data from web pages
using only CSS selectors. However, XPath offers more power because besides
navigating the structure, it can also look at the content: you&#8217;re
able to select things like: <em>the link that contains the text &#8216;Next Page&#8217;</em>.
Because of this, we encourage you to learn about XPath even if you
already know how to construct CSS selectors.</p>
</div>
<p>For working with CSS and XPath expressions, Scrapy provides
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> class and convenient shortcuts to avoid
instantiating selectors yourself every time you need to select something from a
response.</p>
<p>You can see selectors as objects that represent nodes in the document
structure. So, the first instantiated selectors are associated with the root
node, or the entire document.</p>
<p>Selectors have four basic methods (click on the method to see the complete API
documentation):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><code class="xref py py-meth docutils literal"><span class="pre">xpath()</span></code></a>: returns a list of selectors, each of
which represents the nodes selected by the xpath expression given as
argument.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><code class="xref py py-meth docutils literal"><span class="pre">css()</span></code></a>: returns a list of selectors, each of
which represents the nodes selected by the CSS expression given as argument.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><code class="xref py py-meth docutils literal"><span class="pre">extract()</span></code></a>: returns a unicode string with the
selected data.</li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector.re" title="scrapy.selector.Selector.re"><code class="xref py py-meth docutils literal"><span class="pre">re()</span></code></a>: returns a list of unicode strings
extracted by applying the regular expression given as argument.</li>
</ul>
</div>
<div class="section" id="trying-selectors-in-the-shell">
<h6>Trying Selectors in the Shell<a class="headerlink" href="#trying-selectors-in-the-shell" title="Permalink to this headline">¶</a></h6>
<p>To illustrate the use of Selectors we&#8217;re going to use the built-in <a class="reference internal" href="index.html#topics-shell"><span>Scrapy
shell</span></a>, which also requires <a class="reference external" href="http://ipython.org/">IPython</a> (an extended Python console)
installed on your system.</p>
<p>To start a shell, you must go to the project&#8217;s top level directory and run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy shell &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Remember to always enclose urls in quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (ie. <code class="docutils literal"><span class="pre">&amp;</span></code> character)
will not work.</p>
</div>
<p>This is what the shell looks like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[ ... Scrapy log here ... ]

2014-01-23 17:11:42-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x3636b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   response   &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x3fadc50&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x3cebf50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

In [1]:
</pre></div>
</div>
<p>After the shell loads, you will have the response fetched in a local
<code class="docutils literal"><span class="pre">response</span></code> variable, so if you type <code class="docutils literal"><span class="pre">response.body</span></code> you will see the body
of the response, or you can type <code class="docutils literal"><span class="pre">response.headers</span></code> to see its headers.</p>
<p>More importantly <code class="docutils literal"><span class="pre">response</span></code> has a <code class="docutils literal"><span class="pre">selector</span></code> attribute which is an instance of
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> class, instantiated with this particular <code class="docutils literal"><span class="pre">response</span></code>.
You can run queries on <code class="docutils literal"><span class="pre">response</span></code> by calling <code class="docutils literal"><span class="pre">response.selector.xpath()</span></code> or
<code class="docutils literal"><span class="pre">response.selector.css()</span></code>. There are also some convenience shortcuts like <code class="docutils literal"><span class="pre">response.xpath()</span></code>
or <code class="docutils literal"><span class="pre">response.css()</span></code> which map directly to <code class="docutils literal"><span class="pre">response.selector.xpath()</span></code> and
<code class="docutils literal"><span class="pre">response.selector.css()</span></code>.</p>
<p>So let&#8217;s try it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>In [1]: response.xpath(&#39;//title&#39;)
Out[1]: [&lt;Selector xpath=&#39;//title&#39; data=u&#39;&lt;title&gt;Open Directory - Computers: Progr&#39;&gt;]

In [2]: response.xpath(&#39;//title&#39;).extract()
Out[2]: [u&#39;&lt;title&gt;Open Directory - Computers: Programming: Languages: Python: Books&lt;/title&gt;&#39;]

In [3]: response.xpath(&#39;//title/text()&#39;)
Out[3]: [&lt;Selector xpath=&#39;//title/text()&#39; data=u&#39;Open Directory - Computers: Programming:&#39;&gt;]

In [4]: response.xpath(&#39;//title/text()&#39;).extract()
Out[4]: [u&#39;Open Directory - Computers: Programming: Languages: Python: Books&#39;]

In [5]: response.xpath(&#39;//title/text()&#39;).re(&#39;(\w+):&#39;)
Out[5]: [u&#39;Computers&#39;, u&#39;Programming&#39;, u&#39;Languages&#39;, u&#39;Python&#39;]
</pre></div>
</div>
</div>
<div class="section" id="extracting-the-data">
<h6>Extracting the data<a class="headerlink" href="#extracting-the-data" title="Permalink to this headline">¶</a></h6>
<p>Now, let&#8217;s try to extract some real information from those pages.</p>
<p>You could type <code class="docutils literal"><span class="pre">response.body</span></code> in the console, and inspect the source code to
figure out the XPaths you need to use. However, inspecting the raw HTML code
there could become a very tedious task. To make it easier, you can
use Firefox Developer Tools or some Firefox extensions like Firebug. For more
information see <a class="reference internal" href="index.html#topics-firebug"><span>Using Firebug for scraping</span></a> and <a class="reference internal" href="index.html#topics-firefox"><span>Using Firefox for scraping</span></a>.</p>
<p>After inspecting the page source, you&#8217;ll find that the web site&#8217;s information
is inside a <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code> element, in fact the <em>second</em> <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code> element.</p>
<p>So we can select each <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code> element belonging to the site&#8217;s list with this
code:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>And from them, the site&#8217;s descriptions:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>The site&#8217;s titles:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>And the site&#8217;s links:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li/a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>As we&#8217;ve said before, each <code class="docutils literal"><span class="pre">.xpath()</span></code> call returns a list of selectors, so we can
concatenate further <code class="docutils literal"><span class="pre">.xpath()</span></code> calls to dig deeper into a node. We are going to use
that property here, so:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">desc</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For a more detailed description of using nested selectors, see
<a class="reference internal" href="index.html#topics-selectors-nesting-selectors"><span>Nesting selectors</span></a> and
<a class="reference internal" href="index.html#topics-selectors-relative-xpaths"><span>Working with relative XPaths</span></a> in the <a class="reference internal" href="index.html#topics-selectors"><span>Selectors</span></a>
documentation</p>
</div>
<p>Let&#8217;s add this code to our spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">link</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">desc</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">print</span> <span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">desc</span>
</pre></div>
</div>
<p>Now try crawling dmoz.org again and you&#8217;ll see sites being printed
in your output. Run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy crawl dmoz
</pre></div>
</div>
</div>
</div>
<div class="section" id="using-our-item">
<h5>Using our item<a class="headerlink" href="#using-our-item" title="Permalink to this headline">¶</a></h5>
<p><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects are custom Python dicts; you can access the
values of their fields (attributes of the class we defined earlier) using the
standard dict syntax like:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;Example title&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span>
<span class="go">&#39;Example title&#39;</span>
</pre></div>
</div>
<p>So, in order to return the data we&#8217;ve scraped so far, the final code for our
Spider would be like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">tutorial.items</span> <span class="kn">import</span> <span class="n">DmozItem</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;link&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can find a fully-functional variant of this spider in the <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a>
project available at <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
</div>
<p>Now crawling dmoz.org yields <code class="docutils literal"><span class="pre">DmozItem</span></code> objects:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[scrapy] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By David Mertz; Addison Wesley. Book in progress, full text, ASCII format. Asks for feedback. [author website, Gnosis Software, Inc.\n],
      &#39;link&#39;: [u&#39;http://gnosis.cx/TPiP/&#39;],
      &#39;title&#39;: [u&#39;Text Processing in Python&#39;]}
[scrapy] DEBUG: Scraped from &lt;200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&gt;
     {&#39;desc&#39;: [u&#39; - By Sean McGrath; Prentice Hall PTR, 2000, ISBN 0130211192, has CD-ROM. Methods to build XML applications fast, Python tutorial, DOM and SAX, new Pyxie open source XML processing library. [Prentice Hall PTR]\n&#39;],
      &#39;link&#39;: [u&#39;http://www.informit.com/store/product.aspx?isbn=0130211192&#39;],
      &#39;title&#39;: [u&#39;XML Processing with Python&#39;]}
</pre></div>
</div>
</div>
</div>
<div class="section" id="following-links">
<h4>Following links<a class="headerlink" href="#following-links" title="Permalink to this headline">¶</a></h4>
<p>Let&#8217;s say, instead of just scraping the stuff in <em>Books</em> and <em>Resources</em> pages,
you want everything that is under the <a class="reference external" href="http://www.dmoz.org/Computers/Programming/Languages/Python/">Python directory</a>.</p>
<p>Now that you know how to extract data from a page, why not extract the links
for the pages you are interested, follow them and then extract the data you
want for all of them?</p>
<p>Here is a modification to our spider that does just that:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">tutorial.items</span> <span class="kn">import</span> <span class="n">DmozItem</span>

<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;ul.directory.dir-col &gt; li &gt; a::attr(&#39;href&#39;)&quot;</span><span class="p">):</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_dir_contents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_dir_contents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">sel</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//ul/li&#39;</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">DmozItem</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;link&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="k">yield</span> <span class="n">item</span>
</pre></div>
</div>
<p>Now the <cite>parse()</cite> method only extracts the interesting links from the page,
builds a full absolute URL using the <cite>response.urljoin</cite> method (since the links can
be relative) and yields new requests to be sent later, registering as callback
the method <cite>parse_dir_contents()</cite> that will ultimately scrape the data we want.</p>
<p>What you see here is Scrapy&#8217;s mechanism of following links: when you yield
a Request in a callback method, Scrapy will schedule that request to be sent
and register a callback method to be executed when that request finishes.</p>
<p>Using this, you can build complex crawlers that follow links according to rules
you define, and extract different kinds of data depending on the page it&#8217;s
visiting.</p>
<p>A common pattern is a callback method that extracts some items, looks for a link
to follow to the next page and then yields a <cite>Request</cite> with the same callback
for it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_articles_follow_next_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">article</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//article&quot;</span><span class="p">):</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">ArticleItem</span><span class="p">()</span>

        <span class="o">...</span> <span class="n">extract</span> <span class="n">article</span> <span class="n">data</span> <span class="n">here</span>

        <span class="k">yield</span> <span class="n">item</span>

    <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;ul.navigation &gt; li.next-page &gt; a::attr(&#39;href&#39;)&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">next_page</span><span class="p">:</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_page</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_articles_follow_next_page</span><span class="p">)</span>
</pre></div>
</div>
<p>This creates a sort of loop, following all the links to the next page until it
doesn&#8217;t find one &#8211; handy for crawling blogs, forums and other sites with
pagination.</p>
<p>Another common pattern is to build an item with data from more than one page,
using a <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span>trick to pass additional data to the callbacks</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">As an example spider that leverages this mechanism, check out the
<a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> class for a generic spider
that implements a small rules engine that you can use to write your
crawlers on top of it.</p>
</div>
</div>
<div class="section" id="storing-the-scraped-data">
<h4>Storing the scraped data<a class="headerlink" href="#storing-the-scraped-data" title="Permalink to this headline">¶</a></h4>
<p>The simplest way to store the scraped data is by using <a class="reference internal" href="index.html#topics-feed-exports"><span>Feed exports</span></a>, with the following command:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy crawl dmoz -o items.json
</pre></div>
</div>
<p>That will generate an <code class="docutils literal"><span class="pre">items.json</span></code> file containing all scraped items,
serialized in <a class="reference external" href="https://en.wikipedia.org/wiki/JSON">JSON</a>.</p>
<p>In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>. As with Items, a
placeholder file for Item Pipelines has been set up for you when the project is
created, in <code class="docutils literal"><span class="pre">tutorial/pipelines.py</span></code>. Though you don&#8217;t need to implement any item
pipelines if you just want to store the scraped items.</p>
</div>
<div class="section" id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">¶</a></h4>
<p>This tutorial covered only the basics of Scrapy, but there&#8217;s a lot of other
features not mentioned here. Check the <a class="reference internal" href="index.html#topics-whatelse"><span>What else?</span></a> section in
<a class="reference internal" href="index.html#intro-overview"><span>Scrapy at a glance</span></a> chapter for a quick overview of the most important ones.</p>
<p>Then, we recommend you continue by playing with an example project (see
<a class="reference internal" href="index.html#intro-examples"><span>Examples</span></a>), and then continue with the section
<a class="reference internal" href="index.html#section-basics"><span>Basic concepts</span></a>.</p>
</div>
</div>
<span id="document-intro/examples"></span><div class="section" id="examples">
<span id="intro-examples"></span><h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a>, that you can use to
play and learn more about Scrapy. It contains the dmoz spider described in the
tutorial.</p>
<p>This <a class="reference external" href="https://github.com/scrapy/dirbot">dirbot</a> project is available at: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
<p>It contains a README file with a detailed description of the project contents.</p>
<p>If you&#8217;re familiar with git, you can checkout the code. Otherwise you can
download a tarball or zip file of the project by clicking on <a class="reference external" href="https://github.com/scrapy/dirbot/downloads">Downloads</a>.</p>
<p>The <a class="reference external" href="http://snipplr.com/all/tags/scrapy/">scrapy tag on Snipplr</a> is used for sharing code snippets such as spiders,
middlewares, extensions, or scripts. Feel free (and encouraged!) to share any
code there.</p>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-intro/overview"><em>Scrapy at a glance</em></a></dt>
<dd>Understand what Scrapy is and how it can help you.</dd>
<dt><a class="reference internal" href="index.html#document-intro/install"><em>Installation guide</em></a></dt>
<dd>Get Scrapy installed on your computer.</dd>
<dt><a class="reference internal" href="index.html#document-intro/tutorial"><em>Scrapy Tutorial</em></a></dt>
<dd>Write your first Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-intro/examples"><em>Examples</em></a></dt>
<dd>Learn more by playing with a pre-made Scrapy project.</dd>
</dl>
</div>
<div class="section" id="basic-concepts">
<span id="section-basics"></span><h2>Basic concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id2">
<span id="document-topics/commands"></span><div class="section" id="command-line-tool">
<span id="topics-commands"></span><h3>Command line tool<a class="headerlink" href="#command-line-tool" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Scrapy is controlled through the <code class="docutils literal"><span class="pre">scrapy</span></code> command-line tool, to be referred
here as the &#8220;Scrapy tool&#8221; to differentiate it from the sub-commands, which we
just call &#8220;commands&#8221; or &#8220;Scrapy commands&#8221;.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.</p>
<p>(The <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">deploy</span></code> command has been removed in 1.0 in favor of the
standalone <code class="docutils literal"><span class="pre">scrapyd-deploy</span></code>. See <a class="reference external" href="http://scrapyd.readthedocs.org/en/latest/deploy.html">Deploying your project</a>.)</p>
<div class="section" id="configuration-settings">
<span id="topics-config-settings"></span><h4>Configuration settings<a class="headerlink" href="#configuration-settings" title="Permalink to this headline">¶</a></h4>
<p>Scrapy will look for configuration parameters in ini-style <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> files
in standard locations:</p>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">/etc/scrapy.cfg</span></code> or <code class="docutils literal"><span class="pre">c:\scrapy\scrapy.cfg</span></code> (system-wide),</li>
<li><code class="docutils literal"><span class="pre">~/.config/scrapy.cfg</span></code> (<code class="docutils literal"><span class="pre">$XDG_CONFIG_HOME</span></code>) and <code class="docutils literal"><span class="pre">~/.scrapy.cfg</span></code> (<code class="docutils literal"><span class="pre">$HOME</span></code>)
for global (user-wide) settings, and</li>
<li><code class="docutils literal"><span class="pre">scrapy.cfg</span></code> inside a scrapy project&#8217;s root (see next section).</li>
</ol>
<p>Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults
and project-wide settings will override all others, when defined.</p>
<p>Scrapy also understands, and can be configured through, a number of environment
variables. Currently these are:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> (see <a class="reference internal" href="index.html#topics-settings-module-envvar"><span>Designating the settings</span></a>)</li>
<li><code class="docutils literal"><span class="pre">SCRAPY_PROJECT</span></code></li>
<li><code class="docutils literal"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (see <a class="reference internal" href="index.html#topics-shell"><span>Scrapy shell</span></a>)</li>
</ul>
</div>
<div class="section" id="default-structure-of-scrapy-projects">
<span id="topics-project-structure"></span><h4>Default structure of Scrapy projects<a class="headerlink" href="#default-structure-of-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>Before delving into the command-line tool and its sub-commands, let&#8217;s first
understand the directory structure of a Scrapy project.</p>
<p>Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy.cfg
myproject/
    __init__.py
    items.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
</pre></div>
</div>
<p>The directory where the <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> file resides is known as the <em>project
root directory</em>. That file contains the name of the python module that defines
the project settings. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">myproject</span><span class="o">.</span><span class="n">settings</span>
</pre></div>
</div>
</div>
<div class="section" id="using-the-scrapy-tool">
<h4>Using the <code class="docutils literal"><span class="pre">scrapy</span></code> tool<a class="headerlink" href="#using-the-scrapy-tool" title="Permalink to this headline">¶</a></h4>
<p>You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Scrapy X.Y - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
</pre></div>
</div>
<p>The first line will print the currently active project if you&#8217;re inside a
Scrapy project. In this example it was run from outside a project. If run from inside
a project it would have printed something like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Scrapy X.Y - project: myproject

Usage:
  scrapy &lt;command&gt; [options] [args]

[...]
</pre></div>
</div>
<div class="section" id="creating-projects">
<h5>Creating projects<a class="headerlink" href="#creating-projects" title="Permalink to this headline">¶</a></h5>
<p>The first thing you typically do with the <code class="docutils literal"><span class="pre">scrapy</span></code> tool is create your Scrapy
project:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy startproject myproject
</pre></div>
</div>
<p>That will create a Scrapy project under the <code class="docutils literal"><span class="pre">myproject</span></code> directory.</p>
<p>Next, you go inside the new project directory:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>cd myproject
</pre></div>
</div>
<p>And you&#8217;re ready to use the <code class="docutils literal"><span class="pre">scrapy</span></code> command to manage and control your
project from there.</p>
</div>
<div class="section" id="controlling-projects">
<h5>Controlling projects<a class="headerlink" href="#controlling-projects" title="Permalink to this headline">¶</a></h5>
<p>You use the <code class="docutils literal"><span class="pre">scrapy</span></code> tool from inside your projects to control and manage
them.</p>
<p>For example, to create a new spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy genspider mydomain mydomain.com
</pre></div>
</div>
<p>Some Scrapy commands (like <a class="reference internal" href="#std:command-crawl"><code class="xref std std-command docutils literal"><span class="pre">crawl</span></code></a>) must be run from inside a Scrapy
project. See the <a class="reference internal" href="#topics-commands-ref"><span>commands reference</span></a> below for more
information on which commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider-overridden behaviours (such as the <code class="docutils literal"><span class="pre">user_agent</span></code> attribute to override
the user-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the <code class="docutils literal"><span class="pre">fetch</span></code> command is meant to be used to
check how spiders are downloading pages.</p>
</div>
</div>
<div class="section" id="available-tool-commands">
<span id="topics-commands-ref"></span><h4>Available tool commands<a class="headerlink" href="#available-tool-commands" title="Permalink to this headline">¶</a></h4>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more info
about each command by running:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>And you can see all available commands with:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>There are two kinds of commands, those that only work from inside a Scrapy
project (Project-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly different
when running from inside a project (as they would use the project overridden
settings).</p>
<p>Global commands:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a></li>
<li><a class="reference internal" href="#std:command-settings"><code class="xref std std-command docutils literal"><span class="pre">settings</span></code></a></li>
<li><a class="reference internal" href="#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a></li>
<li><a class="reference internal" href="#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a></li>
<li><a class="reference internal" href="#std:command-fetch"><code class="xref std std-command docutils literal"><span class="pre">fetch</span></code></a></li>
<li><a class="reference internal" href="#std:command-view"><code class="xref std std-command docutils literal"><span class="pre">view</span></code></a></li>
<li><a class="reference internal" href="#std:command-version"><code class="xref std std-command docutils literal"><span class="pre">version</span></code></a></li>
</ul>
<p>Project-only commands:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:command-crawl"><code class="xref std std-command docutils literal"><span class="pre">crawl</span></code></a></li>
<li><a class="reference internal" href="#std:command-check"><code class="xref std std-command docutils literal"><span class="pre">check</span></code></a></li>
<li><a class="reference internal" href="#std:command-list"><code class="xref std std-command docutils literal"><span class="pre">list</span></code></a></li>
<li><a class="reference internal" href="#std:command-edit"><code class="xref std std-command docutils literal"><span class="pre">edit</span></code></a></li>
<li><a class="reference internal" href="#std:command-parse"><code class="xref std std-command docutils literal"><span class="pre">parse</span></code></a></li>
<li><a class="reference internal" href="#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a></li>
<li><a class="reference internal" href="#std:command-bench"><code class="xref std std-command docutils literal"><span class="pre">bench</span></code></a></li>
</ul>
<div class="section" id="startproject">
<span id="std:command-startproject"></span><h5>startproject<a class="headerlink" href="#startproject" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">&lt;project_name&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Creates a new Scrapy project named <code class="docutils literal"><span class="pre">project_name</span></code>, under the <code class="docutils literal"><span class="pre">project_name</span></code>
directory.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy startproject myproject
</pre></div>
</div>
</div>
<div class="section" id="genspider">
<span id="std:command-genspider"></span><h5>genspider<a class="headerlink" href="#genspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">[-t</span> <span class="pre">template]</span> <span class="pre">&lt;name&gt;</span> <span class="pre">&lt;domain&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Create a new spider in the current project.</p>
<p>This is just a convenience shortcut command for creating spiders based on
pre-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider -d basic
import scrapy

class $classname(scrapy.Spider):
    name = &quot;$name&quot;
    allowed_domains = [&quot;$domain&quot;]
    start_urls = (
        &#39;http://www.$domain/&#39;,
        )

    def parse(self, response):
        pass

$ scrapy genspider -t basic example example.com
Created spider &#39;example&#39; using template &#39;basic&#39; in module:
  mybot.spiders.example
</pre></div>
</div>
</div>
<div class="section" id="crawl">
<span id="std:command-crawl"></span><h5>crawl<a class="headerlink" href="#crawl" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Start crawling using a spider.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="check">
<span id="std:command-check"></span><h5>check<a class="headerlink" href="#check" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">check</span> <span class="pre">[-l]</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; &#39;RetailPricex&#39; field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</pre></div>
</div>
</div>
<div class="section" id="list">
<span id="std:command-list"></span><h5>list<a class="headerlink" href="#list" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">list</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>List all available spiders in the current project. The output is one spider per
line.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy list
spider1
spider2
</pre></div>
</div>
</div>
<div class="section" id="edit">
<span id="std:command-edit"></span><h5>edit<a class="headerlink" href="#edit" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Edit the given spider using the editor defined in the <a class="reference internal" href="index.html#std:setting-EDITOR"><code class="xref std std-setting docutils literal"><span class="pre">EDITOR</span></code></a>
setting.</p>
<p>This command is provided only as a convenience shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug his spiders.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy edit spider1
</pre></div>
</div>
</div>
<div class="section" id="fetch">
<span id="std:command-fetch"></span><h5>fetch<a class="headerlink" href="#fetch" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">&lt;url&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.</p>
<p>The interesting thing about this command is that it fetches the page how the
spider would download it. For example, if the spider has a <code class="docutils literal"><span class="pre">USER_AGENT</span></code>
attribute which overrides the User Agent, it will use that one.</p>
<p>So this command can be used to &#8220;see&#8221; how your spider would fetch a certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be applied
and it will just use the default Scrapy downloader settings.</p>
<p>Usage examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],
 &#39;Age&#39;: [&#39;1263   &#39;],
 &#39;Connection&#39;: [&#39;close     &#39;],
 &#39;Content-Length&#39;: [&#39;596&#39;],
 &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],
 &#39;Date&#39;: [&#39;Wed, 18 Aug 2010 23:59:46 GMT&#39;],
 &#39;Etag&#39;: [&#39;&quot;573c1-254-48c9c87349680&quot;&#39;],
 &#39;Last-Modified&#39;: [&#39;Fri, 30 Jul 2010 15:30:18 GMT&#39;],
 &#39;Server&#39;: [&#39;Apache/2.2.3 (CentOS)&#39;]}
</pre></div>
</div>
</div>
<div class="section" id="view">
<span id="std:command-view"></span><h5>view<a class="headerlink" href="#view" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">view</span> <span class="pre">&lt;url&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Opens the given URL in a browser, as your Scrapy spider would &#8220;see&#8221; it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider &#8220;sees&#8221; and confirm it&#8217;s what you expect.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="shell">
<span id="std:command-shell"></span><h5>shell<a class="headerlink" href="#shell" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">[url]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is
given. Also supports UNIX-style local file paths, either relative with
<code class="docutils literal"><span class="pre">./</span></code> or <code class="docutils literal"><span class="pre">../</span></code> prefixes or absolute file paths.
See <a class="reference internal" href="index.html#topics-shell"><span>Scrapy shell</span></a> for more info.</p>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="parse">
<span id="std:command-parse"></span><h5>parse<a class="headerlink" href="#parse" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">&lt;url&gt;</span> <span class="pre">[options]</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Fetches the given URL and parses it with the spider that handles it, using the
method passed with the <code class="docutils literal"><span class="pre">--callback</span></code> option, or <code class="docutils literal"><span class="pre">parse</span></code> if not given.</p>
<p>Supported options:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>
<li><code class="docutils literal"><span class="pre">--a</span> <span class="pre">NAME=VALUE</span></code>: set spider argument (may be repeated)</li>
<li><code class="docutils literal"><span class="pre">--callback</span></code> or <code class="docutils literal"><span class="pre">-c</span></code>: spider method to use as callback for parsing the
response</li>
<li><code class="docutils literal"><span class="pre">--pipelines</span></code>: process items through pipelines</li>
<li><code class="docutils literal"><span class="pre">--rules</span></code> or <code class="docutils literal"><span class="pre">-r</span></code>: use <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a>
rules to discover the callback (i.e. spider method) to use for parsing the
response</li>
<li><code class="docutils literal"><span class="pre">--noitems</span></code>: don&#8217;t show scraped items</li>
<li><code class="docutils literal"><span class="pre">--nolinks</span></code>: don&#8217;t show extracted links</li>
<li><code class="docutils literal"><span class="pre">--nocolour</span></code>: avoid using pygments to colorize the output</li>
<li><code class="docutils literal"><span class="pre">--depth</span></code> or <code class="docutils literal"><span class="pre">-d</span></code>: depth level for which the requests should be followed
recursively (default: 1)</li>
<li><code class="docutils literal"><span class="pre">--verbose</span></code> or <code class="docutils literal"><span class="pre">-v</span></code>: display information for each depth level</li>
</ul>
<p>Usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;name&#39;: u&#39;Example item&#39;,
 &#39;category&#39;: u&#39;Furniture&#39;,
 &#39;length&#39;: u&#39;12 cm&#39;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
</div>
<div class="section" id="settings">
<span id="std:command-settings"></span><h5>settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">settings</span> <span class="pre">[options]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it&#8217;ll show the project setting value, otherwise it&#8217;ll
show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
</pre></div>
</div>
</div>
<div class="section" id="runspider">
<span id="std:command-runspider"></span><h5>runspider<a class="headerlink" href="#runspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">&lt;spider_file.py&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="version">
<span id="std:command-version"></span><h5>version<a class="headerlink" href="#version" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">[-v]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Prints the Scrapy version. If used with <code class="docutils literal"><span class="pre">-v</span></code> it also prints Python, Twisted
and Platform info, which is useful for bug reports.</p>
</div>
<div class="section" id="bench">
<span id="std:command-bench"></span><h5>bench<a class="headerlink" href="#bench" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<ul class="simple">
<li>Syntax: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">bench</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a quick benchmark test. <a class="reference internal" href="index.html#benchmarking"><span>Benchmarking</span></a>.</p>
</div>
</div>
<div class="section" id="custom-project-commands">
<h4>Custom project commands<a class="headerlink" href="#custom-project-commands" title="Permalink to this headline">¶</a></h4>
<p>You can also add your custom project commands by using the
<a class="reference internal" href="#std:setting-COMMANDS_MODULE"><code class="xref std std-setting docutils literal"><span class="pre">COMMANDS_MODULE</span></code></a> setting. See the Scrapy commands in
<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/commands">scrapy/commands</a> for examples on how to implement your commands.</p>
<div class="section" id="commands-module">
<span id="std:setting-COMMANDS_MODULE"></span><h5>COMMANDS_MODULE<a class="headerlink" href="#commands-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">''</span></code> (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to add custom
commands for your Scrapy project.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">COMMANDS_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.commands&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="register-commands-via-setup-py-entry-points">
<h5>Register commands via setup.py entry points<a class="headerlink" href="#register-commands-via-setup-py-entry-points" title="Permalink to this headline">¶</a></h5>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is an experimental feature, use with caution.</p>
</div>
<p>You can also add Scrapy commands from an external library by adding a
<code class="docutils literal"><span class="pre">scrapy.commands</span></code> section in the entry points of the library <code class="docutils literal"><span class="pre">setup.py</span></code>
file.</p>
<p>The following example adds <code class="docutils literal"><span class="pre">my_command</span></code> command:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;scrapy-mymodule&#39;</span><span class="p">,</span>
  <span class="n">entry_points</span><span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;scrapy.commands&#39;</span><span class="p">:</span> <span class="p">[</span>
      <span class="s1">&#39;my_command=my_scrapy_module.commands:MyCommand&#39;</span><span class="p">,</span>
    <span class="p">],</span>
  <span class="p">},</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/spiders"></span><div class="section" id="spiders">
<span id="topics-spiders"></span><h3>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h3>
<p>Spiders are classes which define how a certain site (or a group of sites) will be
scraped, including how to perform the crawl (i.e. follow links) and how to
extract structured data from their pages (i.e. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<ol class="arabic">
<li><p class="first">You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.</p>
<p>The first requests to perform are obtained by calling the
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> method which (by default)
generates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> for the URLs specified in the
<a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> and the
<a class="reference internal" href="#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-attr docutils literal"><span class="pre">parse</span></code></a> method as callback function for the
Requests.</p>
</li>
<li><p class="first">In the callback function, you parse the response (web page) and return either
dicts with extracted data, <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects,
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects, or an iterable of these objects.
Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.</p>
</li>
<li><p class="first">In callback functions, you parse the page contents, typically using
<a class="reference internal" href="index.html#topics-selectors"><span>Selectors</span></a> (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.</p>
</li>
<li><p class="first">Finally, the items returned from the spider will be typically persisted to a
database (in some <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>) or written to
a file using <a class="reference internal" href="index.html#topics-feed-exports"><span>Feed exports</span></a>.</p>
</li>
</ol>
<p>Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.</p>
<span class="target" id="module-scrapy.spiders"></span><div class="section" id="scrapy-spider">
<span id="topics-spiders-ref"></span><h4>scrapy.Spider<a class="headerlink" href="#scrapy-spider" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.spiders.Spider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">Spider</code><a class="headerlink" href="#scrapy.spiders.Spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the simplest spider, and the one from which every other spider
must inherit (including spiders that come bundled with Scrapy, as well as spiders
that you write yourself). It doesn&#8217;t provide any special functionality. It just
provides a default <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> implementation which sends requests from
the <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> spider attribute and calls the spider&#8217;s method <code class="docutils literal"><span class="pre">parse</span></code>
for each of the resulting responses.</p>
<dl class="attribute">
<dt id="scrapy.spiders.Spider.name">
<code class="descname">name</code><a class="headerlink" href="#scrapy.spiders.Spider.name" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it&#8217;s required.</p>
<p>If the spider scrapes a single domain, a common practice is to name the
spider after the domain, with or without the <a class="reference external" href="https://en.wikipedia.org/wiki/Top-level_domain">TLD</a>. So, for example, a
spider that crawls <code class="docutils literal"><span class="pre">mywebsite.com</span></code> would often be called
<code class="docutils literal"><span class="pre">mywebsite</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.allowed_domains">
<code class="descname">allowed_domains</code><a class="headerlink" href="#scrapy.spiders.Spider.allowed_domains" title="Permalink to this definition">¶</a></dt>
<dd><p>An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list (or their subdomains) won&#8217;t be followed if
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></code></a> is enabled.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.start_urls">
<code class="descname">start_urls</code><a class="headerlink" href="#scrapy.spiders.Spider.start_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.custom_settings">
<code class="descname">custom_settings</code><a class="headerlink" href="#scrapy.spiders.Spider.custom_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary of settings that will be overridden from the project wide
configuration when running this spider. It must be defined as a class
attribute since the settings are updated before instantiation.</p>
<p>For a list of available built-in settings see:
<a class="reference internal" href="index.html#topics-settings-ref"><span>Built-in settings reference</span></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.crawler">
<code class="descname">crawler</code><a class="headerlink" href="#scrapy.spiders.Spider.crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is set by the <a class="reference internal" href="index.html#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a> class method after
initializating the class, and links to the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object to which this spider instance is
bound.</p>
<p>Crawlers encapsulate a lot of components in the project for their single
entry access (such as extensions, middlewares, signals managers, etc).
See <a class="reference internal" href="index.html#topics-api-crawler"><span>Crawler API</span></a> to know more about them.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.settings">
<code class="descname">settings</code><a class="headerlink" href="#scrapy.spiders.Spider.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Configuration for running this spider. This is a
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> instance, see the
<a class="reference internal" href="index.html#topics-settings"><span>Settings</span></a> topic for a detailed introduction on this subject.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.logger">
<code class="descname">logger</code><a class="headerlink" href="#scrapy.spiders.Spider.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Python logger created with the Spider&#8217;s <a class="reference internal" href="#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal"><span class="pre">name</span></code></a>. You can use it to
send log messages through it as described on
<a class="reference internal" href="index.html#topics-logging-from-spiders"><span>Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>crawler</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the class method used by Scrapy to create your spiders.</p>
<p>You probably won&#8217;t need to override this directly because the default
implementation acts as a proxy to the <code class="xref py py-meth docutils literal"><span class="pre">__init__()</span></code> method, calling
it with the given arguments <cite>args</cite> and named arguments <cite>kwargs</cite>.</p>
<p>Nonetheless, this method sets the <a class="reference internal" href="#scrapy.spiders.Spider.crawler" title="scrapy.spiders.Spider.crawler"><code class="xref py py-attr docutils literal"><span class="pre">crawler</span></code></a> and <a class="reference internal" href="#scrapy.spiders.Spider.settings" title="scrapy.spiders.Spider.settings"><code class="xref py py-attr docutils literal"><span class="pre">settings</span></code></a>
attributes in the new instance so they can be accessed later inside the
spider&#8217;s code.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> instance) &#8211; crawler to which the spider will be bound</li>
<li><strong>args</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; arguments passed to the <code class="xref py py-meth docutils literal"><span class="pre">__init__()</span></code> method</li>
<li><strong>kwargs</strong> (<em>dict</em>) &#8211; keyword arguments passed to the <code class="xref py py-meth docutils literal"><span class="pre">__init__()</span></code> method</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.start_requests">
<code class="descname">start_requests</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.start_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>This method must return an iterable with the first Requests to crawl for
this spider.</p>
<p>This is the method called by Scrapy when the spider is opened for
scraping when no particular URLs are specified. If particular URLs are
specified, the <a class="reference internal" href="#scrapy.spiders.Spider.make_requests_from_url" title="scrapy.spiders.Spider.make_requests_from_url"><code class="xref py py-meth docutils literal"><span class="pre">make_requests_from_url()</span></code></a> is used instead to create
the Requests. This method is also called only once from Scrapy, so it&#8217;s
safe to implement it as a generator.</p>
<p>The default implementation uses <a class="reference internal" href="#scrapy.spiders.Spider.make_requests_from_url" title="scrapy.spiders.Spider.make_requests_from_url"><code class="xref py py-meth docutils literal"><span class="pre">make_requests_from_url()</span></code></a> to
generate Requests for each url in <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a>.</p>
<p>If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/login&quot;</span><span class="p">,</span>
                                   <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;pass&#39;</span><span class="p">:</span> <span class="s1">&#39;secret&#39;</span><span class="p">},</span>
                                   <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># here you would extract links to follow and return Requests for</span>
        <span class="c1"># each of them, with another callback</span>
        <span class="k">pass</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.make_requests_from_url">
<code class="descname">make_requests_from_url</code><span class="sig-paren">(</span><em>url</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.make_requests_from_url" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives a URL and returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object (or a list of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects) to scrape. This
method is used to construct the initial requests in the
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> method, and is typically used to convert urls to
requests.</p>
<p>Unless overridden, this method returns Requests with the <a class="reference internal" href="#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a>
method as their callback function, and with dont_filter parameter enabled
(see <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> class for more info).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.parse">
<code class="descname">parse</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.parse" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the default callback used by Scrapy to process downloaded
responses, when their requests don&#8217;t specify a callback.</p>
<p>The <code class="docutils literal"><span class="pre">parse</span></code> method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the <a class="reference internal" href="#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> class.</p>
<p>This method, as well as any other Request callback, must return an
iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and/or
dicts or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>) &#8211; the response to parse</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.log">
<code class="descname">log</code><span class="sig-paren">(</span><em>message</em><span class="optional">[</span>, <em>level</em>, <em>component</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper that sends a log message through the Spider&#8217;s <a class="reference internal" href="#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal"><span class="pre">logger</span></code></a>,
kept for backwards compatibility. For more information see
<a class="reference internal" href="index.html#topics-logging-from-spiders"><span>Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.closed">
<code class="descname">closed</code><span class="sig-paren">(</span><em>reason</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider closes. This method provides a shortcut to
signals.connect() for the <a class="reference internal" href="index.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> signal.</p>
</dd></dl>

</dd></dl>

<p>Let&#8217;s see an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;A response from </span><span class="si">%s</span><span class="s1"> just arrived!&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>Return multiple Requests and items from a single callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">h3</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Instead of <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> you can use <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> directly;
to give data more structure you can use <a class="reference internal" href="index.html#topics-items"><span>Items</span></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-arguments">
<span id="spiderargs"></span><h4>Spider arguments<a class="headerlink" href="#spider-arguments" title="Permalink to this headline">¶</a></h4>
<p>Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.</p>
<p>Spider arguments are passed through the <a class="reference internal" href="index.html#std:command-crawl"><code class="xref std std-command docutils literal"><span class="pre">crawl</span></code></a> command using the
<code class="docutils literal"><span class="pre">-a</span></code> option. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy crawl myspider -a category=electronics
</pre></div>
</div>
<p>Spiders receive arguments in their constructors:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/categories/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">category</span><span class="p">]</span>
        <span class="c1"># ...</span>
</pre></div>
</div>
<p>Spider arguments can also be passed through the Scrapyd <code class="docutils literal"><span class="pre">schedule.json</span></code> API.
See <a class="reference external" href="http://scrapyd.readthedocs.org/en/latest/">Scrapyd documentation</a>.</p>
</div>
<div class="section" id="generic-spiders">
<span id="builtin-spiders"></span><h4>Generic Spiders<a class="headerlink" href="#generic-spiders" title="Permalink to this headline">¶</a></h4>
<p>Scrapy comes with some useful generic spiders that you can use to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>, or parsing an XML/CSV feed.</p>
<p>For the examples used in the following spiders, we&#8217;ll assume you have a project
with a <code class="docutils literal"><span class="pre">TestItem</span></code> declared in a <code class="docutils literal"><span class="pre">myproject.items</span></code> module:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TestItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="crawlspider">
<h5>CrawlSpider<a class="headerlink" href="#crawlspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.CrawlSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">CrawlSpider</code><a class="headerlink" href="#scrapy.spiders.CrawlSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it&#8217;s generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.</p>
<p>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:</p>
<dl class="attribute">
<dt id="scrapy.spiders.CrawlSpider.rules">
<code class="descname">rules</code><a class="headerlink" href="#scrapy.spiders.CrawlSpider.rules" title="Permalink to this definition">¶</a></dt>
<dd><p>Which is a list of one (or more) <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a> objects.  Each <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a>
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they&#8217;re defined in this attribute.</p>
</dd></dl>

<p>This spider also exposes an overrideable method:</p>
<dl class="method">
<dt id="scrapy.spiders.CrawlSpider.parse_start_url">
<code class="descname">parse_start_url</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CrawlSpider.parse_start_url" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the start_urls responses. It allows to parse
the initial responses and must return either an
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object, a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object, or an iterable containing any of them.</p>
</dd></dl>

</dd></dl>

<div class="section" id="crawling-rules">
<h6>Crawling rules<a class="headerlink" href="#crawling-rules" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.spiders.Rule">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">Rule</code><span class="sig-paren">(</span><em>link_extractor</em>, <em>callback=None</em>, <em>cb_kwargs=None</em>, <em>follow=None</em>, <em>process_links=None</em>, <em>process_request=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Rule" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal"><span class="pre">link_extractor</span></code> is a <a class="reference internal" href="index.html#topics-link-extractors"><span>Link Extractor</span></a> object which
defines how links will be extracted from each crawled page.</p>
<p><code class="docutils literal"><span class="pre">callback</span></code> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link_extractor. This callback receives a response as its first
argument and must return a list containing <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> and/or
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects (or any subclass of them).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When writing crawl spider rules, avoid using <code class="docutils literal"><span class="pre">parse</span></code> as
callback, since the <a class="reference internal" href="#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> uses the <code class="docutils literal"><span class="pre">parse</span></code> method
itself to implement its logic. So if you override the <code class="docutils literal"><span class="pre">parse</span></code> method,
the crawl spider will no longer work.</p>
</div>
<p><code class="docutils literal"><span class="pre">cb_kwargs</span></code> is a dict containing the keyword arguments to be passed to the
callback function.</p>
<p><code class="docutils literal"><span class="pre">follow</span></code> is a boolean which specifies if links should be followed from each
response extracted with this rule. If <code class="docutils literal"><span class="pre">callback</span></code> is None <code class="docutils literal"><span class="pre">follow</span></code> defaults
to <code class="docutils literal"><span class="pre">True</span></code>, otherwise it defaults to <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p><code class="docutils literal"><span class="pre">process_links</span></code> is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified <code class="docutils literal"><span class="pre">link_extractor</span></code>.
This is mainly used for filtering purposes.</p>
<p><code class="docutils literal"><span class="pre">process_request</span></code> is a callable, or a string (in which case a method from
the spider object with that name will be used) which will be called with
every request extracted by this rule, and must return a request or None (to
filter out the request).</p>
</dd></dl>

</div>
<div class="section" id="crawlspider-example">
<h6>CrawlSpider example<a class="headerlink" href="#crawlspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let&#8217;s now take a look at an example CrawlSpider with rules:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com&#39;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># Extract links matching &#39;category.php&#39; (but not matching &#39;subsection.php&#39;)</span>
        <span class="c1"># and follow links from them (since no callback means follow=True by default).</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;category\.php&#39;</span><span class="p">,</span> <span class="p">),</span> <span class="n">deny</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;subsection\.php&#39;</span><span class="p">,</span> <span class="p">))),</span>

        <span class="c1"># Extract links matching &#39;item.php&#39; and parse them with the spider&#39;s method parse_item</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;item\.php&#39;</span><span class="p">,</span> <span class="p">)),</span> <span class="n">callback</span><span class="o">=</span><span class="s1">&#39;parse_item&#39;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is an item page! </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">r&#39;ID: (\d+)&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>This spider would start crawling example.com&#8217;s home page, collecting category
links, and item links, parsing the latter with the <code class="docutils literal"><span class="pre">parse_item</span></code> method. For
each item response, some data will be extracted from the HTML using XPath, and
an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> will be filled with it.</p>
</div>
</div>
<div class="section" id="xmlfeedspider">
<h5>XMLFeedSpider<a class="headerlink" href="#xmlfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.XMLFeedSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">XMLFeedSpider</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: <code class="docutils literal"><span class="pre">iternodes</span></code>, <code class="docutils literal"><span class="pre">xml</span></code>,
and <code class="docutils literal"><span class="pre">html</span></code>.  It&#8217;s recommended to use the <code class="docutils literal"><span class="pre">iternodes</span></code> iterator for
performance reasons, since the <code class="docutils literal"><span class="pre">xml</span></code> and <code class="docutils literal"><span class="pre">html</span></code> iterators generate the
whole DOM at once in order to parse it.  However, using <code class="docutils literal"><span class="pre">html</span></code> as the
iterator may be useful when parsing XML with bad markup.</p>
<p>To set the iterator and the tag name, you must define the following class
attributes:</p>
<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.iterator">
<code class="descname">iterator</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the iterator to use. It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">'iternodes'</span></code> - a fast iterator based on regular expressions</li>
<li><code class="docutils literal"><span class="pre">'html'</span></code> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
<li><code class="docutils literal"><span class="pre">'xml'</span></code> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
</ul>
</div></blockquote>
<p>It defaults to: <code class="docutils literal"><span class="pre">'iternodes'</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.itertag">
<code class="descname">itertag</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.itertag" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the name of the node (or element) to iterate in. Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;product&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.namespaces">
<code class="descname">namespaces</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of <code class="docutils literal"><span class="pre">(prefix,</span> <span class="pre">uri)</span></code> tuples which define the namespaces
available in that document that will be processed with this spider. The
<code class="docutils literal"><span class="pre">prefix</span></code> and <code class="docutils literal"><span class="pre">uri</span></code> will be used to automatically register
namespaces using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.register_namespace" title="scrapy.selector.Selector.register_namespace"><code class="xref py py-meth docutils literal"><span class="pre">register_namespace()</span></code></a> method.</p>
<p>You can then specify nodes with namespaces in the <a class="reference internal" href="#scrapy.spiders.XMLFeedSpider.itertag" title="scrapy.spiders.XMLFeedSpider.itertag"><code class="xref py py-attr docutils literal"><span class="pre">itertag</span></code></a>
attribute.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">YourSpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>

    <span class="n">namespaces</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;http://www.sitemaps.org/schemas/sitemap/0.9&#39;</span><span class="p">)]</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;n:url&#39;</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
</dd></dl>

<p>Apart from these new attributes, this spider has the following overrideable
methods too:</p>
<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.adapt_response">
<code class="descname">adapt_response</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.adapt_response" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.parse_node">
<code class="descname">parse_node</code><span class="sig-paren">(</span><em>response</em>, <em>selector</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.parse_node" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the nodes matching the provided tag name
(<code class="docutils literal"><span class="pre">itertag</span></code>).  Receives the response and an
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> for each node.  Overriding this
method is mandatory. Otherwise, you spider won&#8217;t work.  This method
must return either a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object, a
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, or an iterable containing any of
them.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.process_results">
<code class="descname">process_results</code><span class="sig-paren">(</span><em>response</em>, <em>results</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.process_results" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each result (item or request) returned by the
spider, and it&#8217;s intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (Items or Requests).</p>
</dd></dl>

</dd></dl>

<div class="section" id="xmlfeedspider-example">
<h6>XMLFeedSpider example<a class="headerlink" href="#xmlfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>These spiders are pretty easy to use, let&#8217;s have a look at one example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">XMLFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/feed.xml&#39;</span><span class="p">]</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="s1">&#39;iternodes&#39;</span>  <span class="c1"># This is actually unnecessary, since it&#39;s the default value</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;item&#39;</span>

    <span class="k">def</span> <span class="nf">parse_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is a &lt;</span><span class="si">%s</span><span class="s1">&gt; node!: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">itertag</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">extract</span><span class="p">()))</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@id&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;description&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically what we did up there was to create a spider that downloads a feed from
the given <code class="docutils literal"><span class="pre">start_urls</span></code>, and then iterates through each of its <code class="docutils literal"><span class="pre">item</span></code> tags,
prints them out, and stores some random data in an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>.</p>
</div>
</div>
<div class="section" id="csvfeedspider">
<h5>CSVFeedSpider<a class="headerlink" href="#csvfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.CSVFeedSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">CSVFeedSpider</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is <a class="reference internal" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="scrapy.spiders.CSVFeedSpider.parse_row"><code class="xref py py-meth docutils literal"><span class="pre">parse_row()</span></code></a>.</p>
<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.delimiter">
<code class="descname">delimiter</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.delimiter" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the separator character for each field in the CSV file
Defaults to <code class="docutils literal"><span class="pre">','</span></code> (comma).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.quotechar">
<code class="descname">quotechar</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.quotechar" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the enclosure character for each field in the CSV file
Defaults to <code class="docutils literal"><span class="pre">'&quot;'</span></code> (quotation mark).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of the rows contained in the file CSV feed which will be used to
extract fields from it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.CSVFeedSpider.parse_row">
<code class="descname">parse_row</code><span class="sig-paren">(</span><em>response</em>, <em>row</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override <code class="docutils literal"><span class="pre">adapt_response</span></code> and <code class="docutils literal"><span class="pre">process_results</span></code> methods
for pre- and post-processing purposes.</p>
</dd></dl>

</dd></dl>

<div class="section" id="csvfeedspider-example">
<h6>CSVFeedSpider example<a class="headerlink" href="#csvfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let&#8217;s see an example similar to the previous one, but using a
<a class="reference internal" href="#scrapy.spiders.CSVFeedSpider" title="scrapy.spiders.CSVFeedSpider"><code class="xref py py-class docutils literal"><span class="pre">CSVFeedSpider</span></code></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CSVFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CSVFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/feed.csv&#39;</span><span class="p">]</span>
    <span class="n">delimiter</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span>
    <span class="n">quotechar</span> <span class="o">=</span> <span class="s2">&quot;&#39;&quot;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;description&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is a row!: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sitemapspider">
<h5>SitemapSpider<a class="headerlink" href="#sitemapspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.SitemapSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">SitemapSpider</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>SitemapSpider allows you to crawl a site by discovering the URLs using
<a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>.</p>
<p>It supports nested sitemaps and discovering sitemap urls from
<a class="reference external" href="http://www.robotstxt.org/">robots.txt</a>.</p>
<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_urls">
<code class="descname">sitemap_urls</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of urls pointing to the sitemaps whose urls you want to crawl.</p>
<p>You can also point to a <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> and it will be parsed to extract
sitemap urls from it.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_rules">
<code class="descname">sitemap_rules</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_rules" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of tuples <code class="docutils literal"><span class="pre">(regex,</span> <span class="pre">callback)</span></code> where:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">regex</span></code> is a regular expression to match urls extracted from sitemaps.
<code class="docutils literal"><span class="pre">regex</span></code> can be either a str or a compiled regex object.</li>
<li>callback is the callback to use for processing the urls that match
the regular expression. <code class="docutils literal"><span class="pre">callback</span></code> can be a string (indicating the
name of a spider method) or a callable.</li>
</ul>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;/product/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_product&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Rules are applied in order, and only the first one that matches will be
used.</p>
<p>If you omit this attribute, all urls found in sitemaps will be
processed with the <code class="docutils literal"><span class="pre">parse</span></code> callback.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_follow">
<code class="descname">sitemap_follow</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_follow" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of regexes of sitemap that should be followed. This is is only
for sites that use <a class="reference external" href="http://www.sitemaps.org/protocol.html#index">Sitemap index files</a> that point to other sitemap
files.</p>
<p>By default, all sitemaps are followed.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_alternate_links">
<code class="descname">sitemap_alternate_links</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_alternate_links" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies if alternate links for one <code class="docutils literal"><span class="pre">url</span></code> should be followed. These
are links for the same website in another language passed within
the same <code class="docutils literal"><span class="pre">url</span></code> block.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>&lt;url&gt;
    &lt;loc&gt;http://example.com/&lt;/loc&gt;
    &lt;xhtml:link rel=&quot;alternate&quot; hreflang=&quot;de&quot; href=&quot;http://example.com/de&quot;/&gt;
&lt;/url&gt;
</pre></div>
</div>
<p>With <code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> set, this would retrieve both URLs. With
<code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> disabled, only <code class="docutils literal"><span class="pre">http://example.com/</span></code> would be
retrieved.</p>
<p>Default is <code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> disabled.</p>
</dd></dl>

</dd></dl>

<div class="section" id="sitemapspider-examples">
<h6>SitemapSpider examples<a class="headerlink" href="#sitemapspider-examples" title="Permalink to this headline">¶</a></h6>
<p>Simplest example: process all urls discovered through sitemaps using the
<code class="docutils literal"><span class="pre">parse</span></code> callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape item here ...</span>
</pre></div>
</div>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/product/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_product&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;/category/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_category&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape product ...</span>

    <span class="k">def</span> <span class="nf">parse_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape category ...</span>
</pre></div>
</div>
<p>Follow sitemaps defined in the <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> file and only follow sitemaps
whose url contains <code class="docutils literal"><span class="pre">/sitemap_shop</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">sitemap_follow</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;/sitemap_shops&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape shop here ...</span>
</pre></div>
</div>
<p>Combine SitemapSpider with other sources of urls:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">other_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/about&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">requests</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">start_requests</span><span class="p">())</span>
        <span class="n">requests</span> <span class="o">+=</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_other</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_urls</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">requests</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape shop here ...</span>

    <span class="k">def</span> <span class="nf">parse_other</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape other here ...</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/selectors"></span><div class="section" id="selectors">
<span id="topics-selectors"></span><h3>Selectors<a class="headerlink" href="#selectors" title="Permalink to this headline">¶</a></h3>
<p>When you&#8217;re scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> is a very popular web scraping library among Python
programmers which constructs a Python object based on the structure of the
HTML code and also deals with bad markup reasonably well, but it has one
drawback: it&#8217;s slow.</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a> is an XML parsing library (which also parses HTML) with a pythonic
API based on <a class="reference external" href="https://docs.python.org/2/library/xml.etree.elementtree.html">ElementTree</a>. (lxml is not part of the Python standard
library.)</li>
</ul>
</div></blockquote>
<p>Scrapy comes with its own mechanism for extracting data. They&#8217;re called
selectors because they &#8220;select&#8221; certain parts of the HTML document specified
either by <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> expressions.</p>
<p><a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> is a language for selecting nodes in XML documents, which can also be
used with HTML. <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.</p>
<p>Scrapy selectors are built over the <a class="reference external" href="http://lxml.de/">lxml</a> library, which means they&#8217;re very
similar in speed and parsing accuracy.</p>
<p>This page explains how selectors work and describes their API which is very
small and simple, unlike the <a class="reference external" href="http://lxml.de/">lxml</a> API which is much bigger because the
<a class="reference external" href="http://lxml.de/">lxml</a> library can be used for many other tasks, besides selecting markup
documents.</p>
<p>For a complete reference of the selectors API see
<a class="reference internal" href="#topics-selectors-ref"><span>Selector reference</span></a></p>
<div class="section" id="using-selectors">
<h4>Using selectors<a class="headerlink" href="#using-selectors" title="Permalink to this headline">¶</a></h4>
<div class="section" id="constructing-selectors">
<h5>Constructing selectors<a class="headerlink" href="#constructing-selectors" title="Permalink to this headline">¶</a></h5>
<p>Scrapy selectors are instances of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> class
constructed by passing <strong>text</strong> or <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
object. It automatically chooses the best parsing rules (XML vs HTML) based on
input type:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
</pre></div>
</div>
<p>Constructing from text:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">body</span> <span class="o">=</span> <span class="s1">&#39;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>Constructing from response:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
<p>For convenience, response objects expose a selector on <cite>.selector</cite> attribute,
it&#8217;s totally OK to use this shortcut when possible:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;good&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h5>Using selectors<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>To explain how to use the selectors we&#8217;ll use the <cite>Scrapy shell</cite> (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<blockquote>
<div><a class="reference external" href="http://doc.scrapy.org/en/latest/_static/selectors-sample1.html">http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a></div></blockquote>
<p id="topics-selectors-htmlcode">Here&#8217;s its HTML code:</p>
<div class="highlight-html"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">html</span><span class="p">&gt;</span>
 <span class="p">&lt;</span><span class="nt">head</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">base</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;http://example.com/&#39;</span> <span class="p">/&gt;</span>
  <span class="p">&lt;</span><span class="nt">title</span><span class="p">&gt;</span>Example website<span class="p">&lt;/</span><span class="nt">title</span><span class="p">&gt;</span>
 <span class="p">&lt;/</span><span class="nt">head</span><span class="p">&gt;</span>
 <span class="p">&lt;</span><span class="nt">body</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">&#39;images&#39;</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image1.html&#39;</span><span class="p">&gt;</span>Name: My image 1 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image1_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image2.html&#39;</span><span class="p">&gt;</span>Name: My image 2 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image2_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image3.html&#39;</span><span class="p">&gt;</span>Name: My image 3 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image3_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image4.html&#39;</span><span class="p">&gt;</span>Name: My image 4 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image4_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image5.html&#39;</span><span class="p">&gt;</span>Name: My image 5 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image5_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
 <span class="p">&lt;/</span><span class="nt">body</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">html</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>First, let&#8217;s open the shell:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html
</pre></div>
</div>
<p>Then, after the shell loads, you&#8217;ll have the response available as <code class="docutils literal"><span class="pre">response</span></code>
shell variable, and its attached selector in <code class="docutils literal"><span class="pre">response.selector</span></code> attribute.</p>
<p>Since we&#8217;re dealing with HTML, the selector will automatically use an HTML parser.</p>
<p>So, by looking at the <a class="reference internal" href="#topics-selectors-htmlcode"><span>HTML code</span></a> of that
page, let&#8217;s construct an XPath for selecting the text inside the title tag:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>Querying responses using XPath and CSS is so common that responses include two
convenience shortcuts: <code class="docutils literal"><span class="pre">response.xpath()</span></code> and <code class="docutils literal"><span class="pre">response.css()</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector (text) xpath=//title/text()&gt;]</span>
</pre></div>
</div>
<p>As you can see, <code class="docutils literal"><span class="pre">.xpath()</span></code> and <code class="docutils literal"><span class="pre">.css()</span></code> methods return a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance, which is a list of new
selectors. This API can be used for quickly selecting nested data:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>To actually extract the textual data, you must call the selector <code class="docutils literal"><span class="pre">.extract()</span></code>
method, as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>If you want to extract only first matched element, you can call the selector <code class="docutils literal"><span class="pre">.extract_first()</span></code></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;images&quot;]/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
<span class="go">u&#39;Name: My image 1 &#39;</span>
</pre></div>
</div>
<p>It returns <code class="docutils literal"><span class="pre">None</span></code> if no element was found:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span> <span class="ow">is</span> <span class="bp">None</span>
<span class="go">True</span>
</pre></div>
</div>
<p>A default return value can be provided as an argument, to be used instead of <code class="docutils literal"><span class="pre">None</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s1">&#39;not-found&#39;</span><span class="p">)</span>
<span class="go">&#39;not-found&#39;</span>
</pre></div>
</div>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;Example website&#39;]</span>
</pre></div>
</div>
<p>Now we&#8217;re going to get the base URL and some image links:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//base/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;http://example.com/&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a[href*=image]::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1.html&#39;,</span>
<span class="go"> u&#39;image2.html&#39;,</span>
<span class="go"> u&#39;image3.html&#39;,</span>
<span class="go"> u&#39;image4.html&#39;,</span>
<span class="go"> u&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a[href*=image] img::attr(src)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image2_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image3_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image4_thumb.jpg&#39;,</span>
<span class="go"> u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="nesting-selectors">
<span id="topics-selectors-nesting-selectors"></span><h5>Nesting selectors<a class="headerlink" href="#nesting-selectors" title="Permalink to this headline">¶</a></h5>
<p>The selection methods (<code class="docutils literal"><span class="pre">.xpath()</span></code> or <code class="docutils literal"><span class="pre">.css()</span></code>) return a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here&#8217;s an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">links</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> u&#39;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">link</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">links</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">(),</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">())</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s1">&#39;Link number </span><span class="si">%d</span><span class="s1"> points to url </span><span class="si">%s</span><span class="s1"> and image </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">args</span>

<span class="go">Link number 0 points to url [u&#39;image1.html&#39;] and image [u&#39;image1_thumb.jpg&#39;]</span>
<span class="go">Link number 1 points to url [u&#39;image2.html&#39;] and image [u&#39;image2_thumb.jpg&#39;]</span>
<span class="go">Link number 2 points to url [u&#39;image3.html&#39;] and image [u&#39;image3_thumb.jpg&#39;]</span>
<span class="go">Link number 3 points to url [u&#39;image4.html&#39;] and image [u&#39;image4_thumb.jpg&#39;]</span>
<span class="go">Link number 4 points to url [u&#39;image5.html&#39;] and image [u&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-selectors-with-regular-expressions">
<h5>Using selectors with regular expressions<a class="headerlink" href="#using-selectors-with-regular-expressions" title="Permalink to this headline">¶</a></h5>
<p><a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> also has a <code class="docutils literal"><span class="pre">.re()</span></code> method for extracting
data using regular expressions. However, unlike using <code class="docutils literal"><span class="pre">.xpath()</span></code> or
<code class="docutils literal"><span class="pre">.css()</span></code> methods, <code class="docutils literal"><span class="pre">.re()</span></code> returns a list of unicode strings. So you
can&#8217;t construct nested <code class="docutils literal"><span class="pre">.re()</span></code> calls.</p>
<p>Here&#8217;s an example used to extract image names from the <a class="reference internal" href="#topics-selectors-htmlcode"><span>HTML code</span></a> above:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="s1">r&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">[u&#39;My image 1&#39;,</span>
<span class="go"> u&#39;My image 2&#39;,</span>
<span class="go"> u&#39;My image 3&#39;,</span>
<span class="go"> u&#39;My image 4&#39;,</span>
<span class="go"> u&#39;My image 5&#39;]</span>
</pre></div>
</div>
<p>There&#8217;s an additional helper reciprocating <code class="docutils literal"><span class="pre">.extract_first()</span></code> for <code class="docutils literal"><span class="pre">.re()</span></code>,
named <code class="docutils literal"><span class="pre">.re_first()</span></code>. Use it to extract just the first matching string:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="s1">r&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">u&#39;My image 1&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-relative-xpaths">
<span id="topics-selectors-relative-xpaths"></span><h5>Working with relative XPaths<a class="headerlink" href="#working-with-relative-xpaths" title="Permalink to this headline">¶</a></h5>
<p>Keep in mind that if you are nesting selectors and use an XPath that starts
with <code class="docutils literal"><span class="pre">/</span></code>, that XPath will be absolute to the document and not relative to the
<code class="docutils literal"><span class="pre">Selector</span></code> you&#8217;re calling it from.</p>
<p>For example, suppose you want to extract all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> elements inside <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code>
elements. First, you would get all <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> elements from the document, not only those
inside <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p&#39;</span><span class="p">):</span>  <span class="c1"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>This is the proper way to do it (note the dot prefixing the <code class="docutils literal"><span class="pre">.//p</span></code> XPath):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;.//p&#39;</span><span class="p">):</span>  <span class="c1"># extracts all &lt;p&gt; inside</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>Another common case would be to extract all direct <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> children:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>For more details about relative XPaths see the <a class="reference external" href="https://www.w3.org/TR/xpath#location-paths">Location Paths</a> section in the
XPath specification.</p>
</div>
<div class="section" id="using-exslt-extensions">
<h5>Using EXSLT extensions<a class="headerlink" href="#using-exslt-extensions" title="Permalink to this headline">¶</a></h5>
<p>Being built atop <a class="reference external" href="http://lxml.de/">lxml</a>, Scrapy selectors also support some <a class="reference external" href="http://exslt.org/">EXSLT</a> extensions
and come with these pre-registered namespaces to use in XPath expressions:</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="56%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">prefix</th>
<th class="head">namespace</th>
<th class="head">usage</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>re</td>
<td>http://exslt.org/regular-expressions</td>
<td><a class="reference external" href="http://exslt.org/regexp/index.html">regular expressions</a></td>
</tr>
<tr class="row-odd"><td>set</td>
<td>http://exslt.org/sets</td>
<td><a class="reference external" href="http://exslt.org/set/index.html">set manipulation</a></td>
</tr>
</tbody>
</table>
<div class="section" id="regular-expressions">
<h6>Regular expressions<a class="headerlink" href="#regular-expressions" title="Permalink to this headline">¶</a></h6>
<p>The <code class="docutils literal"><span class="pre">test()</span></code> function, for example, can prove quite useful when XPath&#8217;s
<code class="docutils literal"><span class="pre">starts-with()</span></code> or <code class="docutils literal"><span class="pre">contains()</span></code> are not sufficient.</p>
<p>Example selecting links in list item with a &#8220;class&#8221; attribute ending with a digit:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;ul&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link3.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[re:test(@class, &quot;item-\d$&quot;)]//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;link1.html&#39;, u&#39;link2.html&#39;, u&#39;link4.html&#39;, u&#39;link5.html&#39;]</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">C library <code class="docutils literal"><span class="pre">libxslt</span></code> doesn&#8217;t natively support EXSLT regular
expressions so <a class="reference external" href="http://lxml.de/">lxml</a>&#8216;s implementation uses hooks to Python&#8217;s <code class="docutils literal"><span class="pre">re</span></code> module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.</p>
</div>
</div>
<div class="section" id="set-operations">
<h6>Set operations<a class="headerlink" href="#set-operations" title="Permalink to this headline">¶</a></h6>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from <a class="reference external" href="http://schema.org/Product">http://schema.org/Product</a>)
with groups of itemscopes and corresponding itemprops:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;img src=&quot;kenmore-microwave-17in.jpg&quot; alt=&#39;Kenmore 17&quot; Microwave&#39; /&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;aggregateRating&quot;</span>
<span class="gp">... </span><span class="s2">    itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">   Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</span>
<span class="gp">... </span><span class="s2">   based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  Product description:</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</span>
<span class="gp">... </span><span class="s2">  Has six preset cooking categories and convenience features like</span>
<span class="gp">... </span><span class="s2">  Add-A-Minute and Child Lock.&lt;/span&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  Customer reviews:</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</span>
<span class="gp">... </span><span class="s2">    it. &lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</span>
<span class="gp">... </span><span class="s2">    fits in my apartment.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  ...</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@itemscope]&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s2">&quot;current scope:&quot;</span><span class="p">,</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@itemtype&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">props</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">                set:difference(./descendant::*/@itemprop,</span>
<span class="gp">... </span><span class="s1">                               .//*[@itemscope]/*/@itemprop)&#39;&#39;&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">print</span> <span class="s2">&quot;    properties:&quot;</span><span class="p">,</span> <span class="n">props</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">print</span>

<span class="go">current scope: [u&#39;http://schema.org/Product&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;aggregateRating&#39;, u&#39;offers&#39;, u&#39;description&#39;, u&#39;review&#39;, u&#39;review&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/AggregateRating&#39;]</span>
<span class="go">    properties: [u&#39;ratingValue&#39;, u&#39;reviewCount&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Offer&#39;]</span>
<span class="go">    properties: [u&#39;price&#39;, u&#39;availability&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [u&#39;name&#39;, u&#39;author&#39;, u&#39;datePublished&#39;, u&#39;reviewRating&#39;, u&#39;description&#39;]</span>

<span class="go">current scope: [u&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [u&#39;worstRating&#39;, u&#39;ratingValue&#39;, u&#39;bestRating&#39;]</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>Here we first iterate over <code class="docutils literal"><span class="pre">itemscope</span></code> elements, and for each one,
we look for all <code class="docutils literal"><span class="pre">itemprops</span></code> elements and exclude those that are themselves
inside another <code class="docutils literal"><span class="pre">itemscope</span></code>.</p>
</div>
</div>
<div class="section" id="some-xpath-tips">
<h5>Some XPath tips<a class="headerlink" href="#some-xpath-tips" title="Permalink to this headline">¶</a></h5>
<p>Here are some tips that you may find useful when using XPath
with Scrapy selectors, based on <a class="reference external" href="https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/">this post from ScrapingHub&#8217;s blog</a>.
If you are not much familiar with XPath yet,
you may want to take a look first at this <a class="reference external" href="http://www.zvon.org/comp/r/tut-XPath_1.html">XPath tutorial</a>.</p>
<div class="section" id="using-text-nodes-in-a-condition">
<h6>Using text nodes in a condition<a class="headerlink" href="#using-text-nodes-in-a-condition" title="Permalink to this headline">¶</a></h6>
<p>When you need to use the text content as argument to an <a class="reference external" href="https://www.w3.org/TR/xpath/#section-String-Functions">XPath string function</a>,
avoid using <code class="docutils literal"><span class="pre">.//text()</span></code> and use just <code class="docutils literal"><span class="pre">.</span></code> instead.</p>
<p>This is because the expression <code class="docutils literal"><span class="pre">.//text()</span></code> yields a collection of text elements &#8211; a <em>node-set</em>.
And when a node-set is converted to a string, which happens when it is passed as argument to
a string function like <code class="docutils literal"><span class="pre">contains()</span></code> or <code class="docutils literal"><span class="pre">starts-with()</span></code>, it results in the text for the first element only.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Converting a <em>node-set</em> to string:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a//text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span> <span class="c1"># take a peek at the node-set</span>
<span class="go">[u&#39;Click here to go to the &#39;, u&#39;Next Page&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1]//text())&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span> <span class="c1"># convert it to string</span>
<span class="go">[u&#39;Click here to go to the &#39;]</span>
</pre></div>
</div>
<p>A <em>node</em> converted to a string, however, puts together the text of itself plus of all its descendants:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[1]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span> <span class="c1"># select the first node</span>
<span class="go">[u&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1])&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span> <span class="c1"># convert it to string</span>
<span class="go">[u&#39;Click here to go to the Next Page&#39;]</span>
</pre></div>
</div>
<p>So, using the <code class="docutils literal"><span class="pre">.//text()</span></code> node-set won&#8217;t select anything in this case:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(.//text(), &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But using the <code class="docutils literal"><span class="pre">.</span></code> to mean the node, works:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(., &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="beware-of-the-difference-between-node-1-and-node-1">
<h6>Beware of the difference between //node[1] and (//node)[1]<a class="headerlink" href="#beware-of-the-difference-between-node-1-and-node-1" title="Permalink to this headline">¶</a></h6>
<p><code class="docutils literal"><span class="pre">//node[1]</span></code> selects all the nodes occurring first under their respective parents.</p>
<p><code class="docutils literal"><span class="pre">(//node)[1]</span></code> selects all the nodes in the document, and then gets only the first of them.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="go">....:     &lt;ul class=&quot;list&quot;&gt;</span>
<span class="go">....:         &lt;li&gt;1&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;2&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;3&lt;/li&gt;</span>
<span class="go">....:     &lt;/ul&gt;</span>
<span class="go">....:     &lt;ul class=&quot;list&quot;&gt;</span>
<span class="go">....:         &lt;li&gt;4&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;5&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;6&lt;/li&gt;</span>
<span class="go">....:     &lt;/ul&gt;&quot;&quot;&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  elements under whatever it is its parent:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//li[1]&quot;</span><span class="p">)</span>
<span class="go">[u&#39;&lt;li&gt;1&lt;/li&gt;&#39;, u&#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  element in the whole document:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//li)[1]&quot;</span><span class="p">)</span>
<span class="go">[u&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  elements under an <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code>  parent:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//ul/li[1]&quot;</span><span class="p">)</span>
<span class="go">[u&#39;&lt;li&gt;1&lt;/li&gt;&#39;, u&#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  element under an <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code>  parent in the whole document:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//ul/li)[1]&quot;</span><span class="p">)</span>
<span class="go">[u&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="when-querying-by-class-consider-using-css">
<h6>When querying by class, consider using CSS<a class="headerlink" href="#when-querying-by-class-consider-using-css" title="Permalink to this headline">¶</a></h6>
<p>Because an element can contain multiple CSS classes, the XPath way to select elements
by class is the rather verbose:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>*[contains(concat(&#39; &#39;, normalize-space(@class), &#39; &#39;), &#39; someclass &#39;)]
</pre></div>
</div>
<p>If you use <code class="docutils literal"><span class="pre">&#64;class='someclass'</span></code> you may end up missing elements that have
other classes, and if you just use <code class="docutils literal"><span class="pre">contains(&#64;class,</span> <span class="pre">'someclass')</span></code> to make up
for that you may end up with more elements that you want, if they have a different
class name that shares the string <code class="docutils literal"><span class="pre">someclass</span></code>.</p>
<p>As it turns out, Scrapy selectors allow you to chain selectors, so most of the time
you can just select by class using CSS and then switch to XPath when needed:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;div class=&quot;hero shout&quot;&gt;&lt;time datetime=&quot;2014-07-23 19:00&quot;&gt;Special date&lt;/time&gt;&lt;/div&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.shout&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;./time/@datetime&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;2014-07-23 19:00&#39;]</span>
</pre></div>
</div>
<p>This is cleaner than using the verbose XPath trick shown above. Just remember
to use the <code class="docutils literal"><span class="pre">.</span></code> in the XPath expressions that will follow.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.selector">
<span id="built-in-selectors-reference"></span><span id="topics-selectors-ref"></span><h4>Built-in Selectors reference<a class="headerlink" href="#module-scrapy.selector" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.selector.Selector">
<em class="property">class </em><code class="descclassname">scrapy.selector.</code><code class="descname">Selector</code><span class="sig-paren">(</span><em>response=None</em>, <em>text=None</em>, <em>type=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector" title="Permalink to this definition">¶</a></dt>
<dd><p>An instance of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> is a wrapper over response to select
certain parts of its content.</p>
<p><code class="docutils literal"><span class="pre">response</span></code> is an <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> or an
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> object that will be used for selecting and
extracting data.</p>
<p><code class="docutils literal"><span class="pre">text</span></code> is a unicode string or utf-8 encoded text for cases when a
<code class="docutils literal"><span class="pre">response</span></code> isn&#8217;t available. Using <code class="docutils literal"><span class="pre">text</span></code> and <code class="docutils literal"><span class="pre">response</span></code> together is
undefined behavior.</p>
<p><code class="docutils literal"><span class="pre">type</span></code> defines the selector type, it can be <code class="docutils literal"><span class="pre">&quot;html&quot;</span></code>, <code class="docutils literal"><span class="pre">&quot;xml&quot;</span></code> or <code class="docutils literal"><span class="pre">None</span></code> (default).</p>
<blockquote>
<div><blockquote>
<div><p>If <code class="docutils literal"><span class="pre">type</span></code> is <code class="docutils literal"><span class="pre">None</span></code>, the selector automatically chooses the best type
based on <code class="docutils literal"><span class="pre">response</span></code> type (see below), or defaults to <code class="docutils literal"><span class="pre">&quot;html&quot;</span></code> in case it
is used together with <code class="docutils literal"><span class="pre">text</span></code>.</p>
<p>If <code class="docutils literal"><span class="pre">type</span></code> is <code class="docutils literal"><span class="pre">None</span></code> and a <code class="docutils literal"><span class="pre">response</span></code> is passed, the selector type is
inferred from the response type as follows:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">&quot;html&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> type</li>
<li><code class="docutils literal"><span class="pre">&quot;xml&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> type</li>
<li><code class="docutils literal"><span class="pre">&quot;html&quot;</span></code> for anything else</li>
</ul>
</div></blockquote>
</div></blockquote>
<p>Otherwise, if <code class="docutils literal"><span class="pre">type</span></code> is set, the selector type will be forced and no
detection will occur.</p>
</div></blockquote>
<dl class="method">
<dt id="scrapy.selector.Selector.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Find nodes matching the xpath <code class="docutils literal"><span class="pre">query</span></code> and return the result as a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> interface too.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is a string containing the XPATH query to apply.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience, this method can be called as <code class="docutils literal"><span class="pre">response.xpath()</span></code></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given CSS selector and return a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is a string containing the CSS selector to apply.</p>
<p>In the background, CSS queries are translated into XPath queries using
<a class="reference external" href="https://pypi.python.org/pypi/cssselect/">cssselect</a> library and run <code class="docutils literal"><span class="pre">.xpath()</span></code> method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience this method can be called as <code class="docutils literal"><span class="pre">response.css()</span></code></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.extract">
<code class="descname">extract</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched nodes as a list of unicode strings.
Percent encoded content is unquoted.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.re">
<code class="descname">re</code><span class="sig-paren">(</span><em>regex</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given regex and return a list of unicode strings with the
matches.</p>
<p><code class="docutils literal"><span class="pre">regex</span></code> can be either a compiled regular expression or a string which
will be compiled to a regular expression using <code class="docutils literal"><span class="pre">re.compile(regex)</span></code></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.register_namespace">
<code class="descname">register_namespace</code><span class="sig-paren">(</span><em>prefix</em>, <em>uri</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.register_namespace" title="Permalink to this definition">¶</a></dt>
<dd><p>Register the given namespace to be used in this <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a>.
Without registering namespaces you can&#8217;t select or extract data from
non-standard namespaces. See examples below.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.remove_namespaces">
<code class="descname">remove_namespaces</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.remove_namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all namespaces, allowing to traverse the document using
namespace-less xpaths. See example below.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.__nonzero__">
<code class="descname">__nonzero__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.__nonzero__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal"><span class="pre">True</span></code> if there is any real content selected or <code class="docutils literal"><span class="pre">False</span></code>
otherwise.  In other words, the boolean value of a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> is
given by the contents it selects.</p>
</dd></dl>

</dd></dl>

<div class="section" id="selectorlist-objects">
<h5>SelectorList objects<a class="headerlink" href="#selectorlist-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.selector.SelectorList">
<em class="property">class </em><code class="descclassname">scrapy.selector.</code><code class="descname">SelectorList</code><a class="headerlink" href="#scrapy.selector.SelectorList" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> class is a subclass of the builtin <code class="docutils literal"><span class="pre">list</span></code>
class, which provides a few additional methods.</p>
<dl class="method">
<dt id="scrapy.selector.SelectorList.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal"><span class="pre">.xpath()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><code class="xref py py-meth docutils literal"><span class="pre">Selector.xpath()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal"><span class="pre">.css()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><code class="xref py py-meth docutils literal"><span class="pre">Selector.css()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.extract">
<code class="descname">extract</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal"><span class="pre">.extract()</span></code> method for each element in this list and return
their results flattened, as a list of unicode strings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.re">
<code class="descname">re</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal"><span class="pre">.re()</span></code> method for each element in this list and return
their results flattened, as a list of unicode strings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.__nonzero__">
<code class="descname">__nonzero__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.__nonzero__" title="Permalink to this definition">¶</a></dt>
<dd><p>returns True if the list is not empty, False otherwise.</p>
</dd></dl>

</dd></dl>

<div class="section" id="selector-examples-on-html-response">
<h6>Selector examples on HTML response<a class="headerlink" href="#selector-examples-on-html-response" title="Permalink to this headline">¶</a></h6>
<p>Here&#8217;s a couple of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> examples to illustrate several concepts.
In all cases, we assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instantiated with
a <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> object like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">html_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <code class="docutils literal"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body, returning a list of
<a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract the text of all <code class="docutils literal"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body,
returning a list of unicode strings:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>         <span class="c1"># this includes the h1 tag</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>  <span class="c1"># this excludes the h1 tag</span>
</pre></div>
</div>
</li>
<li><p class="first">Iterate over all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> tags and print their class attribute:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//p&quot;</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;@class&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="selector-examples-on-xml-response">
<h6>Selector examples on XML response<a class="headerlink" href="#selector-examples-on-xml-response" title="Permalink to this headline">¶</a></h6>
<p>Here&#8217;s a couple of examples to illustrate several concepts. In both cases we
assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instantiated with an
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> object like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">xml_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <code class="docutils literal"><span class="pre">&lt;product&gt;</span></code> elements from an XML response body, returning a list
of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//product&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract all prices from a <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> which requires registering
a namespace:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">register_namespace</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;http://base.google.com/ns/1.0&quot;</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//g:price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="removing-namespaces">
<span id="id3"></span><h6>Removing namespaces<a class="headerlink" href="#removing-namespaces" title="Permalink to this headline">¶</a></h6>
<p>When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
<a class="reference internal" href="#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><code class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></code></a> method for that.</p>
<p>Let&#8217;s show an example that illustrates this with GitHub blog atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>$ scrapy shell https://github.com/blog.atom
</pre></div>
</div>
<p>Once in the shell we can try selecting all <code class="docutils literal"><span class="pre">&lt;link&gt;</span></code> objects and see that it
doesn&#8217;t work (because the Atom XML namespace is obfuscating those nodes):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But once we call the <a class="reference internal" href="#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><code class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></code></a> method, all
nodes can be accessed directly by their names:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">remove_namespaces</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> &lt;Selector xpath=&#39;//link&#39; data=u&#39;&lt;link xmlns=&quot;http://www.w3.org/2005/Atom&#39;&gt;,</span>
<span class="go"> ...</span>
</pre></div>
</div>
<p>If you wonder why the namespace removal procedure isn&#8217;t always called by default
instead of having to call it manually, this is because of two reasons, which, in order
of relevance, are:</p>
<ol class="arabic simple">
<li>Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to perform for all
documents crawled by Scrapy</li>
<li>There could be some cases where using namespaces is actually required, in
case some element names clash between namespaces. These cases are very rare
though.</li>
</ol>
</div>
</div>
</div>
</div>
<span id="document-topics/items"></span><div class="section" id="module-scrapy.item">
<span id="items"></span><span id="topics-items"></span><h3>Items<a class="headerlink" href="#module-scrapy.item" title="Permalink to this headline">¶</a></h3>
<p>The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. Scrapy spiders can return the extracted data
as Python dicts. While convenient and familiar, Python dicts lack structure:
it is easy to make a typo in a field name or return inconsistent data,
especially in a larger project with many spiders.</p>
<p>To define common output data format Scrapy provides the <a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> class.
<a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects are simple containers used to collect the scraped data.
They provide a <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dictionary-like</a> API with a convenient syntax for declaring
their available fields.</p>
<p>Various Scrapy components use extra information provided by Items:
exporters look at declared fields to figure out columns to export,
serialization can be customized using Item fields metadata, <code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code>
tracks Item instances to help finding memory leaks
(see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span>Debugging memory leaks with trackref</span></a>), etc.</p>
<div class="section" id="declaring-items">
<span id="topics-items-declaring"></span><h4>Declaring Items<a class="headerlink" href="#declaring-items" title="Permalink to this headline">¶</a></h4>
<p>Items are declared using a simple class definition syntax and <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a>
objects. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">last_updated</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Those familiar with <a class="reference external" href="https://www.djangoproject.com/">Django</a> will notice that Scrapy Items are
declared similar to <a class="reference external" href="https://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a>, except that Scrapy Items are much
simpler as there is no concept of different field types.</p>
</div>
</div>
<div class="section" id="item-fields">
<span id="topics-items-fields"></span><h4>Item Fields<a class="headerlink" href="#item-fields" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects are used to specify metadata for each field. For
example, the serializer function for the <code class="docutils literal"><span class="pre">last_updated</span></code> field illustrated in
the example above.</p>
<p>You can specify any kind of metadata for each field. There is no restriction on
the values accepted by <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects. For this same
reason, there is no reference list of all available metadata keys. Each key
defined in <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects could be used by a different component, and
only those components know about it. You can also define and use any other
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> key in your project too, for your own needs. The main goal of
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.</p>
<p>It&#8217;s important to note that the <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the <a class="reference internal" href="#scrapy.item.Item.fields" title="scrapy.item.Item.fields"><code class="xref py py-attr docutils literal"><span class="pre">Item.fields</span></code></a> attribute.</p>
</div>
<div class="section" id="working-with-items">
<h4>Working with Items<a class="headerlink" href="#working-with-items" title="Permalink to this headline">¶</a></h4>
<p>Here are some examples of common tasks performed with items, using the
<code class="docutils literal"><span class="pre">Product</span></code> item <a class="reference internal" href="#topics-items-declaring"><span>declared above</span></a>. You will
notice the API is very similar to the <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>.</p>
<div class="section" id="creating-items">
<h5>Creating items<a class="headerlink" href="#creating-items" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Desktop PC&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-field-values">
<h5>Getting field values<a class="headerlink" href="#getting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
<span class="go">Desktop PC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="go">Desktop PC</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>
<span class="go">1000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;last_updated&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s1">&#39;not set&#39;</span><span class="p">)</span>
<span class="go">not set</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;lala&#39;</span><span class="p">]</span> <span class="c1"># getting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;lala&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;lala&#39;</span><span class="p">,</span> <span class="s1">&#39;unknown field&#39;</span><span class="p">)</span>
<span class="go">&#39;unknown field&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is name field populated?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is last_updated populated?</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is last_updated a declared field?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;lala&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is lala a declared field?</span>
<span class="go">False</span>
</pre></div>
</div>
</div>
<div class="section" id="setting-field-values">
<h5>Setting field values<a class="headerlink" href="#setting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;today&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="go">today</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;lala&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span> <span class="c1"># setting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-all-populated-values">
<h5>Accessing all populated values<a class="headerlink" href="#accessing-all-populated-values" title="Permalink to this headline">¶</a></h5>
<p>To access all populated values, just use the typical <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;price&#39;, &#39;name&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="go">[(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;Desktop PC&#39;)]</span>
</pre></div>
</div>
</div>
<div class="section" id="other-common-tasks">
<h5>Other common tasks<a class="headerlink" href="#other-common-tasks" title="Permalink to this headline">¶</a></h5>
<p>Copying items:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product2</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product2</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product3</span> <span class="o">=</span> <span class="n">product2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">product3</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
<p>Creating dicts from items:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">dict</span><span class="p">(</span><span class="n">product</span><span class="p">)</span> <span class="c1"># create a dict from all populated values</span>
<span class="go">{&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;}</span>
</pre></div>
</div>
<p>Creating items from dicts:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>
<span class="go">Product(price=1500, name=&#39;Laptop PC&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s1">&#39;lala&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span> <span class="c1"># warning: unknown field in dict</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="extending-items">
<h4>Extending Items<a class="headerlink" href="#extending-items" title="Permalink to this headline">¶</a></h4>
<p>You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DiscountedProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">discount_percent</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">discount_expiration_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SpecificProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">Product</span><span class="o">.</span><span class="n">fields</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">serializer</span><span class="o">=</span><span class="n">my_serializer</span><span class="p">)</span>
</pre></div>
</div>
<p>That adds (or replaces) the <code class="docutils literal"><span class="pre">serializer</span></code> metadata key for the <code class="docutils literal"><span class="pre">name</span></code> field,
keeping all the previously existing metadata values.</p>
</div>
<div class="section" id="item-objects">
<h4>Item objects<a class="headerlink" href="#item-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Item">
<em class="property">class </em><code class="descclassname">scrapy.item.</code><code class="descname">Item</code><span class="sig-paren">(</span><span class="optional">[</span><em>arg</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.item.Item" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item optionally initialized from the given argument.</p>
<p>Items replicate the standard <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>, including its constructor. The
only additional attribute provided by Items is:</p>
<dl class="attribute">
<dt id="scrapy.item.Item.fields">
<code class="descname">fields</code><a class="headerlink" href="#scrapy.item.Item.fields" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing <em>all declared fields</em> for this Item, not only
those populated. The keys are the field names and the values are the
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects used in the <a class="reference internal" href="#topics-items-declaring"><span>Item declaration</span></a>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="field-objects">
<h4>Field objects<a class="headerlink" href="#field-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Field">
<em class="property">class </em><code class="descclassname">scrapy.item.</code><code class="descname">Field</code><span class="sig-paren">(</span><span class="optional">[</span><em>arg</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.item.Field" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> class is just an alias to the built-in <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict</a> class and
doesn&#8217;t provide any extra functionality or attributes. In other words,
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects are plain-old Python dicts. A separate class is used
to support the <a class="reference internal" href="#topics-items-declaring"><span>item declaration syntax</span></a>
based on class attributes.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/loaders"></span><div class="section" id="module-scrapy.loader">
<span id="item-loaders"></span><span id="topics-loaders"></span><h3>Item Loaders<a class="headerlink" href="#module-scrapy.loader" title="Permalink to this headline">¶</a></h3>
<p>Item Loaders provide a convenient mechanism for populating scraped <a class="reference internal" href="index.html#topics-items"><span>Items</span></a>. Even though Items can be populated using their own
dictionary-like API, Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="reference internal" href="index.html#topics-items"><span>Items</span></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<div class="section" id="using-item-loaders-to-populate-items">
<h4>Using Item Loaders to populate items<a class="headerlink" href="#using-item-loaders-to-populate-items" title="Permalink to this headline">¶</a></h4>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with a dict-like object (e.g. Item or dict) or without one, in
which case an Item is automatically instantiated in the Item Loader constructor
using the Item class specified in the <a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_item_class</span></code></a>
attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a class="reference internal" href="index.html#topics-selectors"><span>Selectors</span></a>. You can add more than one value to
the same item field; the Item Loader will know how to &#8220;join&#8221; those values later
using a proper processing function.</p>
<p>Here is a typical Item Loader usage in a <a class="reference internal" href="index.html#topics-spiders"><span>Spider</span></a>, using
the <a class="reference internal" href="index.html#topics-items-declaring"><span>Product item</span></a> declared in the <a class="reference internal" href="index.html#topics-items"><span>Items
chapter</span></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">Product</span>

<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_name&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_title&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;stock&#39;</span><span class="p">,</span> <span class="s1">&#39;p#stock]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s1">&#39;today&#39;</span><span class="p">)</span> <span class="c1"># you can also use literal values</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>By quickly looking at that code, we can see the <code class="docutils literal"><span class="pre">name</span></code> field is being
extracted from two different XPath locations in the page:</p>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_name&quot;]</span></code></li>
<li><code class="docutils literal"><span class="pre">//div[&#64;class=&quot;product_title&quot;]</span></code></li>
</ol>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a> method. This is the
data that will be assigned to the <code class="docutils literal"><span class="pre">name</span></code> field later.</p>
<p>Afterwards, similar calls are used for <code class="docutils literal"><span class="pre">price</span></code> and <code class="docutils literal"><span class="pre">stock</span></code> fields
(the latter using a CSS selector with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> method),
and finally the <code class="docutils literal"><span class="pre">last_update</span></code> field is populated directly with a literal value
(<code class="docutils literal"><span class="pre">today</span></code>) using a different method: <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>.</p>
<p>Finally, when all data is collected, the <a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></code></a> method is
called which actually returns the item populated with the data
previously extracted and collected with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a>, and <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> calls.</p>
</div>
<div class="section" id="input-and-output-processors">
<span id="topics-loaders-processors"></span><h4>Input and Output processors<a class="headerlink" href="#input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it&#8217;s
received (through the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> or
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
<a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></code></a> method is called to populate and get the populated
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object.  That&#8217;s when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.</p>
<p>Let&#8217;s see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span> <span class="c1"># (1)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span> <span class="c1"># (2)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span> <span class="c1"># (3)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span> <span class="c1"># (4)</span>
<span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span> <span class="c1"># (5)</span>
</pre></div>
</div>
<p>So what happens is:</p>
<ol class="arabic simple">
<li>Data from <code class="docutils literal"><span class="pre">xpath1</span></code> is extracted, and passed through the <em>input processor</em> of
the <code class="docutils literal"><span class="pre">name</span></code> field. The result of the input processor is collected and kept in
the Item Loader (but not yet assigned to the item).</li>
<li>Data from <code class="docutils literal"><span class="pre">xpath2</span></code> is extracted, and passed through the same <em>input
processor</em> used in (1). The result of the input processor is appended to the
data collected in (1) (if any).</li>
<li>This case is similar to the previous ones, except that the data is extracted
from the <code class="docutils literal"><span class="pre">css</span></code> CSS selector, and passed through the same <em>input
processor</em> used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</li>
<li>This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.</li>
<li>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the <code class="docutils literal"><span class="pre">name</span></code> field.
The result of the output processor is the value assigned to the <code class="docutils literal"><span class="pre">name</span></code>
field in the item.</li>
</ol>
<p>It&#8217;s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
</div>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, Scrapy comes with some <a class="reference internal" href="#topics-loaders-available-processors"><span>commonly used processors</span></a> built-in for convenience.</p>
</div>
<div class="section" id="declaring-item-loaders">
<h4>Declaring Item Loaders<a class="headerlink" href="#declaring-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>

<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>

    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">unicode</span><span class="o">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c1"># ...</span>
</pre></div>
</div>
<p>As you can see, input processors are declared using the <code class="docutils literal"><span class="pre">_in</span></code> suffix while
output processors are declared using the <code class="docutils literal"><span class="pre">_out</span></code> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_input_processor</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_output_processor</span></code></a> attributes.</p>
</div>
<div class="section" id="declaring-input-and-output-processors">
<span id="topics-loaders-processors-declaring"></span><h4>Declaring Input and Output Processors<a class="headerlink" href="#declaring-input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it&#8217;s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="reference internal" href="index.html#topics-items-fields"><span>Item Field</span></a>
metadata. Here is an example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">TakeFirst</span>
<span class="kn">from</span> <span class="nn">w3lib.html</span> <span class="kn">import</span> <span class="n">remove_tags</span>

<span class="k">def</span> <span class="nf">filter_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">value</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">Join</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">,</span> <span class="n">filter_price</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">TakeFirst</span><span class="p">(),</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">u&#39;Welcome to my&#39;</span><span class="p">,</span> <span class="s1">u&#39;&lt;strong&gt;website&lt;/strong&gt;&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">u&#39;&amp;euro;&#39;</span><span class="p">,</span> <span class="s1">u&#39;&lt;span&gt;1000&lt;/span&gt;&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
<span class="go">{&#39;name&#39;: u&#39;Welcome to my website&#39;, &#39;price&#39;: u&#39;1000&#39;}</span>
</pre></div>
</div>
<p>The precedence order, for both input and output processors, is as follows:</p>
<ol class="arabic simple">
<li>Item Loader field-specific attributes: <code class="docutils literal"><span class="pre">field_in</span></code> and <code class="docutils literal"><span class="pre">field_out</span></code> (most
precedence)</li>
<li>Field metadata (<code class="docutils literal"><span class="pre">input_processor</span></code> and <code class="docutils literal"><span class="pre">output_processor</span></code> key)</li>
<li>Item Loader defaults: <a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_input_processor()</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.default_output_processor()</span></code></a> (least precedence)</li>
</ol>
<p>See also: <a class="reference internal" href="#topics-loaders-extending"><span>Reusing and extending Item Loaders</span></a>.</p>
</div>
<div class="section" id="item-loader-context">
<span id="topics-loaders-context"></span><h4>Item Loader Context<a class="headerlink" href="#item-loader-context" title="Permalink to this headline">¶</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <code class="docutils literal"><span class="pre">parse_length</span></code> which receives a text
value and extracts a length from it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_length</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">loader_context</span><span class="p">):</span>
    <span class="n">unit</span> <span class="o">=</span> <span class="n">loader_context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="c1"># ... length parsing code goes here ...</span>
    <span class="k">return</span> <span class="n">parsed_length</span>
</pre></div>
</div>
<p>By accepting a <code class="docutils literal"><span class="pre">loader_context</span></code> argument the function is explicitly telling
the Item Loader that it&#8217;s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<code class="docutils literal"><span class="pre">parse_length</span></code> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol class="arabic">
<li><p class="first">By modifying the currently active Item Loader context
(<a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal"><span class="pre">context</span></code></a> attribute):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cm&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s1">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code> is one of
them:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">length_out</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">parse_length</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s1">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="itemloader-objects">
<h4>ItemLoader objects<a class="headerlink" href="#itemloader-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.loader.ItemLoader">
<em class="property">class </em><code class="descclassname">scrapy.loader.</code><code class="descname">ItemLoader</code><span class="sig-paren">(</span><span class="optional">[</span><em>item</em>, <em>selector</em>, <em>response</em>, <span class="optional">]</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal"><span class="pre">default_item_class</span></code></a>.</p>
<p>When instantiated with a <cite>selector</cite> or a <cite>response</cite> parameters
the <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> class provides convenient mechanisms for extracting
data from web pages using <a class="reference internal" href="index.html#topics-selectors"><span>selectors</span></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object) &#8211; The item instance to populate using subsequent calls to
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a>,
or <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>.</li>
<li><strong>selector</strong> (<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> object) &#8211; The selector to extract data from, when using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a> (resp. <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a>) or <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_xpath" title="scrapy.loader.ItemLoader.replace_xpath"><code class="xref py py-meth docutils literal"><span class="pre">replace_xpath()</span></code></a>
(resp. <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_css" title="scrapy.loader.ItemLoader.replace_css"><code class="xref py py-meth docutils literal"><span class="pre">replace_css()</span></code></a>) method.</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; The response used to construct the selector using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></code></a>, unless the selector argument is given,
in which case this argument is ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal"><span class="pre">context</span></code></a> attribute).</p>
<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> instances have the following methods:</p>
<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_value">
<code class="descname">get_value</code><span class="sig-paren">(</span><em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process the given <code class="docutils literal"><span class="pre">value</span></code> by the given <code class="docutils literal"><span class="pre">processors</span></code> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
given value using <code class="xref py py-meth docutils literal"><span class="pre">extract_regex()</span></code> method,
applied before processors</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s1">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="go">&#39;FOO`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_value">
<code class="descname">add_value</code><span class="sig-paren">(</span><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process and then add the given <code class="docutils literal"><span class="pre">value</span></code> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">get_value()</span></code></a> by giving the
<code class="docutils literal"><span class="pre">processors</span></code> and <code class="docutils literal"><span class="pre">kwargs</span></code>, and then passed through the
<a class="reference internal" href="#topics-loaders-processors"><span>field input processor</span></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <code class="docutils literal"><span class="pre">field_name</span></code> can be <code class="docutils literal"><span class="pre">None</span></code>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">u&#39;Color TV&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;colours&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">u&#39;white&#39;</span><span class="p">,</span> <span class="s1">u&#39;blue&#39;</span><span class="p">])</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="s1">u&#39;100&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">u&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">u&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">:</span> <span class="s1">u&#39;male&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_value">
<code class="descname">replace_value</code><span class="sig-paren">(</span><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> but replaces the collected data with the
new value instead of adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_xpath">
<code class="descname">get_xpath</code><span class="sig-paren">(</span><em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected XPath region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_xpath">
<code class="descname">add_xpath</code><span class="sig-paren">(</span><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_xpath" title="scrapy.loader.ItemLoader.get_xpath"><code class="xref py py-meth docutils literal"><span class="pre">get_xpath()</span></code></a> for <code class="docutils literal"><span class="pre">kwargs</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>xpath</strong> (<em>str</em>) &#8211; the XPath to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_xpath">
<code class="descname">replace_xpath</code><span class="sig-paren">(</span><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_css">
<code class="descname">get_css</code><span class="sig-paren">(</span><em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</li>
<li><strong>re</strong> (<em>str or compiled regex</em>) &#8211; a regular expression to use for extracting data from the
selected CSS region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_css">
<code class="descname">add_css</code><span class="sig-paren">(</span><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_css" title="scrapy.loader.ItemLoader.get_css"><code class="xref py py-meth docutils literal"><span class="pre">get_css()</span></code></a> for <code class="docutils literal"><span class="pre">kwargs</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>css</strong> (<em>str</em>) &#8211; the CSS selector to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_css">
<code class="descname">replace_css</code><span class="sig-paren">(</span><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.load_item">
<code class="descname">load_item</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.load_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference internal" href="#topics-loaders-processors"><span>output processors</span></a> to get the final value to assign to each
item field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.nested_xpath">
<code class="descname">nested_xpath</code><span class="sig-paren">(</span><em>xpath</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with an xpath selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.nested_css">
<code class="descname">nested_css</code><span class="sig-paren">(</span><em>css</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with a css selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_collected_values">
<code class="descname">get_collected_values</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_collected_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_output_value">
<code class="descname">get_output_value</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_output_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values parsed using the output processor, for the
given field. This method doesn&#8217;t populate or modify the item at all.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_input_processor">
<code class="descname">get_input_processor</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the input processor for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_output_processor">
<code class="descname">get_output_processor</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the output processor for the given field.</p>
</dd></dl>

<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> instances have the following attributes:</p>
<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.item">
<code class="descname">item</code><a class="headerlink" href="#scrapy.loader.ItemLoader.item" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object being parsed by this Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.context">
<code class="descname">context</code><a class="headerlink" href="#scrapy.loader.ItemLoader.context" title="Permalink to this definition">¶</a></dt>
<dd><p>The currently active <a class="reference internal" href="#topics-loaders-context"><span>Context</span></a> of this
Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_item_class">
<code class="descname">default_item_class</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_item_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An Item class (or factory), used to instantiate items when not given in
the constructor.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_input_processor">
<code class="descname">default_input_processor</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default input processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_output_processor">
<code class="descname">default_output_processor</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default output processor to use for those fields which don&#8217;t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_selector_class">
<code class="descname">default_selector_class</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_selector_class" title="Permalink to this definition">¶</a></dt>
<dd><p>The class used to construct the <a class="reference internal" href="#scrapy.loader.ItemLoader.selector" title="scrapy.loader.ItemLoader.selector"><code class="xref py py-attr docutils literal"><span class="pre">selector</span></code></a> of this
<a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.selector">
<code class="descname">selector</code><a class="headerlink" href="#scrapy.loader.ItemLoader.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> object to extract data from.
It&#8217;s either the selector given in the constructor or one created from
the response given in the constructor using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></code></a>. This attribute is meant to be
read-only.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="nested-loaders">
<span id="topics-loaders-nested"></span><h4>Nested Loaders<a class="headerlink" href="#nested-loaders" title="Permalink to this headline">¶</a></h4>
<p>When parsing related values from a subsection of a document, it can be
useful to create nested loaders.  Imagine you&#8217;re extracting details from
a footer of a page that looks something like:</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>&lt;footer&gt;
    &lt;a class=&quot;social&quot; href=&quot;http://facebook.com/whatever&quot;&gt;Like Us&lt;/a&gt;
    &lt;a class=&quot;social&quot; href=&quot;http://twitter.com/whatever&quot;&gt;Follow Us&lt;/a&gt;
    &lt;a class=&quot;email&quot; href=&quot;mailto:whatever@example.com&quot;&gt;Email Us&lt;/a&gt;
&lt;/footer&gt;
</pre></div>
</div>
<p>Without nested loaders, you need to specify the full xpath (or css) for each value
that you wish to extract.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;social&#39;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>Instead, you can create a nested loader with the footer selector and add values
relative to the footer.  The functionality is the same but you avoid repeating
the footer selector.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">footer_loader</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">nested_xpath</span><span class="p">(</span><span class="s1">&#39;//footer&#39;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;social&#39;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="c1"># no need to call footer_loader.load_item()</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>You can nest loaders arbitrarily and they work with either xpath or css selectors.
As a general guideline, use nested loaders when they make your code simpler but do
not go overboard with nesting or your parser can become difficult to read.</p>
</div>
<div class="section" id="reusing-and-extending-item-loaders">
<span id="topics-loaders-extending"></span><h4>Reusing and extending Item Loaders<a class="headerlink" href="#reusing-and-extending-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <code class="docutils literal"><span class="pre">---Plasma</span> <span class="pre">TV---</span></code>) and you don&#8217;t want to end up scraping
those dashes in the final product names.</p>
<p>Here&#8217;s how you can remove those dashes by reusing and extending the default
Product Item Loader (<code class="docutils literal"><span class="pre">ProductLoader</span></code>):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>

<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <code class="docutils literal"><span class="pre">CDATA</span></code> occurrences. Here&#8217;s an example of how to do it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>
<span class="kn">from</span> <span class="nn">myproject.utils.xml</span> <span class="kn">import</span> <span class="n">remove_cdata</span>

<span class="k">class</span> <span class="nc">XmlProductLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_cdata</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>And that&#8217;s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="reference internal" href="#topics-loaders-processors-declaring"><span>Declaring Input and Output Processors</span></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn&#8217;t impose any specific
organization of your Loaders collection - that&#8217;s up to you and your project&#8217;s
needs.</p>
</div>
<div class="section" id="module-scrapy.loader.processors">
<span id="available-built-in-processors"></span><span id="topics-loaders-available-processors"></span><h4>Available built-in processors<a class="headerlink" href="#module-scrapy.loader.processors" title="Permalink to this headline">¶</a></h4>
<p>Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the <a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code></a> (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.</p>
<p>Here is a list of all built-in processors:</p>
<dl class="class">
<dt id="scrapy.loader.processors.Identity">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Identity</code><a class="headerlink" href="#scrapy.loader.processors.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>The simplest processor, which doesn&#8217;t do anything. It returns the original
values unchanged. It doesn&#8217;t receive any constructor arguments, nor does it
accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.TakeFirst">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">TakeFirst</code><a class="headerlink" href="#scrapy.loader.processors.TakeFirst" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first non-null/non-empty value from the values received,
so it&#8217;s typically used as an output processor to single-valued fields.
It doesn&#8217;t receive any constructor arguments, nor does it accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.Join">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Join</code><span class="sig-paren">(</span><em>separator=u' '</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.Join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the values joined with the separator given in the constructor, which
defaults to <code class="docutils literal"><span class="pre">u'</span> <span class="pre">'</span></code>. It doesn&#8217;t accept Loader contexts.</p>
<p>When using the default separator, this processor is equivalent to the
function: <code class="docutils literal"><span class="pre">u'</span> <span class="pre">'.join</span></code></p>
<p>Examples:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">Join</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one two three&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">(</span><span class="s1">&#39;&lt;br&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">u&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.Compose">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Compose</code><span class="sig-paren">(</span><em>*functions</em>, <em>**default_loader_context</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.Compose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.</p>
<p>By default, stop process on <code class="docutils literal"><span class="pre">None</span></code> value. This behaviour can be changed by
passing keyword argument <code class="docutils literal"><span class="pre">stop_on_none=False</span></code>.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">Compose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;world&#39;</span><span class="p">])</span>
<span class="go">&#39;HELLO&#39;</span>
</pre></div>
</div>
<p>Each function can optionally receive a <code class="docutils literal"><span class="pre">loader_context</span></code> parameter. For
those which do, this processor will pass the currently active <a class="reference internal" href="#topics-loaders-context"><span>Loader
context</span></a> through that parameter.</p>
<p>The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the <code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.context()</span></code>
attribute.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.MapCompose">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">MapCompose</code><span class="sig-paren">(</span><em>*functions</em>, <em>**default_loader_context</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.MapCompose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions, similar to the <a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal"><span class="pre">Compose</span></code></a> processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:</p>
<p>The input value of this processor is <em>iterated</em> and the first function is
applied to each element. The results of these function calls (one for each element)
are concatenated to construct a new iterable, which is then used to apply the
second function, and so on, until the last function is applied to each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.</p>
<p>Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return <code class="docutils literal"><span class="pre">None</span></code> in which
case the output of that function is ignored for further processing over the
chain.</p>
<p>This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
<a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code></a> processor is typically used as input processor, since
data is often extracted using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><code class="xref py py-meth docutils literal"><span class="pre">extract()</span></code></a> method of <a class="reference internal" href="index.html#topics-selectors"><span>selectors</span></a>, which returns a list of unicode strings.</p>
<p>The example below should clarify how it works:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">filter_world</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;world&#39;</span> <span class="k">else</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">filter_world</span><span class="p">,</span> <span class="nb">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">u&#39;hello&#39;</span><span class="p">,</span> <span class="s1">u&#39;world&#39;</span><span class="p">,</span> <span class="s1">u&#39;this&#39;</span><span class="p">,</span> <span class="s1">u&#39;is&#39;</span><span class="p">,</span> <span class="s1">u&#39;scrapy&#39;</span><span class="p">])</span>
<span class="go">[u&#39;HELLO, u&#39;THIS&#39;, u&#39;IS&#39;, u&#39;SCRAPY&#39;]</span>
</pre></div>
</div>
<p>As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
<a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal"><span class="pre">Compose</span></code></a> processor for more info.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.SelectJmes">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">SelectJmes</code><span class="sig-paren">(</span><em>json_path</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.SelectJmes" title="Permalink to this definition">¶</a></dt>
<dd><p>Queries the value using the json path provided to the constructor and returns the output.
Requires jmespath (<a class="reference external" href="https://github.com/jmespath/jmespath.py">https://github.com/jmespath/jmespath.py</a>) to run.
This processor takes only one input at a time.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="kn">import</span> <span class="n">SelectJmes</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">SelectJmes</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span> <span class="c1">#for direct use on lists and dictionaries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="s1">&#39;bar&#39;</span><span class="p">})</span>
<span class="go">&#39;bar&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="s1">&#39;baz&#39;</span><span class="p">}})</span>
<span class="go">{&#39;bar&#39;: &#39;baz&#39;}</span>
</pre></div>
</div>
<p>Working with Json:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_single_json_str</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">,</span> <span class="n">SelectJmes</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_single_json_str</span><span class="p">(</span><span class="s1">&#39;{&quot;foo&quot;: &quot;bar&quot;}&#39;</span><span class="p">)</span>
<span class="go">u&#39;bar&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_json_list</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">SelectJmes</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_json_list</span><span class="p">(</span><span class="s1">&#39;[{&quot;foo&quot;:&quot;bar&quot;}, {&quot;baz&quot;:&quot;tar&quot;}]&#39;</span><span class="p">)</span>
<span class="go">[u&#39;bar&#39;]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<span id="document-topics/shell"></span><div class="section" id="scrapy-shell">
<span id="topics-shell"></span><h3>Scrapy shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It&#8217;s meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you&#8217;re trying to scrape. It
allows you to interactively test your expressions while you&#8217;re writing your
spider, without having to run the spider to test every change.</p>
<p>Once you get familiarized with the Scrapy shell, you&#8217;ll see that it&#8217;s an
invaluable tool for developing and debugging your spiders.</p>
<div class="section" id="configuring-the-shell">
<h4>Configuring the shell<a class="headerlink" href="#configuring-the-shell" title="Permalink to this headline">¶</a></h4>
<p>If you have <a class="reference external" href="http://ipython.org/">IPython</a> installed, the Scrapy shell will use it (instead of the
standard Python console). The <a class="reference external" href="http://ipython.org/">IPython</a> console is much more powerful and
provides smart auto-completion and colorized output, among other things.</p>
<p>We highly recommend you install <a class="reference external" href="http://ipython.org/">IPython</a>, specially if you&#8217;re working on
Unix systems (where <a class="reference external" href="http://ipython.org/">IPython</a> excels). See the <a class="reference external" href="http://ipython.org/install.html">IPython installation guide</a>
for more info.</p>
<p>Scrapy also has support for <a class="reference external" href="http://www.bpython-interpreter.org/">bpython</a>, and will try to use it where <a class="reference external" href="http://ipython.org/">IPython</a>
is unavailable.</p>
<p>Through scrapy&#8217;s settings you can configure it to use any one of
<code class="docutils literal"><span class="pre">ipython</span></code>, <code class="docutils literal"><span class="pre">bpython</span></code> or the standard <code class="docutils literal"><span class="pre">python</span></code> shell, regardless of which
are installed. This is done by setting the <code class="docutils literal"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> environment
variable; or by defining it in your <a class="reference internal" href="index.html#topics-config-settings"><span>scrapy.cfg</span></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">shell</span> <span class="o">=</span> <span class="n">bpython</span>
</pre></div>
</div>
</div>
<div class="section" id="launch-the-shell">
<h4>Launch the shell<a class="headerlink" href="#launch-the-shell" title="Permalink to this headline">¶</a></h4>
<p>To launch the Scrapy shell you can use the <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> command like
this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy shell &lt;url&gt;
</pre></div>
</div>
<p>Where the <code class="docutils literal"><span class="pre">&lt;url&gt;</span></code> is the URL you want to scrape.</p>
<p><a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> also works for local files. This can be handy if you want
to play around with a local copy of a web page. <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> understands
the following syntaxes for local files:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span># UNIX-style
scrapy shell ./path/to/file.html
scrapy shell ../other/path/to/file.html
scrapy shell /absolute/path/to/file.html

# File URI
scrapy shell file:///absolute/path/to/file.html
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>When using relative file paths, be explicit and prepend them
with <code class="docutils literal"><span class="pre">./</span></code> (or <code class="docutils literal"><span class="pre">../</span></code> when relevant).
<code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> will not work as one might expect (and
this is by design, not a bug).</p>
<p>Because <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> favors HTTP URLs over File URIs,
and <code class="docutils literal"><span class="pre">index.html</span></code> being syntactically similar to <code class="docutils literal"><span class="pre">example.com</span></code>,
<a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> will treat <code class="docutils literal"><span class="pre">index.html</span></code> as a domain name and trigger
a DNS lookup error:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>$ scrapy shell index.html
[ ... scrapy shell starts ... ]
[ ... traceback ... ]
twisted.internet.error.DNSLookupError: DNS lookup failed:
address &#39;index.html&#39; not found: [Errno -5] No address associated with hostname.
</pre></div>
</div>
<p class="last"><a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> will not test beforehand if a file called <code class="docutils literal"><span class="pre">index.html</span></code>
exists in the current directory. Again, be explicit.</p>
</div>
</div>
<div class="section" id="using-the-shell">
<h4>Using the shell<a class="headerlink" href="#using-the-shell" title="Permalink to this headline">¶</a></h4>
<p>The Scrapy shell is just a regular Python console (or <a class="reference external" href="http://ipython.org/">IPython</a> console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<div class="section" id="available-shortcuts">
<h5>Available Shortcuts<a class="headerlink" href="#available-shortcuts" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">shelp()</span></code> - print a help with the list of available objects and shortcuts</li>
<li><code class="docutils literal"><span class="pre">fetch(request_or_url)</span></code> - fetch a new response from the given request or
URL and update all related objects accordingly.</li>
<li><code class="docutils literal"><span class="pre">view(response)</span></code> - open the given response in your local web browser, for
inspection. This will add a <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt; tag</a> to the response body in order
for external links (such as images and style sheets) to display properly.
Note, however, that this will create a temporary file in your computer,
which won&#8217;t be removed automatically.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="available-scrapy-objects">
<h5>Available Scrapy objects<a class="headerlink" href="#available-scrapy-objects" title="Permalink to this headline">¶</a></h5>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object and the
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (for both HTML and XML
content).</p>
<p>Those objects are:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">crawler</span></code> - the current <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object.</li>
<li><code class="docutils literal"><span class="pre">spider</span></code> - the Spider which is known to handle the URL, or a
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object if there is no spider found for
the current URL</li>
<li><code class="docutils literal"><span class="pre">request</span></code> - a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object of the last fetched
page. You can modify this request using <a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>
or fetch a new request (without leaving the shell) using the <code class="docutils literal"><span class="pre">fetch</span></code>
shortcut.</li>
<li><code class="docutils literal"><span class="pre">response</span></code> - a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object containing the last
fetched page</li>
<li><code class="docutils literal"><span class="pre">settings</span></code> - the current <a class="reference internal" href="index.html#topics-settings"><span>Scrapy settings</span></a></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="example-of-shell-session">
<h4>Example of shell session<a class="headerlink" href="#example-of-shell-session" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s an example of a typical shell session where we start by scraping the
<a class="reference external" href="http://scrapy.org">http://scrapy.org</a> page, and then proceed to scrape the <a class="reference external" href="https://reddit.com">https://reddit.com</a>
page. Finally, we modify the (Reddit) request method to POST and re-fetch it
getting an error. We end the session by typing Ctrl-D (in Unix systems) or
Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.</p>
<p>First, we launch the shell:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>scrapy shell &#39;http://scrapy.org&#39; --nolog
</pre></div>
</div>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you&#8217;ll notice that these lines
all start with the <code class="docutils literal"><span class="pre">[s]</span></code> prefix):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
[s]   item       {}
[s]   request    &lt;GET http://scrapy.org&gt;
[s]   response   &lt;200 http://scrapy.org&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x2bfd650&gt;
[s]   spider     &lt;Spider &#39;default&#39; at 0x20c6f50&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

&gt;&gt;&gt;
</pre></div>
</div>
<p>After that, we can start playing with the objects:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
<span class="go">u&#39;Scrapy | A Fast and Powerful Scraping and Web Crawling Framework&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="s2">&quot;http://reddit.com&quot;</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fb3ed9c9c90&gt;</span>
<span class="go">[s]   item       {}</span>
<span class="go">[s]   request    &lt;GET http://reddit.com&gt;</span>
<span class="go">[s]   response   &lt;200 https://www.reddit.com/&gt;</span>
<span class="go">[s]   settings   &lt;scrapy.settings.Settings object at 0x7fb3ed9c9c10&gt;</span>
<span class="go">[s]   spider     &lt;DefaultSpider &#39;default&#39; at 0x7fb3ecdd3390&gt;</span>
<span class="go">[s] Useful shortcuts:</span>
<span class="go">[s]   shelp()           Shell help (print this help)</span>
<span class="go">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span>
<span class="go">[s]   view(response)    View response in a browser</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[u&#39;reddit: the front page of the internet&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;POST&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
<span class="go">[s] Available Scrapy objects:</span>
<span class="go">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;</span>
<span class="gp">...</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="invoking-the-shell-from-spiders-to-inspect-responses">
<span id="topics-shell-inspect-response"></span><h4>Invoking the shell from spiders to inspect responses<a class="headerlink" href="#invoking-the-shell-from-spiders-to-inspect-responses" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.</p>
<p>This can be achieved by using the <code class="docutils literal"><span class="pre">scrapy.shell.inspect_response</span></code> function.</p>
<p>Here&#8217;s an example of how you would call it from your spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://example.com&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.org&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.net&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># We want to inspect one specific response.</span>
        <span class="k">if</span> <span class="s2">&quot;.org&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="kn">import</span> <span class="n">inspect_response</span>
            <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Rest of parsing code.</span>
</pre></div>
</div>
<p>When you run the spider, you will get something similar to this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>2014-01-23 17:48:31-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://example.com&gt; (referer: None)
2014-01-23 17:48:31-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://example.org&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
...

&gt;&gt;&gt; response.url
&#39;http://example.org&#39;
</pre></div>
</div>
<p>Then, you can check if the extraction code is working:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h1[@class=&quot;fn&quot;]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>Nope, it doesn&#8217;t. So you can open the response in your web browser and see if
it&#8217;s the response you were expecting:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the
crawling:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="o">^</span><span class="n">D</span>
<span class="go">2014-01-23 17:50:03-0400 [scrapy] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>Note that you can&#8217;t use the <code class="docutils literal"><span class="pre">fetch</span></code> shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.</p>
</div>
</div>
<span id="document-topics/item-pipeline"></span><div class="section" id="item-pipeline">
<span id="topics-item-pipeline"></span><h3>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h3>
<p>After an item has been scraped by a spider, it is sent to the Item Pipeline
which processes it through several components that are executed sequentially.</p>
<p>Each item pipeline component (sometimes referred as just &#8220;Item Pipeline&#8221;) is a
Python class that implements a simple method. They receive an item and perform
an action over it, also deciding if the item should continue through the
pipeline or be dropped and no longer processed.</p>
<p>Typical uses of item pipelines are:</p>
<ul class="simple">
<li>cleansing HTML data</li>
<li>validating scraped data (checking that the items contain certain fields)</li>
<li>checking for duplicates (and dropping them)</li>
<li>storing the scraped item in a database</li>
</ul>
<div class="section" id="writing-your-own-item-pipeline">
<h4>Writing your own item pipeline<a class="headerlink" href="#writing-your-own-item-pipeline" title="Permalink to this headline">¶</a></h4>
<p>Each item pipeline component is a Python class that must implement the following method:</p>
<dl class="method">
<dt id="process_item">
<code class="descname">process_item</code><span class="sig-paren">(</span><em>self</em>, <em>item</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#process_item" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for every item pipeline component and must either return
a dict with data, <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> (or any descendant class) object
or raise a <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> exception. Dropped items are no longer
processed by further pipeline components.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object or a dict) &#8211; the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which scraped the item</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>Additionally, they may also implement the following methods:</p>
<dl class="method">
<dt id="open_spider">
<code class="descname">open_spider</code><span class="sig-paren">(</span><em>self</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is opened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which was opened</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="close_spider">
<code class="descname">close_spider</code><span class="sig-paren">(</span><em>self</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is closed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which was closed</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>cls</em>, <em>crawler</em><span class="sig-paren">)</span><a class="headerlink" href="#from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a pipeline instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the pipeline. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for pipeline to
access them and hook its functionality into Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object) &#8211; crawler that uses this pipeline</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-pipeline-example">
<h4>Item pipeline example<a class="headerlink" href="#item-pipeline-example" title="Permalink to this headline">¶</a></h4>
<div class="section" id="price-validation-and-dropping-items-with-no-prices">
<h5>Price validation and dropping items with no prices<a class="headerlink" href="#price-validation-and-dropping-items-with-no-prices" title="Permalink to this headline">¶</a></h5>
<p>Let&#8217;s take a look at the following hypothetical pipeline that adjusts the
<code class="docutils literal"><span class="pre">price</span></code> attribute for those items that do not include VAT
(<code class="docutils literal"><span class="pre">price_excludes_vat</span></code> attribute), and drops those items which don&#8217;t
contain a price:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">PricePipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">vat_factor</span> <span class="o">=</span> <span class="mf">1.15</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price_excludes_vat&#39;</span><span class="p">]:</span>
                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat_factor</span>
            <span class="k">return</span> <span class="n">item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Missing price in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="write-items-to-a-json-file">
<h5>Write items to a JSON file<a class="headerlink" href="#write-items-to-a-json-file" title="Permalink to this headline">¶</a></h5>
<p>The following pipeline stores all scraped items (from all spiders) into a
single <code class="docutils literal"><span class="pre">items.jl</span></code> file, containing one item per line serialized in JSON
format:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="k">class</span> <span class="nc">JsonWriterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;items.jl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the <a class="reference internal" href="index.html#topics-feed-exports"><span>Feed exports</span></a>.</p>
</div>
</div>
<div class="section" id="write-items-to-mongodb">
<h5>Write items to MongoDB<a class="headerlink" href="#write-items-to-mongodb" title="Permalink to this headline">¶</a></h5>
<p>In this example we&#8217;ll write items to <a class="reference external" href="https://www.mongodb.org/">MongoDB</a> using <a class="reference external" href="https://api.mongodb.org/python/current/">pymongo</a>.
MongoDB address and database name are specified in Scrapy settings;
MongoDB collection is named after item class.</p>
<p>The main point of this example is to show how to use <a class="reference internal" href="#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a>
method and how to clean up the resources properly.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Previous example (JsonWriterPipeline) doesn&#8217;t clean up resources properly.
Fixing it is left as an exercise for the reader.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">collection_name</span> <span class="o">=</span> <span class="s1">&#39;scrapy_items&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_URI&#39;</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_DATABASE&#39;</span><span class="p">,</span> <span class="s1">&#39;items&#39;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
<div class="section" id="duplicates-filter">
<h5>Duplicates filter<a class="headerlink" href="#duplicates-filter" title="Permalink to this headline">¶</a></h5>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let&#8217;s say that our items have a unique id, but our spider
returns multiples items with the same id:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Duplicate item found: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="activating-an-item-pipeline-component">
<h4>Activating an Item Pipeline component<a class="headerlink" href="#activating-an-item-pipeline-component" title="Permalink to this headline">¶</a></h4>
<p>To activate an Item Pipeline component you must add its class to the
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting, like in the following example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.pipelines.PricePipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;myproject.pipelines.JsonWriterPipeline&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It&#8217;s customary to define these numbers in the 0-1000 range.</p>
</div>
</div>
<span id="document-topics/feed-exports"></span><div class="section" id="feed-exports">
<span id="topics-feed-exports"></span><h3>Feed exports<a class="headerlink" href="#feed-exports" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating an &#8220;export file&#8221; with the scraped data (commonly called &#8220;export
feed&#8221;) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate a feed with the scraped items, using multiple
serialization formats and storage backends.</p>
<div class="section" id="serialization-formats">
<span id="topics-feed-format"></span><h4>Serialization formats<a class="headerlink" href="#serialization-formats" title="Permalink to this headline">¶</a></h4>
<p>For serializing the scraped data, the feed exports use the <a class="reference internal" href="index.html#topics-exporters"><span>Item exporters</span></a>. These formats are supported out of the box:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#topics-feed-format-json"><span>JSON</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-jsonlines"><span>JSON lines</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-csv"><span>CSV</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-xml"><span>XML</span></a></li>
</ul>
</div></blockquote>
<p>But you can also extend the supported format through the
<a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></code></a> setting.</p>
<div class="section" id="json">
<span id="topics-feed-format-json"></span><h5>JSON<a class="headerlink" href="#json" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">json</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a></li>
<li>See <a class="reference internal" href="index.html#json-with-large-data"><span>this warning</span></a> if you&#8217;re using JSON with
large feeds.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="json-lines">
<span id="topics-feed-format-jsonlines"></span><h5>JSON lines<a class="headerlink" href="#json-lines" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">jsonlines</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="csv">
<span id="topics-feed-format-csv"></span><h5>CSV<a class="headerlink" href="#csv" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">csv</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></code></a></li>
<li>To specify columns to export and their order use
<a class="reference internal" href="#std:setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORT_FIELDS</span></code></a>. Other feed exporters can also use this
option, but it is important for CSV because unlike many other export
formats CSV uses a fixed header.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="xml">
<span id="topics-feed-format-xml"></span><h5>XML<a class="headerlink" href="#xml" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">xml</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="pickle">
<span id="topics-feed-format-pickle"></span><h5>Pickle<a class="headerlink" href="#pickle" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">pickle</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.PickleItemExporter" title="scrapy.exporters.PickleItemExporter"><code class="xref py py-class docutils literal"><span class="pre">PickleItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="marshal">
<span id="topics-feed-format-marshal"></span><h5>Marshal<a class="headerlink" href="#marshal" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal"><span class="pre">marshal</span></code></li>
<li>Exporter used: <code class="xref py py-class docutils literal"><span class="pre">MarshalItemExporter</span></code></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="storages">
<span id="topics-feed-storage"></span><h4>Storages<a class="headerlink" href="#storages" title="Permalink to this headline">¶</a></h4>
<p>When using the feed exports you define where to store the feed using a <a class="reference external" href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>
(through the <a class="reference internal" href="#std:setting-FEED_URI"><code class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></code></a> setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#topics-feed-storage-fs"><span>Local filesystem</span></a></li>
<li><a class="reference internal" href="#topics-feed-storage-ftp"><span>FTP</span></a></li>
<li><a class="reference internal" href="#topics-feed-storage-s3"><span>S3</span></a> (requires <a class="reference external" href="https://github.com/boto/botocore">botocore</a> or <a class="reference external" href="https://github.com/boto/boto">boto</a>)</li>
<li><a class="reference internal" href="#topics-feed-storage-stdout"><span>Standard output</span></a></li>
</ul>
</div></blockquote>
<p>Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the <a class="reference external" href="https://github.com/boto/botocore">botocore</a>
or <a class="reference external" href="https://github.com/boto/boto">boto</a> library is installed (Scrapy supports <a class="reference external" href="https://github.com/boto/boto">boto</a> only on Python 2).</p>
</div>
<div class="section" id="storage-uri-parameters">
<span id="topics-feed-uri-params"></span><h4>Storage URI parameters<a class="headerlink" href="#storage-uri-parameters" title="Permalink to this headline">¶</a></h4>
<p>The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">%(time)s</span></code> - gets replaced by a timestamp when the feed is being created</li>
<li><code class="docutils literal"><span class="pre">%(name)s</span></code> - gets replaced by the spider name</li>
</ul>
</div></blockquote>
<p>Any other named parameter gets replaced by the spider attribute of the same
name. For example, <code class="docutils literal"><span class="pre">%(site_id)s</span></code> would get replaced by the <code class="docutils literal"><span class="pre">spider.site_id</span></code>
attribute the moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<blockquote>
<div><ul class="simple">
<li>Store in FTP using one directory per spider:<ul>
<li><code class="docutils literal"><span class="pre">ftp://user:password&#64;ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</span></code></li>
</ul>
</li>
<li>Store in S3 using one directory per spider:<ul>
<li><code class="docutils literal"><span class="pre">s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</span></code></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="storage-backends">
<span id="topics-feed-storage-backends"></span><h4>Storage backends<a class="headerlink" href="#storage-backends" title="Permalink to this headline">¶</a></h4>
<div class="section" id="local-filesystem">
<span id="topics-feed-storage-fs"></span><h5>Local filesystem<a class="headerlink" href="#local-filesystem" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in the local filesystem.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal"><span class="pre">file</span></code></li>
<li>Example URI: <code class="docutils literal"><span class="pre">file:///tmp/export.csv</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
<p>Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like <code class="docutils literal"><span class="pre">/tmp/export.csv</span></code>. This only works on Unix
systems though.</p>
</div>
<div class="section" id="ftp">
<span id="topics-feed-storage-ftp"></span><h5>FTP<a class="headerlink" href="#ftp" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in a FTP server.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal"><span class="pre">ftp</span></code></li>
<li>Example URI: <code class="docutils literal"><span class="pre">ftp://user:pass&#64;ftp.example.com/path/to/export.csv</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="s3">
<span id="topics-feed-storage-s3"></span><h5>S3<a class="headerlink" href="#s3" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored on <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal"><span class="pre">s3</span></code></li>
<li>Example URIs:<ul>
<li><code class="docutils literal"><span class="pre">s3://mybucket/path/to/export.csv</span></code></li>
<li><code class="docutils literal"><span class="pre">s3://aws_key:aws_secret&#64;mybucket/path/to/export.csv</span></code></li>
</ul>
</li>
<li>Required external libraries: <a class="reference external" href="https://github.com/boto/botocore">botocore</a> or <a class="reference external" href="https://github.com/boto/boto">boto</a></li>
</ul>
</div></blockquote>
<p>The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID"><code class="xref std std-setting docutils literal"><span class="pre">AWS_ACCESS_KEY_ID</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY"><code class="xref std std-setting docutils literal"><span class="pre">AWS_SECRET_ACCESS_KEY</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="standard-output">
<span id="topics-feed-storage-stdout"></span><h5>Standard output<a class="headerlink" href="#standard-output" title="Permalink to this headline">¶</a></h5>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal"><span class="pre">stdout</span></code></li>
<li>Example URI: <code class="docutils literal"><span class="pre">stdout:</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings used for configuring the feed exports:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_URI"><code class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></code></a> (mandatory)</li>
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_FORMAT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORAGES"><code class="xref std std-setting docutils literal"><span class="pre">FEED_STORAGES</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal"><span class="pre">FEED_STORE_EMPTY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORT_FIELDS</span></code></a></li>
</ul>
</div></blockquote>
<div class="section" id="feed-uri">
<span id="std:setting-FEED_URI"></span><h5>FEED_URI<a class="headerlink" href="#feed-uri" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The URI of the export feed. See <a class="reference internal" href="#topics-feed-storage-backends"><span>Storage backends</span></a> for
supported URI schemes.</p>
<p>This setting is required for enabling the feed exports.</p>
</div>
<div class="section" id="feed-format">
<span id="std:setting-FEED_FORMAT"></span><h5>FEED_FORMAT<a class="headerlink" href="#feed-format" title="Permalink to this headline">¶</a></h5>
<p>The serialization format to be used for the feed. See
<a class="reference internal" href="#topics-feed-format"><span>Serialization formats</span></a> for possible values.</p>
</div>
<div class="section" id="feed-export-fields">
<span id="std:setting-FEED_EXPORT_FIELDS"></span><h5>FEED_EXPORT_FIELDS<a class="headerlink" href="#feed-export-fields" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>A list of fields to export, optional.
Example: <code class="docutils literal"><span class="pre">FEED_EXPORT_FIELDS</span> <span class="pre">=</span> <span class="pre">[&quot;foo&quot;,</span> <span class="pre">&quot;bar&quot;,</span> <span class="pre">&quot;baz&quot;]</span></code>.</p>
<p>Use FEED_EXPORT_FIELDS option to define fields to export and their order.</p>
<p>When FEED_EXPORT_FIELDS is empty or None (default), Scrapy uses fields
defined in dicts or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> subclasses a spider is yielding.</p>
<p>If an exporter requires a fixed set of fields (this is the case for
<a class="reference internal" href="#topics-feed-format-csv"><span>CSV</span></a> export format) and FEED_EXPORT_FIELDS
is empty or None, then Scrapy tries to infer field names from the
exported data - currently it uses field names from the first item.</p>
</div>
<div class="section" id="feed-store-empty">
<span id="std:setting-FEED_STORE_EMPTY"></span><h5>FEED_STORE_EMPTY<a class="headerlink" href="#feed-store-empty" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether to export empty feeds (ie. feeds with no items).</p>
</div>
<div class="section" id="feed-storages">
<span id="std:setting-FEED_STORAGES"></span><h5>FEED_STORAGES<a class="headerlink" href="#feed-storages" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.</p>
</div>
<div class="section" id="feed-storages-base">
<span id="std:setting-FEED_STORAGES_BASE"></span><h5>FEED_STORAGES_BASE<a class="headerlink" href="#feed-storages-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;file&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stdout&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.StdoutFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;s3&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.S3FeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FTPFeedStorage&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed storage backends supported by Scrapy. You
can disable any of these backends by assigning <code class="docutils literal"><span class="pre">None</span></code> to their URI scheme in
<a class="reference internal" href="#std:setting-FEED_STORAGES"><code class="xref std std-setting docutils literal"><span class="pre">FEED_STORAGES</span></code></a>. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">FEED_STORAGES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="feed-exporters">
<span id="std:setting-FEED_EXPORTERS"></span><h5>FEED_EXPORTERS<a class="headerlink" href="#feed-exporters" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing additional exporters supported by your project. The keys are
serialization formats and the values are paths to <a class="reference internal" href="index.html#topics-exporters"><span>Item exporter</span></a> classes.</p>
</div>
<div class="section" id="feed-exporters-base">
<span id="std:setting-FEED_EXPORTERS_BASE"></span><h5>FEED_EXPORTERS_BASE<a class="headerlink" href="#feed-exporters-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;json&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;jsonlines&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;jl&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;csv&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.CsvItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xml&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.XmlItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;marshal&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.MarshalItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pickle&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.PickleItemExporter&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed exporters supported by Scrapy. You can
disable any of these exporters by assigning <code class="docutils literal"><span class="pre">None</span></code> to their serialization
format in <a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></code></a>. E.g., to disable the built-in CSV exporter
(without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">FEED_EXPORTERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;csv&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/request-response"></span><div class="section" id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h3>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="#topics-request-response-ref-request-subclasses"><span>Request subclasses</span></a> and
<a class="reference internal" href="#topics-request-response-ref-response-subclasses"><span>Response subclasses</span></a>.</p>
<div class="section" id="request-objects">
<h4>Request objects<a class="headerlink" href="#request-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Request">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">Request</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>callback</em>, <em>method='GET'</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this request</li>
<li><strong>callback</strong> (<em>callable</em>) &#8211; the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see <a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span>Passing additional data to callback functions</span></a> below.
If a Request doesn&#8217;t specify a callback, the spider&#8217;s
<a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> method will be used.
Note that if exceptions are raised during processing, errback is called instead.</li>
<li><strong>method</strong> (<em>string</em>) &#8211; the HTTP method of this request. Defaults to <code class="docutils literal"><span class="pre">'GET'</span></code>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</li>
<li><strong>body</strong> (<em>str or unicode</em>) &#8211; the request body. If a <code class="docutils literal"><span class="pre">unicode</span></code> is passed, then it&#8217;s encoded to
<code class="docutils literal"><span class="pre">str</span></code> using the <cite>encoding</cite> passed (which defaults to <code class="docutils literal"><span class="pre">utf-8</span></code>). If
<code class="docutils literal"><span class="pre">body</span></code> is not given, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a <code class="docutils literal"><span class="pre">str</span></code> (never
<code class="docutils literal"><span class="pre">unicode</span></code> or <code class="docutils literal"><span class="pre">None</span></code>).</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<code class="docutils literal"><span class="pre">None</span></code> is passed as value, the HTTP header will not be sent at all.</li>
<li><strong>cookies</strong> (<em>dict or list</em>) &#8211; <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic">
<li>Using a dict:<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;currency&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;UY&#39;</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li>Using a list of dicts:<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;currency&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="s1">&#39;example.com&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;path&#39;</span><span class="p">:</span> <span class="s1">&#39;/currency&#39;</span><span class="p">}])</span>
</pre></div>
</div>
</li>
</ol>
<p>The latter form allows for customizing the <code class="docutils literal"><span class="pre">domain</span></code> and <code class="docutils literal"><span class="pre">path</span></code>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That&#8217;s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <code class="docutils literal"><span class="pre">dont_merge_cookies</span></code> key to True in the
<a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a>.</p>
<p>Example of request without merging cookies:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;currency&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;UY&#39;</span><span class="p">},</span>
                               <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;dont_merge_cookies&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</pre></div>
</div>
<p>For more info see <a class="reference internal" href="index.html#cookies-mw"><span>CookiesMiddleware</span></a>.</p>
</li>
<li><strong>encoding</strong> (<em>string</em>) &#8211; the encoding of this request (defaults to <code class="docutils literal"><span class="pre">'utf-8'</span></code>).
This encoding will be used to percent-encode the URL and to convert the
body to <code class="docutils literal"><span class="pre">str</span></code> (if given as <code class="docutils literal"><span class="pre">unicode</span></code>).</li>
<li><strong>priority</strong> (<em>int</em>) &#8211; the priority of this request (defaults to <code class="docutils literal"><span class="pre">0</span></code>).
The priority is used by the scheduler to define the order used to process
requests.  Requests with a higher priority value will execute earlier.
Negative values are allowed in order to indicate relatively low-priority.</li>
<li><strong>dont_filter</strong> (<em>boolean</em>) &#8211; indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <code class="docutils literal"><span class="pre">False</span></code>.</li>
<li><strong>errback</strong> (<em>callable</em>) &#8211; a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance
as first parameter.
For more information,
see <a class="reference internal" href="#topics-request-response-ref-errbacks"><span>Using errbacks to catch exceptions in request processing</span></a> below.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Request.url">
<code class="descname">url</code><a class="headerlink" href="#scrapy.http.Request.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.method">
<code class="descname">method</code><a class="headerlink" href="#scrapy.http.Request.method" title="Permalink to this definition">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <code class="docutils literal"><span class="pre">&quot;GET&quot;</span></code>, <code class="docutils literal"><span class="pre">&quot;POST&quot;</span></code>, <code class="docutils literal"><span class="pre">&quot;PUT&quot;</span></code>, etc</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.http.Request.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.body">
<code class="descname">body</code><a class="headerlink" href="#scrapy.http.Request.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.meta">
<code class="descname">meta</code><a class="headerlink" href="#scrapy.http.Request.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="#topics-request-meta"><span>Request.meta special keys</span></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="https://docs.python.org/2/library/copy.html">shallow copied</a> when the request is cloned using the
<code class="docutils literal"><span class="pre">copy()</span></code> or <code class="docutils literal"><span class="pre">replace()</span></code> methods, and can also be accessed, in your
spider, from the <code class="docutils literal"><span class="pre">response.meta</span></code> attribute.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span>Passing additional data to callback functions</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.replace">
<code class="descname">replace</code><span class="sig-paren">(</span><span class="optional">[</span><em>url</em>, <em>method</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding</em>, <em>dont_filter</em>, <em>callback</em>, <em>errback</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> is copied by default (unless a new value
is given in the <code class="docutils literal"><span class="pre">meta</span></code> argument). See also
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span>Passing additional data to callback functions</span></a>.</p>
</dd></dl>

</dd></dl>

<div class="section" id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h5>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="Permalink to this headline">¶</a></h5>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Visited </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute for that.</p>
<p>Here&#8217;s an example of how to pass an item using this mechanism, to populate
different fields from different pages:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
    <span class="n">item</span><span class="p">[</span><span class="s1">&#39;main_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>
    <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;item&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">request</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">item</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;item&#39;</span><span class="p">]</span>
    <span class="n">item</span><span class="p">[</span><span class="s1">&#39;other_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
<div class="section" id="using-errbacks-to-catch-exceptions-in-request-processing">
<span id="topics-request-response-ref-errbacks"></span><h5>Using errbacks to catch exceptions in request processing<a class="headerlink" href="#using-errbacks-to-catch-exceptions-in-request-processing" title="Permalink to this headline">¶</a></h5>
<p>The errback of a request is a function that will be called when an exception
is raise while processing it.</p>
<p>It receives a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance as first parameter and can be
used to track connection establishment timeouts, DNS errors etc.</p>
<p>Here&#8217;s an example spider logging all errors and catching some specific
errors if needed:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">scrapy.spidermiddlewares.httperror</span> <span class="kn">import</span> <span class="n">HttpError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="kn">import</span> <span class="n">DNSLookupError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="kn">import</span> <span class="n">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span>

<span class="k">class</span> <span class="nc">ErrbackSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;errback_example&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.httpbin.org/&quot;</span><span class="p">,</span>              <span class="c1"># HTTP 200 expected</span>
        <span class="s2">&quot;http://www.httpbin.org/status/404&quot;</span><span class="p">,</span>    <span class="c1"># Not found error</span>
        <span class="s2">&quot;http://www.httpbin.org/status/500&quot;</span><span class="p">,</span>    <span class="c1"># server issue</span>
        <span class="s2">&quot;http://www.httpbin.org:12345/&quot;</span><span class="p">,</span>        <span class="c1"># non-responding host, timeout expected</span>
        <span class="s2">&quot;http://www.httphttpbinbin.org/&quot;</span><span class="p">,</span>       <span class="c1"># DNS error expected</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_httpbin</span><span class="p">,</span>
                                    <span class="n">errback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">errback_httpbin</span><span class="p">,</span>
                                    <span class="n">dont_filter</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Got successful response from {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
        <span class="c1"># do something useful here...</span>

    <span class="k">def</span> <span class="nf">errback_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">failure</span><span class="p">):</span>
        <span class="c1"># log all failures</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">failure</span><span class="p">))</span>

        <span class="c1"># in case you want to do something special for some errors,</span>
        <span class="c1"># you may need the failure&#39;s type:</span>

        <span class="k">if</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">HttpError</span><span class="p">):</span>
            <span class="c1"># these exceptions come from HttpError spider middleware</span>
            <span class="c1"># you can get the non-200 response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">response</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;HttpError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">DNSLookupError</span><span class="p">):</span>
            <span class="c1"># this is the original request</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;DNSLookupError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span><span class="p">):</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;TimeoutError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="request-meta-special-keys">
<span id="topics-request-meta"></span><h4>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_retry"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_retry</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_list"><code class="xref std std-reqmeta docutils literal"><span class="pre">handle_httpstatus_list</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_all"><code class="xref std std-reqmeta docutils literal"><span class="pre">handle_httpstatus_all</span></code></a></li>
<li><code class="docutils literal"><span class="pre">dont_merge_cookies</span></code> (see <code class="docutils literal"><span class="pre">cookies</span></code> parameter of <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> constructor)</li>
<li><a class="reference internal" href="index.html#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_cache</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-bindaddress"><code class="xref std std-reqmeta docutils literal"><span class="pre">bindaddress</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_obey_robotstxt"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_obey_robotstxt</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_maxsize</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-proxy"><code class="xref std std-reqmeta docutils literal"><span class="pre">proxy</span></code></a></li>
</ul>
<div class="section" id="bindaddress">
<span id="std:reqmeta-bindaddress"></span><h5>bindaddress<a class="headerlink" href="#bindaddress" title="Permalink to this headline">¶</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</div>
<div class="section" id="download-timeout">
<span id="std:reqmeta-download_timeout"></span><h5>download_timeout<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h5>
<p>The amount of time (in secs) that the downloader will wait before timing out.
See also: <a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a>.</p>
</div>
</div>
<div class="section" id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h4>Request subclasses<a class="headerlink" href="#request-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of built-in <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<div class="section" id="formrequest-objects">
<h5>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="Permalink to this headline">¶</a></h5>
<p>The FormRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects.</p>
<dl class="class">
<dt id="scrapy.http.FormRequest">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">FormRequest</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>formdata</em>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>formdata</strong> (<em>dict or iterable of tuples</em>) &#8211; is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> objects support the following class method in
addition to the standard <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> methods:</p>
<dl class="classmethod">
<dt id="scrapy.http.FormRequest.from_response">
<em class="property">classmethod </em><code class="descname">from_response</code><span class="sig-paren">(</span><em>response</em><span class="optional">[</span>, <em>formname=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>formcss=None</em>, <em>clickdata=None</em>, <em>dont_click=False</em>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.FormRequest.from_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> object with its form field values
pre-populated with those found in the HTML <code class="docutils literal"><span class="pre">&lt;form&gt;</span></code> element contained
in the given response. For an example see
<a class="reference internal" href="#topics-request-response-ref-request-userlogin"><span>Using FormRequest.from_response() to simulate a user login</span></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <code class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></code>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal"><span class="pre">from_response()</span></code></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<code class="docutils literal"><span class="pre">dont_click</span></code> argument to <code class="docutils literal"><span class="pre">True</span></code>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<code class="docutils literal"><span class="pre">clickdata</span></code> argument.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response containing a HTML form which will be used
to pre-populate the form fields</li>
<li><strong>formname</strong> (<em>string</em>) &#8211; if given, the form with name attribute set to this value will be used.</li>
<li><strong>formxpath</strong> (<em>string</em>) &#8211; if given, the first form that matches the xpath will be used.</li>
<li><strong>formcss</strong> (<em>string</em>) &#8211; if given, the first form that matches the css selector will be used.</li>
<li><strong>formnumber</strong> (<em>integer</em>) &#8211; the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <code class="docutils literal"><span class="pre">0</span></code>.</li>
<li><strong>formdata</strong> (<em>dict</em>) &#8211; fields to override in the form data. If a field was
already present in the response <code class="docutils literal"><span class="pre">&lt;form&gt;</span></code> element, its value is
overridden by the one passed in this parameter.</li>
<li><strong>clickdata</strong> (<em>dict</em>) &#8211; attributes to lookup the control clicked. If it&#8217;s not
given, the form data will be submitted simulating a click on the
first clickable element. In addition to html attributes, the control
can be identified by its zero-based index relative to other
submittable inputs inside the form, via the <code class="docutils literal"><span class="pre">nr</span></code> attribute.</li>
<li><strong>dont_click</strong> (<em>boolean</em>) &#8211; If True, the form data will be submitted without
clicking in any element.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.3: </span>The <code class="docutils literal"><span class="pre">formname</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>The <code class="docutils literal"><span class="pre">formxpath</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.0: </span>The <code class="docutils literal"><span class="pre">formcss</span></code> parameter.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="request-usage-examples">
<h5>Request usage examples<a class="headerlink" href="#request-usage-examples" title="Permalink to this headline">¶</a></h5>
<div class="section" id="using-formrequest-to-send-data-via-http-post">
<h6>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this headline">¶</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> object (from your
spider) like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;John Doe&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;27&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-formrequest-from-response-to-simulate-a-user-login">
<span id="topics-request-response-ref-request-userlogin"></span><h6>Using FormRequest.from_response() to simulate a user login<a class="headerlink" href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this headline">¶</a></h6>
<p>It is usual for web sites to provide pre-populated form fields through <code class="docutils literal"><span class="pre">&lt;input</span>
<span class="pre">type=&quot;hidden&quot;&gt;</span></code> elements, such as session related data or authentication
tokens (for login pages). When scraping, you&#8217;ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal"><span class="pre">FormRequest.from_response()</span></code></a>
method for this job. Here&#8217;s an example spider which uses it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/users/login.php&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;username&#39;</span><span class="p">:</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;password&#39;</span><span class="p">:</span> <span class="s1">&#39;secret&#39;</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># check login succeed before going on</span>
        <span class="k">if</span> <span class="s2">&quot;authentication failed&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Login failed&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="response-objects">
<h4>Response objects<a class="headerlink" href="#response-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Response">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">Response</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>status=200</em>, <em>headers</em>, <em>body</em>, <em>flags</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) &#8211; the URL of this response</li>
<li><strong>headers</strong> (<em>dict</em>) &#8211; the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</li>
<li><strong>status</strong> (<em>integer</em>) &#8211; the HTTP status of the response. Defaults to <code class="docutils literal"><span class="pre">200</span></code>.</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the response body. It must be str, not unicode, unless you&#8217;re
using a encoding-aware <a class="reference internal" href="#topics-request-response-ref-response-subclasses"><span>Response subclass</span></a>, such as
<a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>.</li>
<li><strong>meta</strong> (<em>dict</em>) &#8211; the initial values for the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></code></a> attribute. If
given, the dict will be shallow copied.</li>
<li><strong>flags</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; is a list containing the initial values for the
<a class="reference internal" href="#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><code class="xref py py-attr docutils literal"><span class="pre">Response.flags</span></code></a> attribute. If given, the list will be shallow
copied.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Response.url">
<code class="descname">url</code><a class="headerlink" href="#scrapy.http.Response.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.status">
<code class="descname">status</code><a class="headerlink" href="#scrapy.http.Response.status" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <code class="docutils literal"><span class="pre">200</span></code>,
<code class="docutils literal"><span class="pre">404</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.http.Response.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.body">
<code class="descname">body</code><a class="headerlink" href="#scrapy.http.Response.body" title="Permalink to this definition">¶</a></dt>
<dd><p>The body of this Response. Keep in mind that Response.body
is always a bytes object. If you want the unicode version use
<a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.text</span></code></a> (only available in <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.request">
<code class="descname">request</code><a class="headerlink" href="#scrapy.http.Response.request" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="index.html#topics-downloader-middleware"><span>Downloader Middlewares</span></a>.
In particular, this means that:</p>
<ul class="simple">
<li>HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).</li>
<li>Response.request.url doesn&#8217;t always equal Response.url</li>
<li>This attribute is only available in the spider code, and in the
<a class="reference internal" href="index.html#topics-spider-middleware"><span>Spider Middlewares</span></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="reference internal" href="index.html#std:signal-response_downloaded"><code class="xref std std-signal docutils literal"><span class="pre">response_downloaded</span></code></a> signal.</li>
</ul>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.meta">
<code class="descname">meta</code><a class="headerlink" href="#scrapy.http.Response.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal"><span class="pre">Response.request</span></code></a> object (ie. <code class="docutils literal"><span class="pre">self.request.meta</span></code>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal"><span class="pre">Response.request</span></code></a> attribute, the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></code></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.flags">
<code class="descname">flags</code><a class="headerlink" href="#scrapy.http.Response.flags" title="Permalink to this definition">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <cite>&#8216;cached&#8217;</cite>, <cite>&#8216;redirected</cite>&#8216;, etc. And
they&#8217;re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.replace">
<code class="descname">replace</code><span class="sig-paren">(</span><span class="optional">[</span><em>url</em>, <em>status</em>, <em>headers</em>, <em>body</em>, <em>request</em>, <em>flags</em>, <em>cls</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></code></a> is copied by default.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.urljoin">
<code class="descname">urljoin</code><span class="sig-paren">(</span><em>url</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.urljoin" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs an absolute url by combining the Response&#8217;s <a class="reference internal" href="#scrapy.http.Response.url" title="scrapy.http.Response.url"><code class="xref py py-attr docutils literal"><span class="pre">url</span></code></a> with
a possible relative url.</p>
<p>This is a wrapper over <a class="reference external" href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin">urlparse.urljoin</a>, it&#8217;s merely an alias for
making this call:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">urlparse</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h4>Response subclasses<a class="headerlink" href="#response-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<div class="section" id="textresponse-objects">
<h5>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.TextResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">TextResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>encoding</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects adds encoding capabilities to the base
<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> class and is not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>encoding</strong> (<em>string</em>) &#8211; is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If <code class="docutils literal"><span class="pre">encoding</span></code> is <code class="docutils literal"><span class="pre">None</span></code> (default value), the
encoding will be looked up in the response headers and body instead.</td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> ones:</p>
<dl class="attribute">
<dt id="scrapy.http.TextResponse.text">
<code class="descname">text</code><a class="headerlink" href="#scrapy.http.TextResponse.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Response body, as unicode.</p>
<p>The same as <code class="docutils literal"><span class="pre">response.body.decode(response.encoding)</span></code>, but the
result is cached after the first call, so you can access
<code class="docutils literal"><span class="pre">response.text</span></code> multiple times without extra overhead.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">unicode(response.body)</span></code> is not a correct way to convert response
body to unicode: you would be using the system default encoding
(typically <cite>ascii</cite>) instead of the response encoding.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.encoding">
<code class="descname">encoding</code><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li>the encoding passed in the constructor <cite>encoding</cite> argument</li>
<li>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.</li>
<li>the encoding declared in the response body. The TextResponse class
doesn&#8217;t provide any special functionality for this. However, the
<a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> and <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> classes do.</li>
<li>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</li>
</ol>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.selector">
<code class="descname">selector</code><a class="headerlink" href="#scrapy.http.TextResponse.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instance using the response as
target. The selector is lazily instantiated on first access.</p>
</dd></dl>

<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support the following methods in addition to
the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> ones:</p>
<dl class="method">
<dt id="scrapy.http.TextResponse.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal"><span class="pre">TextResponse.selector.xpath(query)</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.css" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal"><span class="pre">TextResponse.selector.css(query)</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.body_as_unicode">
<code class="descname">body_as_unicode</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.body_as_unicode" title="Permalink to this definition">¶</a></dt>
<dd><p>The same as <a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal"><span class="pre">text</span></code></a>, but available as a method. This method is
kept for backwards compatibility; please prefer <code class="docutils literal"><span class="pre">response.text</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="htmlresponse-objects">
<h5>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.HtmlResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">HtmlResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="http://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="xmlresponse-objects">
<h5>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.XmlResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">XmlResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/link-extractors"></span><div class="section" id="link-extractors">
<span id="topics-link-extractors"></span><h3>Link Extractors<a class="headerlink" href="#link-extractors" title="Permalink to this headline">¶</a></h3>
<p>Link extractors are objects whose only purpose is to extract links from web
pages (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></code></a> objects) which will be eventually
followed.</p>
<p>There is <code class="docutils literal"><span class="pre">scrapy.linkextractors</span> <span class="pre">import</span> <span class="pre">LinkExtractor</span></code> available
in Scrapy, but you can create your own custom Link Extractors to suit your
needs by implementing a simple interface.</p>
<p>The only public method that every link extractor has is <code class="docutils literal"><span class="pre">extract_links</span></code>,
which receives a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object and returns a list
of <code class="xref py py-class docutils literal"><span class="pre">scrapy.link.Link</span></code> objects. Link extractors are meant to be
instantiated once and their <code class="docutils literal"><span class="pre">extract_links</span></code> method called several times
with different responses to extract links to follow.</p>
<p>Link extractors are used in the <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a>
class (available in Scrapy), through a set of rules, but you can also use it in
your spiders, even if you don&#8217;t subclass from
<a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a>, as its purpose is very simple: to
extract links.</p>
<div class="section" id="module-scrapy.linkextractors">
<span id="built-in-link-extractors-reference"></span><span id="topics-link-extractors-ref"></span><h4>Built-in link extractors reference<a class="headerlink" href="#module-scrapy.linkextractors" title="Permalink to this headline">¶</a></h4>
<p>Link extractors classes bundled with Scrapy are provided in the
<a class="reference internal" href="#module-scrapy.linkextractors" title="scrapy.linkextractors: Link extractors classes"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.linkextractors</span></code></a> module.</p>
<p>The default link extractor is <code class="docutils literal"><span class="pre">LinkExtractor</span></code>, which is the same as
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal"><span class="pre">LxmlLinkExtractor</span></code></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
</pre></div>
</div>
<p>There used to be other link extractor classes in previous Scrapy versions,
but they are deprecated now.</p>
<div class="section" id="module-scrapy.linkextractors.lxmlhtml">
<span id="lxmllinkextractor"></span><h5>LxmlLinkExtractor<a class="headerlink" href="#module-scrapy.linkextractors.lxmlhtml" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">
<em class="property">class </em><code class="descclassname">scrapy.linkextractors.lxmlhtml.</code><code class="descname">LxmlLinkExtractor</code><span class="sig-paren">(</span><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>restrict_css=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href'</em>, <em>)</em>, <em>canonicalize=True</em>, <em>unique=True</em>, <em>process_value=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml&#8217;s robust HTMLParser.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</li>
<li><strong>deny</strong> (<em>a regular expression (or list of)</em>) &#8211; a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the <code class="docutils literal"><span class="pre">allow</span></code> parameter. If not
given (or empty) it won&#8217;t exclude any links.</li>
<li><strong>allow_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of string containing
domains which will be considered for extracting the links</li>
<li><strong>deny_domains</strong> (<em>str or list</em>) &#8211; a single value or a list of strings containing
domains which won&#8217;t be considered for extracting the links</li>
<li><strong>deny_extensions</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to the
<code class="docutils literal"><span class="pre">IGNORED_EXTENSIONS</span></code> list defined in the
<a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/__init__.py">scrapy.linkextractors</a> package.</li>
<li><strong>restrict_xpaths</strong> (<em>str or list</em>) &#8211; is an XPath (or list of XPath&#8217;s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.</li>
<li><strong>restrict_css</strong> (<em>str or list</em>) &#8211; a CSS selector (or list of selectors) which defines
regions inside the response where links should be extracted from.
Has the same behaviour as <code class="docutils literal"><span class="pre">restrict_xpaths</span></code>.</li>
<li><strong>tags</strong> (<em>str or list</em>) &#8211; a tag or a list of tags to consider when extracting links.
Defaults to <code class="docutils literal"><span class="pre">('a',</span> <span class="pre">'area')</span></code>.</li>
<li><strong>attrs</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <code class="docutils literal"><span class="pre">tags</span></code>
parameter). Defaults to <code class="docutils literal"><span class="pre">('href',)</span></code></li>
<li><strong>canonicalize</strong> (<em>boolean</em>) &#8211; canonicalize each extracted url (using
scrapy.utils.url.canonicalize_url). Defaults to <code class="docutils literal"><span class="pre">True</span></code>.</li>
<li><strong>unique</strong> (<em>boolean</em>) &#8211; whether duplicate filtering should be applied to extracted
links.</li>
<li><strong>process_value</strong> (<em>callable</em>) &#8211; <p>a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return <code class="docutils literal"><span class="pre">None</span></code> to ignore the link altogether. If not
given, <code class="docutils literal"><span class="pre">process_value</span></code> defaults to <code class="docutils literal"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></code>.</p>
<p>For example, to extract links from this code:</p>
<div class="highlight-html"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;javascript:goToPage(&#39;../other/page.html&#39;); return false&quot;</span><span class="p">&gt;</span>Link text<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>You can use the following function in <code class="docutils literal"><span class="pre">process_value</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;javascript:goToPage\(&#39;(.*?)&#39;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/settings"></span><div class="section" id="settings">
<span id="topics-settings"></span><h3>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="reference internal" href="#topics-settings-ref"><span>Built-in settings reference</span></a>.</p>
<div class="section" id="designating-the-settings">
<span id="topics-settings-module-envvar"></span><h4>Designating the settings<a class="headerlink" href="#designating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>When you use Scrapy, you have to tell it which settings you&#8217;re using. You can
do this by using an environment variable, <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code>.</p>
<p>The value of <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> should be in Python path syntax, e.g.
<code class="docutils literal"><span class="pre">myproject.settings</span></code>. Note that the settings module should be on the
Python <a class="reference external" href="https://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a>.</p>
</div>
<div class="section" id="populating-the-settings">
<h4>Populating the settings<a class="headerlink" href="#populating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Command line options (most precedence)</li>
<li>Settings per-spider</li>
<li>Project settings module</li>
<li>Default settings per-command</li>
<li>Default global settings (less precedence)</li>
</ol>
</div></blockquote>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="reference internal" href="index.html#topics-api-settings"><span>Settings API</span></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<div class="section" id="command-line-options">
<h5>1. Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h5>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <code class="docutils literal"><span class="pre">-s</span></code> (or <code class="docutils literal"><span class="pre">--set</span></code>) command line option.</p>
<p>Example:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>scrapy crawl myspider -s <span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</div>
<div class="section" id="settings-per-spider">
<h5>2. Settings per-spider<a class="headerlink" href="#settings-per-spider" title="Permalink to this headline">¶</a></h5>
<p>Spiders (See the <a class="reference internal" href="index.html#topics-spiders"><span>Spiders</span></a> chapter for reference) can define their
own settings that will take precedence and override the project ones. They can
do so by setting their <a class="reference internal" href="index.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal"><span class="pre">custom_settings</span></code></a> attribute:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>class MySpider<span class="o">(</span>scrapy.Spider<span class="o">)</span>:
    <span class="nv">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="nv">custom_settings</span> <span class="o">=</span> <span class="o">{</span>
        <span class="s1">&#39;SOME_SETTING&#39;</span>: <span class="s1">&#39;some value&#39;</span>,
    <span class="o">}</span>
</pre></div>
</div>
</div>
<div class="section" id="project-settings-module">
<h5>3. Project settings module<a class="headerlink" href="#project-settings-module" title="Permalink to this headline">¶</a></h5>
<p>The project settings module is the standard configuration file for your Scrapy
project, it&#8217;s where most of your custom settings will be populated. For a
standard Scrapy project, this means you&#8217;ll be adding or changing the settings
in the <code class="docutils literal"><span class="pre">settings.py</span></code> file created for your project.</p>
</div>
<div class="section" id="default-settings-per-command">
<h5>4. Default settings per-command<a class="headerlink" href="#default-settings-per-command" title="Permalink to this headline">¶</a></h5>
<p>Each <a class="reference internal" href="index.html#document-topics/commands"><em>Scrapy tool</em></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <code class="docutils literal"><span class="pre">default_settings</span></code> attribute of the command
class.</p>
</div>
<div class="section" id="default-global-settings">
<h5>5. Default global settings<a class="headerlink" href="#default-global-settings" title="Permalink to this headline">¶</a></h5>
<p>The global defaults are located in the <code class="docutils literal"><span class="pre">scrapy.settings.default_settings</span></code>
module and documented in the <a class="reference internal" href="#topics-settings-ref"><span>Built-in settings reference</span></a> section.</p>
</div>
</div>
<div class="section" id="how-to-access-settings">
<h4>How to access settings<a class="headerlink" href="#how-to-access-settings" title="Permalink to this headline">¶</a></h4>
<p>In a spider, the settings are available through <code class="docutils literal"><span class="pre">self.settings</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Existing settings: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">attributes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="docutils literal"><span class="pre">settings</span></code> attribute is set in the base Spider class after the spider
is initialized.  If you want to use the settings before the initialization
(e.g., in your spider&#8217;s <code class="docutils literal"><span class="pre">__init__()</span></code> method), you&#8217;ll need to override the
<a class="reference internal" href="index.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a> method.</p>
</div>
<p>Settings can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal"><span class="pre">scrapy.crawler.Crawler.settings</span></code></a>
attribute of the Crawler that is passed to <code class="docutils literal"><span class="pre">from_crawler</span></code> method in
extensions, middlewares and item pipelines:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_is_enabled</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">log_is_enabled</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;log is enabled!&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s1">&#39;LOG_ENABLED&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>The settings object can be used like a dict (e.g.,
<code class="docutils literal"><span class="pre">settings['LOG_ENABLED']</span></code>), but it&#8217;s usually preferred to extract the setting
in the format you need it to avoid type errors, using one of the methods
provided by the <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> API.</p>
</div>
<div class="section" id="rationale-for-setting-names">
<h4>Rationale for setting names<a class="headerlink" href="#rationale-for-setting-names" title="Permalink to this headline">¶</a></h4>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<code class="docutils literal"><span class="pre">ROBOTSTXT_ENABLED</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_CACHEDIR</span></code>, etc.</p>
</div>
<div class="section" id="built-in-settings-reference">
<span id="topics-settings-ref"></span><h4>Built-in settings reference<a class="headerlink" href="#built-in-settings-reference" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it&#8217;s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<div class="section" id="aws-access-key-id">
<span id="std:setting-AWS_ACCESS_KEY_ID"></span><h5>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><span>S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="aws-secret-access-key">
<span id="std:setting-AWS_SECRET_ACCESS_KEY"></span><h5>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><span>S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="bot-name">
<span id="std:setting-BOT_NAME"></span><h5>BOT_NAME<a class="headerlink" href="#bot-name" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapybot'</span></code></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User-Agent by default, and
also for logging.</p>
<p>It&#8217;s automatically populated with your project name when you create your
project with the <a class="reference internal" href="index.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command.</p>
</div>
<div class="section" id="concurrent-items">
<span id="std:setting-CONCURRENT_ITEMS"></span><h5>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>).</p>
</div>
<div class="section" id="concurrent-requests">
<span id="std:setting-CONCURRENT_REQUESTS"></span><h5>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">16</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
</div>
<div class="section" id="concurrent-requests-per-domain">
<span id="std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h5>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">8</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.</p>
<p>See also: <a class="reference internal" href="index.html#topics-autothrottle"><span>AutoThrottle extension</span></a> and its
<a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> option.</p>
</div>
<div class="section" id="concurrent-requests-per-ip">
<span id="std:setting-CONCURRENT_REQUESTS_PER_IP"></span><h5>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and
<a class="reference internal" href="index.html#topics-autothrottle"><span>AutoThrottle extension</span></a>: if <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>
is non-zero, download delay is enforced per IP, not per domain.</p>
</div>
<div class="section" id="default-item-class">
<span id="std:setting-DEFAULT_ITEM_CLASS"></span><h5>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.item.Item'</span></code></p>
<p>The default class that will be used for instantiating items in the <a class="reference internal" href="index.html#topics-shell"><span>the
Scrapy shell</span></a>.</p>
</div>
<div class="section" id="default-request-headers">
<span id="std:setting-DEFAULT_REQUEST_HEADERS"></span><h5>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default headers used for Scrapy HTTP Requests. They&#8217;re populated in the
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal"><span class="pre">DefaultHeadersMiddleware</span></code></a>.</p>
</div>
<div class="section" id="depth-limit">
<span id="std:setting-DEPTH_LIMIT"></span><h5>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
</div>
<div class="section" id="depth-priority">
<span id="std:setting-DEPTH_PRIORITY"></span><h5>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>An integer that is used to adjust the request priority based on its depth:</p>
<ul class="simple">
<li>if zero (default), no priority adjustment is made from depth</li>
<li><strong>a positive value will decrease the priority, i.e. higher depth
requests will be processed later</strong> ; this is commonly used when doing
breadth-first crawls (BFO)</li>
<li>a negative value will increase priority, i.e., higher depth requests
will be processed sooner (DFO)</li>
</ul>
<p>See also: <a class="reference internal" href="index.html#faq-bfo-dfo"><span>Does Scrapy crawl in breadth-first or depth-first order?</span></a> about tuning Scrapy for BFO or DFO.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This setting adjusts priority <strong>in the opposite way</strong> compared to
other priority settings <a class="reference internal" href="#std:setting-REDIRECT_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_PRIORITY_ADJUST</span></code></a>
and <a class="reference internal" href="#std:setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a>.</p>
</div>
</div>
<div class="section" id="depth-stats">
<span id="std:setting-DEPTH_STATS"></span><h5>DEPTH_STATS<a class="headerlink" href="#depth-stats" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect maximum depth stats.</p>
</div>
<div class="section" id="depth-stats-verbose">
<span id="std:setting-DEPTH_STATS_VERBOSE"></span><h5>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
</div>
<div class="section" id="dnscache-enabled">
<span id="std:setting-DNSCACHE_ENABLED"></span><h5>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable DNS in-memory cache.</p>
</div>
<div class="section" id="dnscache-size">
<span id="std:setting-DNSCACHE_SIZE"></span><h5>DNSCACHE_SIZE<a class="headerlink" href="#dnscache-size" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">10000</span></code></p>
<p>DNS in-memory cache size.</p>
</div>
<div class="section" id="dns-timeout">
<span id="std:setting-DNS_TIMEOUT"></span><h5>DNS_TIMEOUT<a class="headerlink" href="#dns-timeout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">60</span></code></p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.</p>
</div>
<div class="section" id="downloader">
<span id="std:setting-DOWNLOADER"></span><h5>DOWNLOADER<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.Downloader'</span></code></p>
<p>The downloader to use for crawling.</p>
</div>
<div class="section" id="downloader-httpclientfactory">
<span id="std:setting-DOWNLOADER_HTTPCLIENTFACTORY"></span><h5>DOWNLOADER_HTTPCLIENTFACTORY<a class="headerlink" href="#downloader-httpclientfactory" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</span></code></p>
<p>Defines a Twisted <code class="docutils literal"><span class="pre">protocol.ClientFactory</span></code>  class to use for HTTP/1.0
connections (for <code class="docutils literal"><span class="pre">HTTP10DownloadHandler</span></code>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you use Twisted&lt;11.1, or if you really want to use HTTP/1.0
and override <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for <code class="docutils literal"><span class="pre">http(s)</span></code> scheme
accordingly, i.e. to
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</span></code>.</p>
</div>
</div>
<div class="section" id="downloader-clientcontextfactory">
<span id="std:setting-DOWNLOADER_CLIENTCONTEXTFACTORY"></span><h5>DOWNLOADER_CLIENTCONTEXTFACTORY<a class="headerlink" href="#downloader-clientcontextfactory" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</span></code></p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, &#8220;ContextFactory&#8221; is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate verification,
or even enable client-side authentication (and various other things).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p class="last">If you do need remote server certificate verification enabled,
Scrapy also has another context factory class that you can set,
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</span></code>,
which uses the platform&#8217;s certificates to validate remote endpoints.
<strong>This is only available if you use Twisted&gt;=14.0.</strong></p>
</div>
<p>If you do use a custom ContextFactory, make sure it accepts a <code class="docutils literal"><span class="pre">method</span></code>
parameter at init (this is the <code class="docutils literal"><span class="pre">OpenSSL.SSL</span></code> method mapping
<a class="reference internal" href="#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>).</p>
</div>
<div class="section" id="downloader-client-tls-method">
<span id="std:setting-DOWNLOADER_CLIENT_TLS_METHOD"></span><h5>DOWNLOADER_CLIENT_TLS_METHOD<a class="headerlink" href="#downloader-client-tls-method" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'TLS'</span></code></p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">'TLS'</span></code>: maps to OpenSSL&#8217;s <code class="docutils literal"><span class="pre">TLS_method()</span></code> (a.k.a <code class="docutils literal"><span class="pre">SSLv23_method()</span></code>),
which allows protocol negotiation, starting from the highest supported
by the platform; <strong>default, recommended</strong></li>
<li><code class="docutils literal"><span class="pre">'TLSv1.0'</span></code>: this value forces HTTPS connections to use TLS version 1.0 ;
set this if you want the behavior of Scrapy&lt;1.1</li>
<li><code class="docutils literal"><span class="pre">'TLSv1.1'</span></code>: forces TLS version 1.1</li>
<li><code class="docutils literal"><span class="pre">'TLSv1.2'</span></code>: forces TLS version 1.2</li>
<li><code class="docutils literal"><span class="pre">'SSLv3'</span></code>: forces SSL version 3 (<strong>not recommended</strong>)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We recommend that you use PyOpenSSL&gt;=0.13 and Twisted&gt;=0.13
or above (Twisted&gt;=14.0 if you can).</p>
</div>
</div>
<div class="section" id="downloader-middlewares">
<span id="std:setting-DOWNLOADER_MIDDLEWARES"></span><h5>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span>Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-middlewares-base">
<span id="std:setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h5>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;</span><span class="p">:</span> <span class="mi">560</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware&#39;</span><span class="p">:</span> <span class="mi">830</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. Low
orders are closer to the engine, high orders are closer to the downloader. You
should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> instead.  For more info see
<a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span>Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-stats">
<span id="std:setting-DOWNLOADER_STATS"></span><h5>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable downloader stats collection.</p>
</div>
<div class="section" id="download-delay">
<span id="std:setting-DOWNLOAD_DELAY"></span><h5>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same website. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.25</span>    <span class="c1"># 250 ms of delay</span>
</pre></div>
</div>
<p>This setting is also affected by the <a class="reference internal" href="#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>
setting (which is enabled by default). By default, Scrapy doesn&#8217;t wait a fixed
amount of time between requests, but uses a random interval between 0.5 and 1.5
* <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
<p>When <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> is non-zero, delays are enforced
per ip address instead of per domain.</p>
<p>You can also change this setting per spider by setting <code class="docutils literal"><span class="pre">download_delay</span></code>
spider attribute.</p>
</div>
<div class="section" id="download-handlers">
<span id="std:setting-DOWNLOAD_HANDLERS"></span><h5>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for example format.</p>
</div>
<div class="section" id="download-handlers-base">
<span id="std:setting-DOWNLOAD_HANDLERS_BASE"></span><h5>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;file&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;http&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;https&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;s3&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> instead.</p>
<p>You can disable any of these download handlers by assigning <code class="docutils literal"><span class="pre">None</span></code> to their
URI scheme in <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>. E.g., to disable the built-in FTP
handler (without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="download-timeout">
<span id="std:setting-DOWNLOAD_TIMEOUT"></span><h5>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">180</span></code></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This timeout can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_timeout</span></code>
spider attribute and per-request using <a class="reference internal" href="index.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a>
Request.meta key.</p>
</div>
</div>
<div class="section" id="download-maxsize">
<span id="std:setting-DOWNLOAD_MAXSIZE"></span><h5>DOWNLOAD_MAXSIZE<a class="headerlink" href="#download-maxsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <cite>1073741824</cite> (1024MB)</p>
<p>The maximum response size (in bytes) that downloader will download.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note" id="std:reqmeta-download_maxsize">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_maxsize</span></code>
spider attribute and per-request using <a class="reference internal" href="#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_maxsize</span></code></a>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="download-warnsize">
<span id="std:setting-DOWNLOAD_WARNSIZE"></span><h5>DOWNLOAD_WARNSIZE<a class="headerlink" href="#download-warnsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <cite>33554432</cite> (32MB)</p>
<p>The response size (in bytes) that downloader will start to warn.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_warnsize</span></code>
spider attribute and per-request using <code class="xref std std-reqmeta docutils literal"><span class="pre">download_warnsize</span></code>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="dupefilter-class">
<span id="std:setting-DUPEFILTER_CLASS"></span><h5>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.dupefilters.RFPDupeFilter'</span></code></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<code class="docutils literal"><span class="pre">RFPDupeFilter</span></code>) filters based on request fingerprint using
the <code class="docutils literal"><span class="pre">scrapy.utils.request.request_fingerprint</span></code> function. In order to change
the way duplicates are checked you could subclass <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> and
override its <code class="docutils literal"><span class="pre">request_fingerprint</span></code> method. This method should accept
scrapy <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object and return its fingerprint
(a string).</p>
</div>
<div class="section" id="dupefilter-debug">
<span id="std:setting-DUPEFILTER_DEBUG"></span><h5>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>By default, <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> only logs the first duplicate request.
Setting <a class="reference internal" href="#std:setting-DUPEFILTER_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_DEBUG</span></code></a> to <code class="docutils literal"><span class="pre">True</span></code> will make it log all duplicate requests.</p>
</div>
<div class="section" id="editor">
<span id="std:setting-EDITOR"></span><h5>EDITOR<a class="headerlink" href="#editor" title="Permalink to this headline">¶</a></h5>
<p>Default: <cite>depends on the environment</cite></p>
<p>The editor to use for editing spiders with the <a class="reference internal" href="index.html#std:command-edit"><code class="xref std std-command docutils literal"><span class="pre">edit</span></code></a> command. It
defaults to the <code class="docutils literal"><span class="pre">EDITOR</span></code> environment variable, if set. Otherwise, it defaults
to <code class="docutils literal"><span class="pre">vi</span></code> (on Unix systems) or the IDLE editor (on Windows).</p>
</div>
<div class="section" id="extensions">
<span id="std:setting-EXTENSIONS"></span><h5>EXTENSIONS<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
</div>
<div class="section" id="extensions-base">
<span id="std:setting-EXTENSIONS_BASE"></span><h5>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memusage.MemoryUsage&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memdebug.MemoryDebugger&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.closespider.CloseSpider&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.feedexport.FeedExporter&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.logstats.LogStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.spiderstate.SpiderState&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.throttle.AutoThrottle&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the extensions available by default in Scrapy, and their
orders. This setting contains all stable built-in extensions. Keep in mind that
some of them need to be enabled through a setting.</p>
<p>For more information See the <a class="reference internal" href="index.html#topics-extensions"><span>extensions user guide</span></a>
and the <a class="reference internal" href="index.html#topics-extensions-ref"><span>list of available extensions</span></a>.</p>
</div>
<div class="section" id="feed-tempdir">
<span id="std:setting-FEED_TEMPDIR"></span><h5>FEED_TEMPDIR<a class="headerlink" href="#feed-tempdir" title="Permalink to this headline">¶</a></h5>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a class="reference internal" href="index.html#topics-feed-storage-ftp"><span>FTP feed storage</span></a> and
<a class="reference internal" href="index.html#topics-feed-storage-s3"><span>Amazon S3</span></a>.</p>
</div>
<div class="section" id="files-store-s3-acl">
<span id="std:setting-FILES_STORE_S3_ACL"></span><h5>FILES_STORE_S3_ACL<a class="headerlink" href="#files-store-s3-acl" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'private'</span></code></p>
<p>S3-specific access control policy (ACL) for S3 files store.</p>
</div>
<div class="section" id="item-pipelines">
<span id="std:setting-ITEM_PIPELINES"></span><h5>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the item pipelines to use, and their orders. Order values are
arbitrary, but it is customary to define them in the 0-1000 range. Lower orders
process before higher orders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;mybot.pipelines.validate.ValidateMyItem&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;mybot.pipelines.validate.StoreMyItem&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="item-pipelines-base">
<span id="std:setting-ITEM_PIPELINES_BASE"></span><h5>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="reference internal" href="#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> instead.</p>
</div>
<div class="section" id="log-enabled">
<span id="std:setting-LOG_ENABLED"></span><h5>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable logging.</p>
</div>
<div class="section" id="log-encoding">
<span id="std:setting-LOG_ENCODING"></span><h5>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'utf-8'</span></code></p>
<p>The encoding to use for logging.</p>
</div>
<div class="section" id="log-file">
<span id="std:setting-LOG_FILE"></span><h5>LOG_FILE<a class="headerlink" href="#log-file" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>File name to use for logging output. If <code class="docutils literal"><span class="pre">None</span></code>, standard error will be used.</p>
</div>
<div class="section" id="log-format">
<span id="std:setting-LOG_FORMAT"></span><h5>LOG_FORMAT<a class="headerlink" href="#log-format" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'%(asctime)s</span> <span class="pre">[%(name)s]</span> <span class="pre">%(levelname)s:</span> <span class="pre">%(message)s'</span></code></p>
<p>String for formatting log messsages. Refer to the <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">Python logging documentation</a> for the whole list of available
placeholders.</p>
</div>
<div class="section" id="log-dateformat">
<span id="std:setting-LOG_DATEFORMAT"></span><h5>LOG_DATEFORMAT<a class="headerlink" href="#log-dateformat" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'%Y-%m-%d</span> <span class="pre">%H:%M:%S'</span></code></p>
<p>String for formatting date/time, expansion of the <code class="docutils literal"><span class="pre">%(asctime)s</span></code> placeholder
in <a class="reference internal" href="#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a>. Refer to the <a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">Python datetime documentation</a> for the whole list of available
directives.</p>
</div>
<div class="section" id="log-level">
<span id="std:setting-LOG_LEVEL"></span><h5>LOG_LEVEL<a class="headerlink" href="#log-level" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'DEBUG'</span></code></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="reference internal" href="index.html#topics-logging"><span>Logging</span></a>.</p>
</div>
<div class="section" id="log-stdout">
<span id="std:setting-LOG_STDOUT"></span><h5>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal"><span class="pre">True</span></code>, all standard output (and error) of your process will be redirected
to the log. For example if you <code class="docutils literal"><span class="pre">print</span> <span class="pre">'hello'</span></code> it will appear in the Scrapy
log.</p>
</div>
<div class="section" id="memdebug-enabled">
<span id="std:setting-MEMDEBUG_ENABLED"></span><h5>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether to enable memory debugging.</p>
</div>
<div class="section" id="memdebug-notify">
<span id="std:setting-MEMDEBUG_NOTIFY"></span><h5>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="memusage-enabled">
<span id="std:setting-MEMUSAGE_ENABLED"></span><h5>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to enable the memory usage extension that will shutdown the Scrapy
process when it exceeds a memory limit, and also notify by email when that
happened.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-limit-mb">
<span id="std:setting-MEMUSAGE_LIMIT_MB"></span><h5>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-check-interval-seconds">
<span id="std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"></span><h5>MEMUSAGE_CHECK_INTERVAL_SECONDS<a class="headerlink" href="#memusage-check-interval-seconds" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">60.0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>
checks the current memory usage, versus the limits set by
<a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a> and <a class="reference internal" href="#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>,
at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-notify-mail">
<span id="std:setting-MEMUSAGE_NOTIFY_MAIL"></span><h5>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-report">
<span id="std:setting-MEMUSAGE_REPORT"></span><h5>MEMUSAGE_REPORT<a class="headerlink" href="#memusage-report" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to send a memory usage report after each spider has been closed.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span>Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-warning-mb">
<span id="std:setting-MEMUSAGE_WARNING_MB"></span><h5>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
</div>
<div class="section" id="newspider-module">
<span id="std:setting-NEWSPIDER_MODULE"></span><h5>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">''</span></code></p>
<p>Module where to create new spiders using the <a class="reference internal" href="index.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a> command.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="randomize-download-delay">
<span id="std:setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h5>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 and 1.5
* <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="http://www.gnu.org/software/wget/manual/wget.html">wget</a> <code class="docutils literal"><span class="pre">--random-wait</span></code> option.</p>
<p>If <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> is zero (default) this option has no effect.</p>
</div>
<div class="section" id="reactor-threadpool-maxsize">
<span id="std:setting-REACTOR_THREADPOOL_MAXSIZE"></span><h5>REACTOR_THREADPOOL_MAXSIZE<a class="headerlink" href="#reactor-threadpool-maxsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">10</span></code></p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase
this value if you&#8217;re experiencing problems with insufficient blocking IO.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>Defines the maximum times a request can be redirected. After this maximum the
request&#8217;s response is returned as is. We used Firefox default value for the
same task.</p>
</div>
<div class="section" id="redirect-priority-adjust">
<span id="std:setting-REDIRECT_PRIORITY_ADJUST"></span><h5>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">+2</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</span></code></p>
<p>Adjust redirect request priority relative to original request:</p>
<ul class="simple">
<li><strong>a positive priority adjust (default) means higher priority.</strong></li>
<li>a negative priority adjust means lower priority.</li>
</ul>
</div>
<div class="section" id="retry-priority-adjust">
<span id="std:setting-RETRY_PRIORITY_ADJUST"></span><h5>RETRY_PRIORITY_ADJUST<a class="headerlink" href="#retry-priority-adjust" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">-1</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.retry.RetryMiddleware</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<ul class="simple">
<li>a positive priority adjust means higher priority.</li>
<li><strong>a negative priority adjust (default) means lower priority.</strong></li>
</ul>
</div>
<div class="section" id="robotstxt-obey">
<span id="std:setting-ROBOTSTXT_OBEY"></span><h5>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.robotstxt</span></code></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="reference internal" href="index.html#topics-dlmw-robots"><span>RobotsTxtMiddleware</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While the default value is <code class="docutils literal"><span class="pre">False</span></code> for historical reasons,
this option is enabled by default in settings.py file generated
by <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span></code> command.</p>
</div>
</div>
<div class="section" id="scheduler">
<span id="std:setting-SCHEDULER"></span><h5>SCHEDULER<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></code></p>
<p>The scheduler to use for crawling.</p>
</div>
<div class="section" id="spider-contracts">
<span id="std:setting-SPIDER_CONTRACTS"></span><h5>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a class="reference internal" href="index.html#topics-contracts"><span>Spiders Contracts</span></a>.</p>
</div>
<div class="section" id="spider-contracts-base">
<span id="std:setting-SPIDER_CONTRACTS_BASE"></span><h5>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.UrlContract&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ReturnsContract&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>
instead. For more info see <a class="reference internal" href="index.html#topics-contracts"><span>Spiders Contracts</span></a>.</p>
<p>You can disable any of these contracts by assigning <code class="docutils literal"><span class="pre">None</span></code> to their class
path in <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>. E.g., to disable the built-in
<code class="docutils literal"><span class="pre">ScrapesContract</span></code>, place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-loader-class">
<span id="std:setting-SPIDER_LOADER_CLASS"></span><h5>SPIDER_LOADER_CLASS<a class="headerlink" href="#spider-loader-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.spiderloader.SpiderLoader'</span></code></p>
<p>The class that will be used for loading spiders, which must implement the
<a class="reference internal" href="index.html#topics-api-spiderloader"><span>SpiderLoader API</span></a>.</p>
</div>
<div class="section" id="spider-middlewares">
<span id="std:setting-SPIDER_MIDDLEWARES"></span><h5>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-spider-middleware-setting"><span>Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-middlewares-base">
<span id="std:setting-SPIDER_MIDDLEWARES_BASE"></span><h5>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the spider middlewares enabled by default in Scrapy, and
their orders. Low orders are closer to the engine, high orders are closer to
the spider. For more info see <a class="reference internal" href="index.html#topics-spider-middleware-setting"><span>Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-modules">
<span id="std:setting-SPIDER_MODULES"></span><h5>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mybot.spiders_prod&#39;</span><span class="p">,</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stats-class">
<span id="std:setting-STATS_CLASS"></span><h5>STATS_CLASS<a class="headerlink" href="#stats-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.statscollectors.MemoryStatsCollector'</span></code></p>
<p>The class to use for collecting stats, who must implement the
<a class="reference internal" href="index.html#topics-api-stats"><span>Stats Collector API</span></a>.</p>
</div>
<div class="section" id="stats-dump">
<span id="std:setting-STATS_DUMP"></span><h5>STATS_DUMP<a class="headerlink" href="#stats-dump" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Dump the <a class="reference internal" href="index.html#topics-stats"><span>Scrapy stats</span></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="reference internal" href="index.html#topics-stats"><span>Stats Collection</span></a>.</p>
</div>
<div class="section" id="statsmailer-rcpts">
<span id="std:setting-STATSMAILER_RCPTS"></span><h5>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<code class="xref py py-class docutils literal"><span class="pre">StatsMailer</span></code> for more info.</p>
</div>
<div class="section" id="telnetconsole-enabled">
<span id="std:setting-TELNETCONSOLE_ENABLED"></span><h5>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>A boolean which specifies if the <a class="reference internal" href="index.html#topics-telnetconsole"><span>telnet console</span></a>
will be enabled (provided its extension is also enabled).</p>
</div>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal"><span class="pre">None</span></code> or <code class="docutils literal"><span class="pre">0</span></code>, a
dynamically assigned port is used. For more info see
<a class="reference internal" href="index.html#topics-telnetconsole"><span>Telnet Console</span></a>.</p>
</div>
<div class="section" id="templates-dir">
<span id="std:setting-TEMPLATES_DIR"></span><h5>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">templates</span></code> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="reference internal" href="index.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command and new spiders with <a class="reference internal" href="index.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a>
command.</p>
<p>The project name must not conflict with the name of custom files or directories
in the <code class="docutils literal"><span class="pre">project</span></code> subdirectory.</p>
</div>
<div class="section" id="urllength-limit">
<span id="std:setting-URLLENGTH_LIMIT"></span><h5>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">2083</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">spidermiddlewares.urllength</span></code></p>
<p>The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: <a class="reference external" href="http://www.boutell.com/newfaq/misc/urllength.html">http://www.boutell.com/newfaq/misc/urllength.html</a></p>
</div>
<div class="section" id="user-agent">
<span id="std:setting-USER_AGENT"></span><h5>USER_AGENT<a class="headerlink" href="#user-agent" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)&quot;</span></code></p>
<p>The default User-Agent to use when crawling, unless overridden.</p>
</div>
<div class="section" id="settings-documented-elsewhere">
<h5>Settings documented elsewhere:<a class="headerlink" href="#settings-documented-elsewhere" title="Permalink to this headline">¶</a></h5>
<p>The following settings are documented elsewhere, please check each specific
case to see how to enable and use them.</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AJAXCRAWL_ENABLED">AJAXCRAWL_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_DEBUG">AUTOTHROTTLE_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED">AUTOTHROTTLE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_MAX_DELAY">AUTOTHROTTLE_MAX_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY">AUTOTHROTTLE_START_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY">AUTOTHROTTLE_TARGET_CONCURRENCY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID">AWS_ACCESS_KEY_ID</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY">AWS_SECRET_ACCESS_KEY</a></li>
<li><a class="reference internal" href="index.html#std:setting-BOT_NAME">BOT_NAME</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT">CLOSESPIDER_ERRORCOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT">CLOSESPIDER_ITEMCOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT">CLOSESPIDER_PAGECOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_TIMEOUT">CLOSESPIDER_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-COMMANDS_MODULE">COMMANDS_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-COMPRESSION_ENABLED">COMPRESSION_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_ITEMS">CONCURRENT_ITEMS</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS">CONCURRENT_REQUESTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN">CONCURRENT_REQUESTS_PER_DOMAIN</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP">CONCURRENT_REQUESTS_PER_IP</a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG">COOKIES_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED">COOKIES_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEFAULT_ITEM_CLASS">DEFAULT_ITEM_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS">DEFAULT_REQUEST_HEADERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT">DEPTH_LIMIT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY">DEPTH_PRIORITY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS">DEPTH_STATS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS_VERBOSE">DEPTH_STATS_VERBOSE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNSCACHE_ENABLED">DNSCACHE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNSCACHE_SIZE">DNSCACHE_SIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNS_TIMEOUT">DNS_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER">DOWNLOADER</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENTCONTEXTFACTORY">DOWNLOADER_CLIENTCONTEXTFACTORY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENT_TLS_METHOD">DOWNLOADER_CLIENT_TLS_METHOD</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_HTTPCLIENTFACTORY">DOWNLOADER_HTTPCLIENTFACTORY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS">DOWNLOADER_STATS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY">DOWNLOAD_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS">DOWNLOAD_HANDLERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS_BASE">DOWNLOAD_HANDLERS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_MAXSIZE">DOWNLOAD_MAXSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT">DOWNLOAD_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_WARNSIZE">DOWNLOAD_WARNSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DUPEFILTER_CLASS">DUPEFILTER_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DUPEFILTER_DEBUG">DUPEFILTER_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-EDITOR">EDITOR</a></li>
<li><a class="reference internal" href="index.html#std:setting-EXTENSIONS">EXTENSIONS</a></li>
<li><a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE">EXTENSIONS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS">FEED_EXPORTERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS_BASE">FEED_EXPORTERS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORT_FIELDS">FEED_EXPORT_FIELDS</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT">FEED_FORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES">FEED_STORAGES</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES_BASE">FEED_STORAGES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORE_EMPTY">FEED_STORE_EMPTY</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_TEMPDIR">FEED_TEMPDIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_URI">FEED_URI</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_EXPIRES">FILES_EXPIRES</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_RESULT_FIELD">FILES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_STORE">FILES_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_STORE_S3_ACL">FILES_STORE_S3_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_URLS_FIELD">FILES_URLS_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_ALWAYS_STORE">HTTPCACHE_ALWAYS_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_DIR">HTTPCACHE_DIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED">HTTPCACHE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_EXPIRATION_SECS">HTTPCACHE_EXPIRATION_SECS</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_GZIP">HTTPCACHE_GZIP</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_HTTP_CODES">HTTPCACHE_IGNORE_HTTP_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_MISSING">HTTPCACHE_IGNORE_MISSING</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_SCHEMES">HTTPCACHE_IGNORE_SCHEMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY">HTTPCACHE_POLICY</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOWED_CODES">HTTPERROR_ALLOWED_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOW_ALL">HTTPERROR_ALLOW_ALL</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPPROXY_AUTH_ENCODING">HTTPPROXY_AUTH_ENCODING</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_EXPIRES">IMAGES_EXPIRES</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_MIN_HEIGHT">IMAGES_MIN_HEIGHT</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_MIN_WIDTH">IMAGES_MIN_WIDTH</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_RESULT_FIELD">IMAGES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_STORE">IMAGES_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS">IMAGES_THUMBS</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_URLS_FIELD">IMAGES_URLS_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES">ITEM_PIPELINES</a></li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES_BASE">ITEM_PIPELINES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT">LOG_DATEFORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED">LOG_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING">LOG_ENCODING</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE">LOG_FILE</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FORMAT">LOG_FORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL">LOG_LEVEL</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT">LOG_STDOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_FROM">MAIL_FROM</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_HOST">MAIL_HOST</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_PASS">MAIL_PASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_PORT">MAIL_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_SSL">MAIL_SSL</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_TLS">MAIL_TLS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_USER">MAIL_USER</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED">MEMDEBUG_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMDEBUG_NOTIFY">MEMDEBUG_NOTIFY</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">MEMUSAGE_CHECK_INTERVAL_SECONDS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED">MEMUSAGE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB">MEMUSAGE_LIMIT_MB</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL">MEMUSAGE_NOTIFY_MAIL</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_REPORT">MEMUSAGE_REPORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB">MEMUSAGE_WARNING_MB</a></li>
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_ENABLED">METAREFRESH_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_MAXDELAY">METAREFRESH_MAXDELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-NEWSPIDER_MODULE">NEWSPIDER_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY">RANDOMIZE_DOWNLOAD_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-REACTOR_THREADPOOL_MAXSIZE">REACTOR_THREADPOOL_MAXSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED">REDIRECT_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_PRIORITY_ADJUST">REDIRECT_PRIORITY_ADJUST</a></li>
<li><a class="reference internal" href="index.html#std:setting-REFERER_ENABLED">REFERER_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_ENABLED">RETRY_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES">RETRY_HTTP_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_PRIORITY_ADJUST">RETRY_PRIORITY_ADJUST</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_TIMES">RETRY_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY">ROBOTSTXT_OBEY</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER">SCHEDULER</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS">SPIDER_CONTRACTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS_BASE">SPIDER_CONTRACTS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_CLASS">SPIDER_LOADER_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES">SPIDER_MIDDLEWARES</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE">SPIDER_MIDDLEWARES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MODULES">SPIDER_MODULES</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS">STATSMAILER_RCPTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATS_CLASS">STATS_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATS_DUMP">STATS_DUMP</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED">TELNETCONSOLE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_HOST">TELNETCONSOLE_HOST</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-TEMPLATES_DIR">TEMPLATES_DIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT">URLLENGTH_LIMIT</a></li>
<li><a class="reference internal" href="index.html#std:setting-USER_AGENT">USER_AGENT</a></li>
</ul>
</div>
</div>
</div>
<span id="document-topics/exceptions"></span><div class="section" id="module-scrapy.exceptions">
<span id="exceptions"></span><span id="topics-exceptions"></span><h3>Exceptions<a class="headerlink" href="#module-scrapy.exceptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="built-in-exceptions-reference">
<span id="topics-exceptions-ref"></span><h4>Built-in Exceptions reference<a class="headerlink" href="#built-in-exceptions-reference" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a list of all exceptions included in Scrapy and their usage.</p>
<div class="section" id="dropitem">
<h5>DropItem<a class="headerlink" href="#dropitem" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.DropItem">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">DropItem</code><a class="headerlink" href="#scrapy.exceptions.DropItem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>.</p>
</div>
<div class="section" id="closespider">
<h5>CloseSpider<a class="headerlink" href="#closespider" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.CloseSpider">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">CloseSpider</code><span class="sig-paren">(</span><em>reason='cancelled'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exceptions.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reason</strong> (<em>str</em>) &#8211; the reason for closing</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;Bandwidth exceeded&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CloseSpider</span><span class="p">(</span><span class="s1">&#39;bandwidth_exceeded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="ignorerequest">
<h5>IgnoreRequest<a class="headerlink" href="#ignorerequest" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.IgnoreRequest">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">IgnoreRequest</code><a class="headerlink" href="#scrapy.exceptions.IgnoreRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.</p>
</div>
<div class="section" id="notconfigured">
<h5>NotConfigured<a class="headerlink" href="#notconfigured" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotConfigured">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">NotConfigured</code><a class="headerlink" href="#scrapy.exceptions.NotConfigured" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by some components to indicate that they will
remain disabled. Those components include:</p>
<blockquote>
<div><ul class="simple">
<li>Extensions</li>
<li>Item pipelines</li>
<li>Downloader middlewares</li>
<li>Spider middlewares</li>
</ul>
</div></blockquote>
<p>The exception must be raised in the component constructor.</p>
</div>
<div class="section" id="notsupported">
<h5>NotSupported<a class="headerlink" href="#notsupported" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotSupported">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">NotSupported</code><a class="headerlink" href="#scrapy.exceptions.NotSupported" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception is raised to indicate an unsupported feature.</p>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><em>Command line tool</em></a></dt>
<dd>Learn about the command-line tool used to manage your Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spiders"><em>Spiders</em></a></dt>
<dd>Write the rules to crawl your websites.</dd>
<dt><a class="reference internal" href="index.html#document-topics/selectors"><em>Selectors</em></a></dt>
<dd>Extract the data from web pages using XPath.</dd>
<dt><a class="reference internal" href="index.html#document-topics/shell"><em>Scrapy shell</em></a></dt>
<dd>Test your extraction code in an interactive environment.</dd>
<dt><a class="reference internal" href="index.html#document-topics/items"><em>Items</em></a></dt>
<dd>Define the data you want to scrape.</dd>
<dt><a class="reference internal" href="index.html#document-topics/loaders"><em>Item Loaders</em></a></dt>
<dd>Populate your items with the extracted data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a></dt>
<dd>Post-process and store your scraped data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/feed-exports"><em>Feed exports</em></a></dt>
<dd>Output your scraped data using different formats and storages.</dd>
<dt><a class="reference internal" href="index.html#document-topics/request-response"><em>Requests and Responses</em></a></dt>
<dd>Understand the classes used to represent HTTP requests and responses.</dd>
<dt><a class="reference internal" href="index.html#document-topics/link-extractors"><em>Link Extractors</em></a></dt>
<dd>Convenient classes to extract links to follow from pages.</dd>
<dt><a class="reference internal" href="index.html#document-topics/settings"><em>Settings</em></a></dt>
<dd>Learn how to configure Scrapy and see all <a class="reference internal" href="index.html#topics-settings-ref"><span>available settings</span></a>.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exceptions"><em>Exceptions</em></a></dt>
<dd>See all available exceptions and their meaning.</dd>
</dl>
</div>
<div class="section" id="built-in-services">
<h2>Built-in services<a class="headerlink" href="#built-in-services" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id3">
<span id="document-topics/logging"></span><div class="section" id="logging">
<span id="topics-logging"></span><h3>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.log</span></code> has been deprecated alongside its functions in favor of
explicit calls to the Python standard logging. Keep reading to learn more
about the new logging system.</p>
</div>
<p>Scrapy uses <a class="reference external" href="https://docs.python.org/2/library/logging.html">Python&#8217;s builtin logging system</a> for event logging. We&#8217;ll
provide some simple examples to get you started, but for more advanced
use-cases it&#8217;s strongly suggested to read thoroughly its documentation.</p>
<p>Logging works out of the box, and can be configured to some extent with the
Scrapy settings listed in <a class="reference internal" href="#topics-logging-settings"><span>Logging settings</span></a>.</p>
<p>Scrapy calls <a class="reference internal" href="#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging"><code class="xref py py-func docutils literal"><span class="pre">scrapy.utils.log.configure_logging()</span></code></a> to set some reasonable
defaults and handle those settings in <a class="reference internal" href="#topics-logging-settings"><span>Logging settings</span></a> when
running commands, so it&#8217;s recommended to manually call it if you&#8217;re running
Scrapy from scripts as described in <a class="reference internal" href="index.html#run-from-script"><span>Run Scrapy from a script</span></a>.</p>
<div class="section" id="log-levels">
<span id="topics-logging-levels"></span><h4>Log levels<a class="headerlink" href="#log-levels" title="Permalink to this headline">¶</a></h4>
<p>Python&#8217;s builtin logging defines 5 different levels to indicate severity on a
given log message. Here are the standard ones, listed in decreasing order:</p>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">logging.CRITICAL</span></code> - for critical errors (highest severity)</li>
<li><code class="docutils literal"><span class="pre">logging.ERROR</span></code> - for regular errors</li>
<li><code class="docutils literal"><span class="pre">logging.WARNING</span></code> - for warning messages</li>
<li><code class="docutils literal"><span class="pre">logging.INFO</span></code> - for informational messages</li>
<li><code class="docutils literal"><span class="pre">logging.DEBUG</span></code> - for debugging messages (lowest severity)</li>
</ol>
</div>
<div class="section" id="how-to-log-messages">
<h4>How to log messages<a class="headerlink" href="#how-to-log-messages" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s a quick example of how to log a message using the <code class="docutils literal"><span class="pre">logging.WARNING</span></code>
level:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>There are shortcuts for issuing log messages on any of the standard 5 levels,
and there&#8217;s also a general <code class="docutils literal"><span class="pre">logging.log</span></code> method which takes a given level as
argument.  If you need so, last example could be rewrote as:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span> <span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On top of that, you can create different &#8220;loggers&#8221; to encapsulate messages (For
example, a common practice it&#8217;s to create different loggers for every module).
These loggers can be configured independently, and they allow hierarchical
constructions.</p>
<p>Last examples use the root logger behind the scenes, which is a top level
logger where all messages are propagated to (unless otherwise specified). Using
<code class="docutils literal"><span class="pre">logging</span></code> helpers is merely a shortcut for getting the root logger
explicitly, so this is also an equivalent of last snippets:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can use a different logger just by getting its name with the
<code class="docutils literal"><span class="pre">logging.getLogger</span></code> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;mycustomlogger&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, you can ensure having a custom logger for any module you&#8217;re working on
by using the <code class="docutils literal"><span class="pre">__name__</span></code> variable, which is populated with current module&#8217;s
path:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<dl class="last docutils">
<dt>Module logging, <a class="reference external" href="https://docs.python.org/2/howto/logging.html">HowTo</a></dt>
<dd>Basic Logging Tutorial</dd>
<dt>Module logging, <a class="reference external" href="https://docs.python.org/2/library/logging.html#logger-objects">Loggers</a></dt>
<dd>Further documentation on loggers</dd>
</dl>
</div>
</div>
<div class="section" id="logging-from-spiders">
<span id="topics-logging-from-spiders"></span><h4>Logging from Spiders<a class="headerlink" href="#logging-from-spiders" title="Permalink to this headline">¶</a></h4>
<p>Scrapy provides a <a class="reference internal" href="index.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-data docutils literal"><span class="pre">logger</span></code></a> within each Spider
instance, that can be accessed and used like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://scrapinghub.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Parse function called on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>That logger is created using the Spider&#8217;s name, but you can use any custom
Python logger you want. For example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;mycustomlogger&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://scrapinghub.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Parse function called on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="logging-configuration">
<span id="topics-logging-configuration"></span><h4>Logging configuration<a class="headerlink" href="#logging-configuration" title="Permalink to this headline">¶</a></h4>
<p>Loggers on their own don&#8217;t manage how messages sent through them are displayed.
For this task, different &#8220;handlers&#8221; can be attached to any logger instance and
they will redirect those messages to appropriate destinations, such as the
standard output, files, emails, etc.</p>
<p>By default, Scrapy sets and configures a handler for the root logger, based on
the settings below.</p>
<div class="section" id="logging-settings">
<span id="topics-logging-settings"></span><h5>Logging settings<a class="headerlink" href="#logging-settings" title="Permalink to this headline">¶</a></h5>
<p>These settings can be used to configure the logging:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENCODING</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_DATEFORMAT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_STDOUT</span></code></a></li>
</ul>
<p>First couple of settings define a destination for log messages. If
<a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a> is set, messages sent through the root logger will be
redirected to a file named <a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a> with encoding
<a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENCODING</span></code></a>. If unset and <a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal"><span class="pre">True</span></code>, log
messages will be displayed on the standard error. Lastly, if
<a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal"><span class="pre">False</span></code>, there won&#8217;t be any visible log output.</p>
<p><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></code></a> determines the minimum level of severity to display, those
messages with lower severity will be filtered out. It ranges through the
possible levels listed in <a class="reference internal" href="#topics-logging-levels"><span>Log levels</span></a>.</p>
<p><a class="reference internal" href="index.html#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a> and <a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_DATEFORMAT</span></code></a> specify formatting strings
used as layouts for all messages. Those strings can contain any placeholders
listed in <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">logging&#8217;s logrecord attributes docs</a> and
<a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">datetime&#8217;s strftime and strptime directives</a>
respectively.</p>
</div>
<div class="section" id="command-line-options">
<h5>Command-line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h5>
<p>There are command-line arguments, available for all commands, that you can use
to override some of the Scrapy settings regarding logging.</p>
<ul>
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">--logfile</span> <span class="pre">FILE</span></code></dt>
<dd><p class="first last">Overrides <a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a></p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">--loglevel/-L</span> <span class="pre">LEVEL</span></code></dt>
<dd><p class="first last">Overrides <a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></code></a></p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">--nolog</span></code></dt>
<dd><p class="first last">Sets <a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a> to <code class="docutils literal"><span class="pre">False</span></code></p>
</dd>
</dl>
</li>
</ul>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<dl class="last docutils">
<dt>Module <a class="reference external" href="https://docs.python.org/2/library/logging.handlers.html">logging.handlers</a></dt>
<dd>Further documentation on available handlers</dd>
</dl>
</div>
</div>
</div>
<div class="section" id="module-scrapy.utils.log">
<span id="scrapy-utils-log-module"></span><h4>scrapy.utils.log module<a class="headerlink" href="#module-scrapy.utils.log" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="scrapy.utils.log.configure_logging">
<code class="descclassname">scrapy.utils.log.</code><code class="descname">configure_logging</code><span class="sig-paren">(</span><em>settings=None</em>, <em>install_root_handler=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.log.configure_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize logging defaults for Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>settings</strong> (dict, <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> object or <code class="docutils literal"><span class="pre">None</span></code>) &#8211; settings used to create and configure a handler for the
root logger (default: None).</li>
<li><strong>install_root_handler</strong> (<em>bool</em>) &#8211; whether to install root logging handler
(default: True)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>This function does:</p>
<ul class="simple">
<li>Route warnings and twisted logging through Python standard logging</li>
<li>Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively</li>
<li>Route stdout to log if LOG_STDOUT setting is True</li>
</ul>
<p>When <code class="docutils literal"><span class="pre">install_root_handler</span></code> is True (default), this function also
creates a handler for the root logger according to given settings
(see <a class="reference internal" href="#topics-logging-settings"><span>Logging settings</span></a>). You can override default options
using <code class="docutils literal"><span class="pre">settings</span></code> argument. When <code class="docutils literal"><span class="pre">settings</span></code> is empty or None, defaults
are used.</p>
<p><code class="docutils literal"><span class="pre">configure_logging</span></code> is automatically called when using Scrapy commands,
but needs to be called explicitly when running custom scripts. In that
case, its usage is not required but it&#8217;s recommended.</p>
<p>If you plan on configuring the handlers yourself is still recommended you
call this function, passing <cite>install_root_handler=False</cite>. Bear in mind
there won&#8217;t be any log output set by default in that case.</p>
<p>To get you started on manually configuring logging&#8217;s output, you can use
<a class="reference external" href="https://docs.python.org/2/library/logging.html#logging.basicConfig">logging.basicConfig()</a> to set a basic root handler. This is an example
on how to redirect <code class="docutils literal"><span class="pre">INFO</span></code> or higher messages to a file:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="kn">import</span> <span class="n">configure_logging</span>

<span class="n">configure_logging</span><span class="p">(</span><span class="n">install_root_handler</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;log.txt&#39;</span><span class="p">,</span>
    <span class="n">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(levelname)s</span><span class="s1">: </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Refer to <a class="reference internal" href="index.html#run-from-script"><span>Run Scrapy from a script</span></a> for more details about using Scrapy this
way.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/stats"></span><div class="section" id="stats-collection">
<span id="topics-stats"></span><h3>Stats Collection<a class="headerlink" href="#stats-collection" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal"><span class="pre">stats</span></code></a>
attribute of the <a class="reference internal" href="index.html#topics-api-crawler"><span>Crawler API</span></a>, as illustrated by the examples in
the <a class="reference internal" href="#topics-stats-usecases"><span>Common Stats Collector uses</span></a> section below.</p>
<p>However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it&#8217;s disabled, the API
will still work but it won&#8217;t collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you&#8217;re
using the Stats Collector from.</p>
<p>Another feature of the Stats Collector is that it&#8217;s very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.</p>
<div class="section" id="common-stats-collector-uses">
<span id="topics-stats-usecases"></span><h4>Common Stats Collector uses<a class="headerlink" href="#common-stats-collector-uses" title="Permalink to this headline">¶</a></h4>
<p>Access the stats collector through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal"><span class="pre">stats</span></code></a>
attribute. Here is an example of an extension that access stats:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExtensionThatAccessStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">stats</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="s1">&#39;hostname&#39;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
</pre></div>
</div>
<p>Increment stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">inc_value</span><span class="p">(</span><span class="s1">&#39;custom_count&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if greater than previous:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">max_value</span><span class="p">(</span><span class="s1">&#39;max_items_scraped&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if lower than previous:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">min_value</span><span class="p">(</span><span class="s1">&#39;min_free_memory_percent&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Get stat value:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s1">&#39;custom_count&#39;</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Get all stats:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="go">{&#39;custom_count&#39;: 1, &#39;start_time&#39;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}</span>
</pre></div>
</div>
</div>
<div class="section" id="available-stats-collectors">
<h4>Available Stats Collectors<a class="headerlink" href="#available-stats-collectors" title="Permalink to this headline">¶</a></h4>
<p>Besides the basic <code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code> there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></code></a> setting. The
default Stats Collector used is the <code class="xref py py-class docutils literal"><span class="pre">MemoryStatsCollector</span></code>.</p>
<span class="target" id="module-scrapy.statscollectors"></span><div class="section" id="memorystatscollector">
<h5>MemoryStatsCollector<a class="headerlink" href="#memorystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscollectors.MemoryStatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">MemoryStatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they&#8217;re closed. The stats can be accessed
through the <a class="reference internal" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="scrapy.statscollectors.MemoryStatsCollector.spider_stats"><code class="xref py py-attr docutils literal"><span class="pre">spider_stats</span></code></a> attribute, which is a dict keyed by spider
domain name.</p>
<p>This is the default Stats Collector used in Scrapy.</p>
<dl class="attribute">
<dt id="scrapy.statscollectors.MemoryStatsCollector.spider_stats">
<code class="descname">spider_stats</code><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dummystatscollector">
<h5>DummyStatsCollector<a class="headerlink" href="#dummystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscollectors.DummyStatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">DummyStatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.DummyStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></code></a>
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/email"></span><div class="section" id="module-scrapy.mail">
<span id="sending-e-mail"></span><span id="topics-email"></span><h3>Sending e-mail<a class="headerlink" href="#module-scrapy.mail" title="Permalink to this headline">¶</a></h3>
<p>Although Python makes sending e-mails relatively easy via the <a class="reference external" href="https://docs.python.org/2/library/smtplib.html">smtplib</a>
library, Scrapy provides its own facility for sending e-mails which is very
easy to use and it&#8217;s implemented using <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, to avoid
interfering with the non-blocking IO of the crawler. It also provides a
simple API for sending attachments and it&#8217;s very easy to configure, with a few
<a class="reference internal" href="#topics-email-settings"><span>settings</span></a>.</p>
<div class="section" id="quick-example">
<h4>Quick example<a class="headerlink" href="#quick-example" title="Permalink to this headline">¶</a></h4>
<p>There are two ways to instantiate the mail sender. You can instantiate it using
the standard constructor:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.mail</span> <span class="kn">import</span> <span class="n">MailSender</span>
<span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="p">()</span>
</pre></div>
</div>
<p>Or you can instantiate it passing a Scrapy settings object, which will respect
the <a class="reference internal" href="#topics-email-settings"><span>settings</span></a>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="o">.</span><span class="n">from_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</pre></div>
</div>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">mailer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;someone@example.com&quot;</span><span class="p">],</span> <span class="n">subject</span><span class="o">=</span><span class="s2">&quot;Some subject&quot;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="s2">&quot;Some body&quot;</span><span class="p">,</span> <span class="n">cc</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;another@example.com&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="mailsender-class-reference">
<h4>MailSender class reference<a class="headerlink" href="#mailsender-class-reference" title="Permalink to this headline">¶</a></h4>
<p>MailSender is the preferred class to use for sending emails from Scrapy, as it
uses <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, like the rest of the framework.</p>
<dl class="class">
<dt id="scrapy.mail.MailSender">
<em class="property">class </em><code class="descclassname">scrapy.mail.</code><code class="descname">MailSender</code><span class="sig-paren">(</span><em>smtphost=None</em>, <em>mailfrom=None</em>, <em>smtpuser=None</em>, <em>smtppass=None</em>, <em>smtpport=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>smtphost</strong> (<em>str</em>) &#8211; the SMTP host to use for sending the emails. If omitted, the
<a class="reference internal" href="#std:setting-MAIL_HOST"><code class="xref std std-setting docutils literal"><span class="pre">MAIL_HOST</span></code></a> setting will be used.</li>
<li><strong>mailfrom</strong> (<em>str</em>) &#8211; the address used to send emails (in the <code class="docutils literal"><span class="pre">From:</span></code> header).
If omitted, the <a class="reference internal" href="#std:setting-MAIL_FROM"><code class="xref std std-setting docutils literal"><span class="pre">MAIL_FROM</span></code></a> setting will be used.</li>
<li><strong>smtpuser</strong> &#8211; the SMTP user. If omitted, the <a class="reference internal" href="#std:setting-MAIL_USER"><code class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></code></a>
setting will be used. If not given, no SMTP authentication will be
performed.</li>
<li><strong>smtppass</strong> (<em>str</em>) &#8211; the SMTP pass for authentication.</li>
<li><strong>smtpport</strong> (<em>int</em>) &#8211; the SMTP port to connect to</li>
<li><strong>smtptls</strong> (<em>boolean</em>) &#8211; enforce using SMTP STARTTLS</li>
<li><strong>smtpssl</strong> (<em>boolean</em>) &#8211; enforce using a secure SSL connection</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="scrapy.mail.MailSender.from_settings">
<em class="property">classmethod </em><code class="descname">from_settings</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate using a Scrapy settings object, which will respect
<a class="reference internal" href="#topics-email-settings"><span>these Scrapy settings</span></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></code></a> object) &#8211; the e-mail recipients</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.mail.MailSender.send">
<code class="descname">send</code><span class="sig-paren">(</span><em>to</em>, <em>subject</em>, <em>body</em>, <em>cc=None</em>, <em>attachs=()</em>, <em>mimetype='text/plain'</em>, <em>charset=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.send" title="Permalink to this definition">¶</a></dt>
<dd><p>Send email to the given recipients.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; the e-mail recipients</li>
<li><strong>subject</strong> (<em>str</em>) &#8211; the subject of the e-mail</li>
<li><strong>cc</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; the e-mails to CC</li>
<li><strong>body</strong> (<em>str</em>) &#8211; the e-mail body</li>
<li><strong>attachs</strong> (<em>iterable</em>) &#8211; an iterable of tuples <code class="docutils literal"><span class="pre">(attach_name,</span> <span class="pre">mimetype,</span>
<span class="pre">file_object)</span></code> where  <code class="docutils literal"><span class="pre">attach_name</span></code> is a string with the name that will
appear on the e-mail&#8217;s attachment, <code class="docutils literal"><span class="pre">mimetype</span></code> is the mimetype of the
attachment and <code class="docutils literal"><span class="pre">file_object</span></code> is a readable file object with the
contents of the attachment</li>
<li><strong>mimetype</strong> (<em>str</em>) &#8211; the MIME type of the e-mail</li>
<li><strong>charset</strong> (<em>str</em>) &#8211; the character encoding to use for the e-mail contents</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mail-settings">
<span id="topics-email-settings"></span><h4>Mail settings<a class="headerlink" href="#mail-settings" title="Permalink to this headline">¶</a></h4>
<p>These settings define the default constructor values of the <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal"><span class="pre">MailSender</span></code></a>
class, and can be used to configure e-mail notifications in your project without
writing any code (for those extensions and code that uses <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal"><span class="pre">MailSender</span></code></a>).</p>
<div class="section" id="mail-from">
<span id="std:setting-MAIL_FROM"></span><h5>MAIL_FROM<a class="headerlink" href="#mail-from" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy&#64;localhost'</span></code></p>
<p>Sender email to use (<code class="docutils literal"><span class="pre">From:</span></code> header) for sending emails.</p>
</div>
<div class="section" id="mail-host">
<span id="std:setting-MAIL_HOST"></span><h5>MAIL_HOST<a class="headerlink" href="#mail-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'localhost'</span></code></p>
<p>SMTP host to use for sending emails.</p>
</div>
<div class="section" id="mail-port">
<span id="std:setting-MAIL_PORT"></span><h5>MAIL_PORT<a class="headerlink" href="#mail-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">25</span></code></p>
<p>SMTP port to use for sending emails.</p>
</div>
<div class="section" id="mail-user">
<span id="std:setting-MAIL_USER"></span><h5>MAIL_USER<a class="headerlink" href="#mail-user" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.</p>
</div>
<div class="section" id="mail-pass">
<span id="std:setting-MAIL_PASS"></span><h5>MAIL_PASS<a class="headerlink" href="#mail-pass" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>Password to use for SMTP authentication, along with <a class="reference internal" href="#std:setting-MAIL_USER"><code class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></code></a>.</p>
</div>
<div class="section" id="mail-tls">
<span id="std:setting-MAIL_TLS"></span><h5>MAIL_TLS<a class="headerlink" href="#mail-tls" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.</p>
</div>
<div class="section" id="mail-ssl">
<span id="std:setting-MAIL_SSL"></span><h5>MAIL_SSL<a class="headerlink" href="#mail-ssl" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enforce connecting using an SSL encrypted connection</p>
</div>
</div>
</div>
<span id="document-topics/telnetconsole"></span><div class="section" id="module-scrapy.extensions.telnet">
<span id="telnet-console"></span><span id="topics-telnetconsole"></span><h3>Telnet Console<a class="headerlink" href="#module-scrapy.extensions.telnet" title="Permalink to this headline">¶</a></h3>
<p>Scrapy comes with a built-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.</p>
<p>The telnet console is a <a class="reference internal" href="index.html#topics-extensions-ref"><span>built-in Scrapy extension</span></a> which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
<a class="reference internal" href="index.html#topics-extensions-ref-telnetconsole"><span>Telnet console extension</span></a>.</p>
<div class="section" id="how-to-access-the-telnet-console">
<h4>How to access the telnet console<a class="headerlink" href="#how-to-access-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console listens in the TCP port defined in the
<a class="reference internal" href="#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></code></a> setting, which defaults to <code class="docutils literal"><span class="pre">6023</span></code>. To access
the console you need to type:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt;
</pre></div>
</div>
<p>You need the telnet program which comes installed by default in Windows, and
most Linux distros.</p>
</div>
<div class="section" id="available-variables-in-the-telnet-console">
<h4>Available variables in the telnet console<a class="headerlink" href="#available-variables-in-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.</p>
<p>However, the telnet console comes with some default variables defined for
convenience:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Shortcut</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">crawler</span></code></td>
<td>the Scrapy Crawler (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.Crawler</span></code></a> object)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">engine</span></code></td>
<td>Crawler.engine attribute</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spider</span></code></td>
<td>the active spider</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">slot</span></code></td>
<td>the engine slot</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">extensions</span></code></td>
<td>the Extension Manager (Crawler.extensions attribute)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">stats</span></code></td>
<td>the Stats Collector (Crawler.stats attribute)</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">settings</span></code></td>
<td>the Scrapy settings object (Crawler.settings attribute)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">est</span></code></td>
<td>print a report of the engine status</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">prefs</span></code></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><span>Debugging memory leaks</span></a>)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">p</span></code></td>
<td>a shortcut to the <a class="reference external" href="https://docs.python.org/library/pprint.html#pprint.pprint">pprint.pprint</a> function</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">hpy</span></code></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><span>Debugging memory leaks</span></a>)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="telnet-console-usage-examples">
<h4>Telnet console usage examples<a class="headerlink" href="#telnet-console-usage-examples" title="Permalink to this headline">¶</a></h4>
<p>Here are some example tasks you can do with the telnet console:</p>
<div class="section" id="view-engine-status">
<h5>View engine status<a class="headerlink" href="#view-engine-status" title="Permalink to this headline">¶</a></h5>
<p>You can use the <code class="docutils literal"><span class="pre">est()</span></code> method of the Scrapy engine to quickly show its state
using the telnet console:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
engine.has_capacity()                           : False
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle(engine.spider)            : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</pre></div>
</div>
</div>
<div class="section" id="pause-resume-and-stop-the-scrapy-engine">
<h5>Pause, resume and stop the Scrapy engine<a class="headerlink" href="#pause-resume-and-stop-the-scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>To pause:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To resume:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To stop:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
</pre></div>
</div>
</div>
</div>
<div class="section" id="telnet-console-signals">
<h4>Telnet Console signals<a class="headerlink" href="#telnet-console-signals" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:signal-update_telnet_vars"></span><dl class="function">
<dt id="scrapy.extensions.telnet.update_telnet_vars">
<code class="descclassname">scrapy.extensions.telnet.</code><code class="descname">update_telnet_vars</code><span class="sig-paren">(</span><em>telnet_vars</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.telnet.update_telnet_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
<code class="docutils literal"><span class="pre">telnet_vars</span></code> dict in your handler.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>telnet_vars</strong> (<em>dict</em>) &#8211; the dict of telnet variables</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="telnet-settings">
<h4>Telnet settings<a class="headerlink" href="#telnet-settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings that control the telnet console&#8217;s behaviour:</p>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal"><span class="pre">None</span></code> or <code class="docutils literal"><span class="pre">0</span></code>, a
dynamically assigned port is used.</p>
</div>
<div class="section" id="telnetconsole-host">
<span id="std:setting-TELNETCONSOLE_HOST"></span><h5>TELNETCONSOLE_HOST<a class="headerlink" href="#telnetconsole-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">'127.0.0.1'</span></code></p>
<p>The interface the telnet console should listen on</p>
</div>
</div>
</div>
<span id="document-topics/webservice"></span><div class="section" id="web-service">
<span id="topics-webservice"></span><h3>Web Service<a class="headerlink" href="#web-service" title="Permalink to this headline">¶</a></h3>
<p>webservice has been moved into a separate project.</p>
<p>It is hosted at:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-jsonrpc">https://github.com/scrapy-plugins/scrapy-jsonrpc</a></div></blockquote>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/logging"><em>Logging</em></a></dt>
<dd>Learn how to use Python&#8217;s builtin logging on Scrapy.</dd>
<dt><a class="reference internal" href="index.html#document-topics/stats"><em>Stats Collection</em></a></dt>
<dd>Collect statistics about your scraping crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/email"><em>Sending e-mail</em></a></dt>
<dd>Send email notifications when certain events occur.</dd>
<dt><a class="reference internal" href="index.html#document-topics/telnetconsole"><em>Telnet Console</em></a></dt>
<dd>Inspect a running crawler using a built-in Python console.</dd>
<dt><a class="reference internal" href="index.html#document-topics/webservice"><em>Web Service</em></a></dt>
<dd>Monitor and control a crawler using a web service.</dd>
</dl>
</div>
<div class="section" id="solving-specific-problems">
<h2>Solving specific problems<a class="headerlink" href="#solving-specific-problems" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id4">
<span id="document-faq"></span><div class="section" id="frequently-asked-questions">
<span id="faq"></span><h3>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="how-does-scrapy-compare-to-beautifulsoup-or-lxml">
<span id="faq-scrapy-bs-cmp"></span><h4>How does Scrapy compare to BeautifulSoup or lxml?<a class="headerlink" href="#how-does-scrapy-compare-to-beautifulsoup-or-lxml" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> and <a class="reference external" href="http://lxml.de/">lxml</a> are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a class="reference internal" href="index.html#topics-selectors"><span>selectors</span></a>) but you can easily use <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>
(or <a class="reference external" href="http://lxml.de/">lxml</a>) instead, if you feel more comfortable working with them. After
all, they&#8217;re just parsing libraries which can be imported and used from any
Python code.</p>
<p>In other words, comparing <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (or <a class="reference external" href="http://lxml.de/">lxml</a>) to Scrapy is like
comparing <a class="reference external" href="http://jinja.pocoo.org/">jinja2</a> to <a class="reference external" href="https://www.djangoproject.com/">Django</a>.</p>
</div>
<div class="section" id="can-i-use-scrapy-with-beautifulsoup">
<h4>Can I use Scrapy with BeautifulSoup?<a class="headerlink" href="#can-i-use-scrapy-with-beautifulsoup" title="Permalink to this headline">¶</a></h4>
<p>Yes, you can.
As mentioned <a class="reference internal" href="#faq-scrapy-bs-cmp"><span>above</span></a>, <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> can be used
for parsing HTML responses in Scrapy callbacks.
You just have to feed the response&#8217;s body into a <code class="docutils literal"><span class="pre">BeautifulSoup</span></code> object
and extract whatever data you need from it.</p>
<p>Here&#8217;s an example spider using BeautifulSoup API, with <code class="docutils literal"><span class="pre">lxml</span></code> as the HTML parser:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from bs4 import BeautifulSoup
import scrapy


class ExampleSpider(scrapy.Spider):
    name = &quot;example&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = (
        &#39;http://www.example.com/&#39;,
    )

    def parse(self, response):
        # use lxml to get decent HTML parsing speed
        soup = BeautifulSoup(response.text, &#39;lxml&#39;)
        yield {
            &quot;url&quot;: response.url,
            &quot;title&quot;: soup.h1.string
        }
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">BeautifulSoup</span></code> supports several HTML/XML parsers.
See <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use">BeautifulSoup&#8217;s official documentation</a> on which ones are available.</p>
</div>
</div>
<div class="section" id="what-python-versions-does-scrapy-support">
<span id="faq-python-versions"></span><h4>What Python versions does Scrapy support?<a class="headerlink" href="#what-python-versions-does-scrapy-support" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is supported under Python 2.7 and Python 3.3+.
Python 2.6 support was dropped starting at Scrapy 0.20.
Python 3 support was added in Scrapy 1.1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Python 3 is not yet supported on Windows.</p>
</div>
</div>
<div class="section" id="did-scrapy-steal-x-from-django">
<h4>Did Scrapy &#8220;steal&#8221; X from Django?<a class="headerlink" href="#did-scrapy-steal-x-from-django" title="Permalink to this headline">¶</a></h4>
<p>Probably, but we don&#8217;t like that word. We think <a class="reference external" href="https://www.djangoproject.com/">Django</a> is a great open source
project and an example to follow, so we&#8217;ve used it as an inspiration for
Scrapy.</p>
<p>We believe that, if something is already done well, there&#8217;s no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.</p>
<p>We&#8217;d be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!</p>
</div>
<div class="section" id="does-scrapy-work-with-http-proxies">
<h4>Does Scrapy work with HTTP proxies?<a class="headerlink" href="#does-scrapy-work-with-http-proxies" title="Permalink to this headline">¶</a></h4>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></code></a>.</p>
</div>
<div class="section" id="how-can-i-scrape-an-item-with-attributes-in-different-pages">
<h4>How can I scrape an item with attributes in different pages?<a class="headerlink" href="#how-can-i-scrape-an-item-with-attributes-in-different-pages" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span>Passing additional data to callback functions</span></a>.</p>
</div>
<div class="section" id="scrapy-crashes-with-importerror-no-module-named-win32api">
<h4>Scrapy crashes with: ImportError: No module named win32api<a class="headerlink" href="#scrapy-crashes-with-importerror-no-module-named-win32api" title="Permalink to this headline">¶</a></h4>
<p>You need to install <a class="reference external" href="https://sourceforge.net/projects/pywin32/">pywin32</a> because of <a class="reference external" href="https://twistedmatrix.com/trac/ticket/3707">this Twisted bug</a>.</p>
</div>
<div class="section" id="how-can-i-simulate-a-user-login-in-my-spider">
<h4>How can I simulate a user login in my spider?<a class="headerlink" href="#how-can-i-simulate-a-user-login-in-my-spider" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><span>Using FormRequest.from_response() to simulate a user login</span></a>.</p>
</div>
<div class="section" id="does-scrapy-crawl-in-breadth-first-or-depth-first-order">
<span id="faq-bfo-dfo"></span><h4>Does Scrapy crawl in breadth-first or depth-first order?<a class="headerlink" href="#does-scrapy-crawl-in-breadth-first-or-depth-first-order" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy uses a <a class="reference external" href="https://en.wikipedia.org/wiki/LIFO">LIFO</a> queue for storing pending requests, which
basically means that it crawls in <a class="reference external" href="https://en.wikipedia.org/wiki/Depth-first_search">DFO order</a>. This order is more convenient
in most cases. If you do want to crawl in true <a class="reference external" href="https://en.wikipedia.org/wiki/Breadth-first_search">BFO order</a>, you can do it by
setting the following settings:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = &#39;scrapy.squeues.PickleFifoDiskQueue&#39;
SCHEDULER_MEMORY_QUEUE = &#39;scrapy.squeues.FifoMemoryQueue&#39;
</pre></div>
</div>
</div>
<div class="section" id="my-scrapy-crawler-has-memory-leaks-what-can-i-do">
<h4>My Scrapy crawler has memory leaks. What can I do?<a class="headerlink" href="#my-scrapy-crawler-has-memory-leaks-what-can-i-do" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-leaks"><span>Debugging memory leaks</span></a>.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a class="reference internal" href="index.html#topics-leaks-without-leaks"><span>Leaks without leaks</span></a>.</p>
</div>
<div class="section" id="how-can-i-make-scrapy-consume-less-memory">
<h4>How can I make Scrapy consume less memory?<a class="headerlink" href="#how-can-i-make-scrapy-consume-less-memory" title="Permalink to this headline">¶</a></h4>
<p>See previous question.</p>
</div>
<div class="section" id="can-i-use-basic-http-authentication-in-my-spiders">
<h4>Can I use Basic HTTP Authentication in my spiders?<a class="headerlink" href="#can-i-use-basic-http-authentication-in-my-spiders" title="Permalink to this headline">¶</a></h4>
<p>Yes, see <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpAuthMiddleware</span></code></a>.</p>
</div>
<div class="section" id="why-does-scrapy-download-pages-in-english-instead-of-my-native-language">
<h4>Why does Scrapy download pages in English instead of my native language?<a class="headerlink" href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-language" title="Permalink to this headline">¶</a></h4>
<p>Try changing the default <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> request header by overriding the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</div>
<div class="section" id="where-can-i-find-some-example-scrapy-projects">
<h4>Where can I find some example Scrapy projects?<a class="headerlink" href="#where-can-i-find-some-example-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#intro-examples"><span>Examples</span></a>.</p>
</div>
<div class="section" id="can-i-run-a-spider-without-creating-a-project">
<h4>Can I run a spider without creating a project?<a class="headerlink" href="#can-i-run-a-spider-without-creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Yes. You can use the <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command. For example, if you have a
spider written in a <code class="docutils literal"><span class="pre">my_spider.py</span></code> file you can run it with:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy runspider my_spider.py
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command for more info.</p>
</div>
<div class="section" id="i-get-filtered-offsite-request-messages-how-can-i-fix-them">
<h4>I get &#8220;Filtered offsite request&#8221; messages. How can I fix them?<a class="headerlink" href="#i-get-filtered-offsite-request-messages-how-can-i-fix-them" title="Permalink to this headline">¶</a></h4>
<p>Those messages (logged with <code class="docutils literal"><span class="pre">DEBUG</span></code> level) don&#8217;t necessarily mean there is a
problem, so you may not need to fix them.</p>
<p>Those messages are thrown by the Offsite Spider Middleware, which is a spider
middleware (enabled by default) whose purpose is to filter out requests to
domains outside the ones covered by the spider.</p>
<p>For more info see:
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></code></a>.</p>
</div>
<div class="section" id="what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">
<h4>What is the recommended way to deploy a Scrapy crawler in production?<a class="headerlink" href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-deploy"><span>Deploying Spiders</span></a>.</p>
</div>
<div class="section" id="can-i-use-json-for-large-exports">
<h4>Can I use JSON for large exports?<a class="headerlink" href="#can-i-use-json-for-large-exports" title="Permalink to this headline">¶</a></h4>
<p>It&#8217;ll depend on how large your output is. See <a class="reference internal" href="index.html#json-with-large-data"><span>this warning</span></a> in <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a>
documentation.</p>
</div>
<div class="section" id="can-i-return-twisted-deferreds-from-signal-handlers">
<h4>Can I return (Twisted) deferreds from signal handlers?<a class="headerlink" href="#can-i-return-twisted-deferreds-from-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning deferreds from their handlers, others don&#8217;t. See
the <a class="reference internal" href="index.html#topics-signals-ref"><span>Built-in signals reference</span></a> to know which ones.</p>
</div>
<div class="section" id="what-does-the-response-status-code-999-means">
<h4>What does the response status code 999 means?<a class="headerlink" href="#what-does-the-response-status-code-999-means" title="Permalink to this headline">¶</a></h4>
<p>999 is a custom response status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of <code class="docutils literal"><span class="pre">2</span></code> (or
higher) in your spider:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(CrawlSpider):

    name = &#39;myspider&#39;

    download_delay = 2

    # [ ... rest of the spider code ... ]
</pre></div>
</div>
<p>Or by setting a global download delay in your project with the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</p>
</div>
<div class="section" id="can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">
<h4>Can I call <code class="docutils literal"><span class="pre">pdb.set_trace()</span></code> from my spiders to debug them?<a class="headerlink" href="#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them" title="Permalink to this headline">¶</a></h4>
<p>Yes, but you can also use the Scrapy shell which allows you to quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old <code class="docutils literal"><span class="pre">pdb.set_trace()</span></code>.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-shell-inspect-response"><span>Invoking the shell from spiders to inspect responses</span></a>.</p>
</div>
<div class="section" id="simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">
<h4>Simplest way to dump all my scraped items into a JSON/CSV/XML file?<a class="headerlink" href="#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file" title="Permalink to this headline">¶</a></h4>
<p>To dump into a JSON file:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy crawl myspider -o items.json
</pre></div>
</div>
<p>To dump into a CSV file:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy crawl myspider -o items.csv
</pre></div>
</div>
<p>To dump into a XML file:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy crawl myspider -o items.xml
</pre></div>
</div>
<p>For more information see <a class="reference internal" href="index.html#topics-feed-exports"><span>Feed exports</span></a></p>
</div>
<div class="section" id="what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">
<h4>What&#8217;s this huge cryptic <code class="docutils literal"><span class="pre">__VIEWSTATE</span></code> parameter used in some forms?<a class="headerlink" href="#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal"><span class="pre">__VIEWSTATE</span></code> parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see <a class="reference external" href="http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">this page</a>. Also, here&#8217;s an <a class="reference external" href="https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example spider</a>
which scrapes one of these sites.</p>
</div>
<div class="section" id="what-s-the-best-way-to-parse-big-xml-csv-data-feeds">
<h4>What&#8217;s the best way to parse big XML/CSV data feeds?<a class="headerlink" href="#what-s-the-best-way-to-parse-big-xml-csv-data-feeds" title="Permalink to this headline">¶</a></h4>
<p>Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can use
the functions <code class="docutils literal"><span class="pre">xmliter</span></code> and <code class="docutils literal"><span class="pre">csviter</span></code> from <code class="docutils literal"><span class="pre">scrapy.utils.iterators</span></code>
module. In fact, this is what the feed spiders (see <a class="reference internal" href="index.html#topics-spiders"><span>Spiders</span></a>) use
under the cover.</p>
</div>
<div class="section" id="does-scrapy-manage-cookies-automatically">
<h4>Does Scrapy manage cookies automatically?<a class="headerlink" href="#does-scrapy-manage-cookies-automatically" title="Permalink to this headline">¶</a></h4>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-request-response"><span>Requests and Responses</span></a> and <a class="reference internal" href="index.html#cookies-mw"><span>CookiesMiddleware</span></a>.</p>
</div>
<div class="section" id="how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">
<h4>How can I see the cookies being sent and received from Scrapy?<a class="headerlink" href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy" title="Permalink to this headline">¶</a></h4>
<p>Enable the <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a> setting.</p>
</div>
<div class="section" id="how-can-i-instruct-a-spider-to-stop-itself">
<h4>How can I instruct a spider to stop itself?<a class="headerlink" href="#how-can-i-instruct-a-spider-to-stop-itself" title="Permalink to this headline">¶</a></h4>
<p>Raise the <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></code></a> exception from a callback. For
more info see: <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></code></a>.</p>
</div>
<div class="section" id="how-can-i-prevent-my-scrapy-bot-from-getting-banned">
<h4>How can I prevent my Scrapy bot from getting banned?<a class="headerlink" href="#how-can-i-prevent-my-scrapy-bot-from-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#bans"><span>Avoiding getting banned</span></a>.</p>
</div>
<div class="section" id="should-i-use-spider-arguments-or-settings-to-configure-my-spider">
<h4>Should I use spider arguments or settings to configure my spider?<a class="headerlink" href="#should-i-use-spider-arguments-or-settings-to-configure-my-spider" title="Permalink to this headline">¶</a></h4>
<p>Both <a class="reference internal" href="index.html#spiderargs"><span>spider arguments</span></a> and <a class="reference internal" href="index.html#topics-settings"><span>settings</span></a>
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don&#8217;t change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.</p>
</div>
<div class="section" id="i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">
<h4>I&#8217;m scraping a XML document and my XPath selector doesn&#8217;t return any items<a class="headerlink" href="#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items" title="Permalink to this headline">¶</a></h4>
<p>You may need to remove namespaces. See <a class="reference internal" href="index.html#removing-namespaces"><span>Removing namespaces</span></a>.</p>
</div>
</div>
<span id="document-topics/debug"></span><div class="section" id="debugging-spiders">
<span id="topics-debug"></span><h3>Debugging Spiders<a class="headerlink" href="#debugging-spiders" title="Permalink to this headline">¶</a></h3>
<p>This document explains the most common techniques for debugging spiders.
Consider the following scrapy spider below:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = &#39;myspider&#39;
    start_urls = (
        &#39;http://example.com/page1&#39;,
        &#39;http://example.com/page2&#39;,
        )

    def parse(self, response):
        # collect `item_urls`
        for item_url in item_urls:
            yield scrapy.Request(item_url, self.parse_item)

    def parse_item(self, response):
        item = MyItem()
        # populate `item` fields
        # and extract item_details_url
        yield scrapy.Request(item_details_url, self.parse_details, meta={&#39;item&#39;: item})

    def parse_details(self, response):
        item = response.meta[&#39;item&#39;]
        # populate more `item` fields
        return item
</pre></div>
</div>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the <code class="docutils literal"><span class="pre">meta</span></code> functionality of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> to pass a
partially populated item.</p>
<div class="section" id="parse-command">
<h4>Parse Command<a class="headerlink" href="#parse-command" title="Permalink to this headline">¶</a></h4>
<p>The most basic way of checking the output of your spider is to use the
<a class="reference internal" href="index.html#std:command-parse"><code class="xref std std-command docutils literal"><span class="pre">parse</span></code></a> command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Using the <code class="docutils literal"><span class="pre">--verbose</span></code> or <code class="docutils literal"><span class="pre">-v</span></code> option we can see the status at each depth level:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Checking items scraped from a single start_url, can also be easily achieved
using:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -d 3 &#39;http://example.com/page1&#39;
</pre></div>
</div>
</div>
<div class="section" id="scrapy-shell">
<h4>Scrapy Shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h4>
<p>While the <a class="reference internal" href="index.html#std:command-parse"><code class="xref std std-command docutils literal"><span class="pre">parse</span></code></a> command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
<code class="docutils literal"><span class="pre">parse_details</span></code> sometimes receives no item?</p>
<p>Fortunately, the <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> is your bread and butter in this case (see
<a class="reference internal" href="index.html#topics-shell-inspect-response"><span>Invoking the shell from spiders to inspect responses</span></a>):</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.shell import inspect_response

def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        inspect_response(response, self)
</pre></div>
</div>
<p>See also: <a class="reference internal" href="index.html#topics-shell-inspect-response"><span>Invoking the shell from spiders to inspect responses</span></a>.</p>
</div>
<div class="section" id="open-in-browser">
<h4>Open in browser<a class="headerlink" href="#open-in-browser" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you just want to see how a certain response looks in a browser, you
can use the <code class="docutils literal"><span class="pre">open_in_browser</span></code> function for that. Here is an example of how
you would use it:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if &quot;item name&quot; not in response.body:
        open_in_browser(response)
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">open_in_browser</span></code> will open a browser with the response received by Scrapy at
that point, adjusting the <a class="reference external" href="http://www.w3schools.com/tags/tag_base.asp">base tag</a> so that images and styles are displayed
properly.</p>
</div>
<div class="section" id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h4>
<p>Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def parse_details(self, response):
    item = response.meta.get(&#39;item&#39;, None)
    if item:
        # populate more `item` fields
        return item
    else:
        self.logger.warning(&#39;No item received for %s&#39;, response.url)
</pre></div>
</div>
<p>For more information, check the <a class="reference internal" href="index.html#topics-logging"><span>Logging</span></a> section.</p>
</div>
</div>
<span id="document-topics/contracts"></span><div class="section" id="spiders-contracts">
<span id="topics-contracts"></span><h3>Spiders Contracts<a class="headerlink" href="#spiders-contracts" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is a new feature (introduced in Scrapy 0.15) and may be subject
to minor functionality/API updates. Check the <a class="reference internal" href="index.html#news"><span>release notes</span></a> to
be notified of updates.</p>
</div>
<p>Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an <code class="docutils literal"><span class="pre">&#64;</span></code> and included in the docstring. See the
following example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def parse(self, response):
    &quot;&quot;&quot; This function parses a sample response. Some contracts are mingled
    with this docstring.

    @url http://www.amazon.com/s?field-keywords=selfish+gene
    @returns items 1 16
    @returns requests 0 0
    @scrapes Title Author Year Price
    &quot;&quot;&quot;
</pre></div>
</div>
<p>This callback is tested using three built-in contracts:</p>
<span class="target" id="module-scrapy.contracts.default"></span><dl class="class">
<dt id="scrapy.contracts.default.UrlContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">UrlContract</code><a class="headerlink" href="#scrapy.contracts.default.UrlContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal"><span class="pre">&#64;url</span></code>) sets the sample url used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>@url url
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ReturnsContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">ReturnsContract</code><a class="headerlink" href="#scrapy.contracts.default.ReturnsContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal"><span class="pre">&#64;returns</span></code>) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>@returns item(s)|request(s) [min [max]]
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ScrapesContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">ScrapesContract</code><a class="headerlink" href="#scrapy.contracts.default.ScrapesContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal"><span class="pre">&#64;scrapes</span></code>) checks that all the items returned by the
callback have the specified fields:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>@scrapes field_1 field_2 ...
</pre></div>
</div>
</dd></dl>

<p>Use the <a class="reference internal" href="index.html#std:command-check"><code class="xref std std-command docutils literal"><span class="pre">check</span></code></a> command to run the contract checks.</p>
<div class="section" id="custom-contracts">
<h4>Custom Contracts<a class="headerlink" href="#custom-contracts" title="Permalink to this headline">¶</a></h4>
<p>If you find you need more power than the built-in scrapy contracts you can
create and load your own contracts in the project by using the
<a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>SPIDER_CONTRACTS = {
    &#39;myproject.contracts.ResponseCheck&#39;: 10,
    &#39;myproject.contracts.ItemValidate&#39;: 10,
}
</pre></div>
</div>
<p>Each contract must inherit from <a class="reference internal" href="#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><code class="xref py py-class docutils literal"><span class="pre">scrapy.contracts.Contract</span></code></a> and can
override three methods:</p>
<span class="target" id="module-scrapy.contracts"></span><dl class="class">
<dt id="scrapy.contracts.Contract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.</code><code class="descname">Contract</code><span class="sig-paren">(</span><em>method</em>, <em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>method</strong> (<em>function</em>) &#8211; callback function to which the contract is associated</li>
<li><strong>args</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; list of arguments passed into the docstring (whitespace
separated)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="scrapy.contracts.Contract.adjust_request_args">
<code class="descname">adjust_request_args</code><span class="sig-paren">(</span><em>args</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.adjust_request_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This receives a <code class="docutils literal"><span class="pre">dict</span></code> as an argument containing default arguments
for <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object. Must return the same or a
modified version of it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.pre_process">
<code class="descname">pre_process</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.pre_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows hooking in various checks on the response received from the
sample request, before it&#8217;s being passed to the callback.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.post_process">
<code class="descname">post_process</code><span class="sig-paren">(</span><em>output</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows processing the output of the callback. Iterators are
converted listified before being passed to this hook.</p>
</dd></dl>

</dd></dl>

<p>Here is a demo contract which checks the presence of a custom header in the
response received. Raise <code class="xref py py-class docutils literal"><span class="pre">scrapy.exceptions.ContractFail</span></code> in order to
get the failures pretty printed:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail

class HasHeaderContract(Contract):
    &quot;&quot;&quot; Demo contract which checks the presence of a custom header
        @has_header X-CustomHeader
    &quot;&quot;&quot;

    name = &#39;has_header&#39;

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                raise ContractFail(&#39;X-CustomHeader not present&#39;)
</pre></div>
</div>
</div>
</div>
<span id="document-topics/practices"></span><div class="section" id="common-practices">
<span id="topics-practices"></span><h3>Common Practices<a class="headerlink" href="#common-practices" title="Permalink to this headline">¶</a></h3>
<p>This section documents common practices when using Scrapy. These are things
that cover many topics and don&#8217;t often fall into any other specific section.</p>
<div class="section" id="run-scrapy-from-a-script">
<span id="run-from-script"></span><h4>Run Scrapy from a script<a class="headerlink" href="#run-scrapy-from-a-script" title="Permalink to this headline">¶</a></h4>
<p>You can use the <a class="reference internal" href="index.html#topics-api"><span>API</span></a> to run Scrapy from a script, instead of
the typical way of running Scrapy via <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>.</p>
<p>Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need to run it inside the Twisted reactor.</p>
<p>First utility you can use to run your spiders is
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerProcess</span></code></a>. This class will start a Twisted reactor
for you, configuring the logging and setting shutdown handlers. This class is
the one used by all Scrapy commands.</p>
<p>Here&#8217;s an example showing how to run a single spider with it.</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy
from scrapy.crawler import CrawlerProcess

class MySpider(scrapy.Spider):
    # Your spider definition
    ...

process = CrawlerProcess({
    &#39;USER_AGENT&#39;: &#39;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#39;
})

process.crawl(MySpider)
process.start() # the script will block here until the crawling is finished
</pre></div>
</div>
<p>Make sure to check <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a> documentation to get
acquainted with its usage details.</p>
<p>If you are inside a Scrapy project there are some additional helpers you can
use to import those components within the project. You can automatically import
your spiders passing their name to <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a>, and
use <code class="docutils literal"><span class="pre">get_project_settings</span></code> to get a <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a>
instance with your project settings.</p>
<p>What follows is a working example of how to do that, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a>
project as example.</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

process = CrawlerProcess(get_project_settings())

# &#39;followall&#39; is the name of one of the spiders of the project.
process.crawl(&#39;followall&#39;, domain=&#39;scrapinghub.com&#39;)
process.start() # the script will block here until the crawling is finished
</pre></div>
</div>
<p>There&#8217;s another Scrapy utility that provides more control over the crawling
process: <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a>. This class is a thin wrapper
that encapsulates some simple helpers to run multiple crawlers, but it won&#8217;t
start or interfere with existing reactors in any way.</p>
<p>Using this class the reactor should be explicitly run after scheduling your
spiders. It&#8217;s recommended you use <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a>
instead of <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a> if your application is
already using Twisted and you want to run Scrapy in the same reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by adding callbacks to the deferred
returned by the <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal"><span class="pre">CrawlerRunner.crawl</span></code></a> method.</p>
<p>Here&#8217;s an example of its usage, along with a callback to manually stop the
reactor after <cite>MySpider</cite> has finished running.</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from twisted.internet import reactor
import scrapy
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging

class MySpider(scrapy.Spider):
    # Your spider definition
    ...

configure_logging({&#39;LOG_FORMAT&#39;: &#39;%(levelname)s: %(message)s&#39;})
runner = CrawlerRunner()

d = runner.crawl(MySpider)
d.addBoth(lambda _: reactor.stop())
reactor.run() # the script will block here until the crawling is finished
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">Twisted Reactor Overview</a>.</p>
</div>
</div>
<div class="section" id="running-multiple-spiders-in-the-same-process">
<span id="run-multiple-spiders"></span><h4>Running multiple spiders in the same process<a class="headerlink" href="#running-multiple-spiders-in-the-same-process" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy runs a single spider per process when you run <code class="docutils literal"><span class="pre">scrapy</span>
<span class="pre">crawl</span></code>. However, Scrapy supports running multiple spiders per process using
the <a class="reference internal" href="index.html#topics-api"><span>internal API</span></a>.</p>
<p>Here is an example that runs multiple spiders simultaneously:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy
from scrapy.crawler import CrawlerProcess

class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...

class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...

process = CrawlerProcess()
process.crawl(MySpider1)
process.crawl(MySpider2)
process.start() # the script will block here until all crawling jobs are finished
</pre></div>
</div>
<p>Same example using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a>:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy
from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging

class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...

class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...

configure_logging()
runner = CrawlerRunner()
runner.crawl(MySpider1)
runner.crawl(MySpider2)
d = runner.join()
d.addBoth(lambda _: reactor.stop())

reactor.run() # the script will block here until all crawling jobs are finished
</pre></div>
</div>
<p>Same example but running the spiders sequentially by chaining the deferreds:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from twisted.internet import reactor, defer
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging

class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...

class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...

configure_logging()
runner = CrawlerRunner()

@defer.inlineCallbacks
def crawl():
    yield runner.crawl(MySpider1)
    yield runner.crawl(MySpider2)
    reactor.stop()

crawl()
reactor.run() # the script will block here until the last crawl call is finished
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#run-from-script"><span>Run Scrapy from a script</span></a>.</p>
</div>
</div>
<div class="section" id="distributed-crawls">
<span id="id1"></span><h4>Distributed crawls<a class="headerlink" href="#distributed-crawls" title="Permalink to this headline">¶</a></h4>
<p>Scrapy doesn&#8217;t provide any built-in facility for running crawls in a distribute
(multi-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list
</pre></div>
</div>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument <code class="docutils literal"><span class="pre">part</span></code> with the number of the partition to
crawl:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
</pre></div>
</div>
</div>
<div class="section" id="avoiding-getting-banned">
<span id="bans"></span><h4>Avoiding getting banned<a class="headerlink" href="#avoiding-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting <a class="reference external" href="http://scrapy.org/support/">commercial support</a> if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kinds of sites:</p>
<ul class="simple">
<li>rotate your user agent from a pool of well-known ones from browsers (google
around to get a list of them)</li>
<li>disable cookies (see <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></code></a>) as some sites may use
cookies to spot bot behaviour</li>
<li>use download delays (2 or higher). See <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</li>
<li>if possible, use <a class="reference external" href="http://www.googleguide.com/cached_pages.html">Google cache</a> to fetch pages, instead of hitting the sites
directly</li>
<li>use a pool of rotating IPs. For example, the free <a class="reference external" href="https://www.torproject.org/">Tor project</a> or paid
services like <a class="reference external" href="http://proxymesh.com/">ProxyMesh</a></li>
<li>use a highly distributed downloader that circumvents bans internally, so you
can just focus on parsing clean pages. One example of such downloaders is
<a class="reference external" href="http://scrapinghub.com/crawlera">Crawlera</a></li>
</ul>
<p>If you are still unable to prevent your bot getting banned, consider contacting
<a class="reference external" href="http://scrapy.org/support/">commercial support</a>.</p>
</div>
</div>
<span id="document-topics/broad-crawls"></span><div class="section" id="broad-crawls">
<span id="topics-broad-crawls"></span><h3>Broad Crawls<a class="headerlink" href="#broad-crawls" title="Permalink to this headline">¶</a></h3>
<p>Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).</p>
<p>In addition to this &#8220;focused crawl&#8221;, there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called &#8220;broad crawls&#8221; and is the typical crawlers employed by search
engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<ul class="simple">
<li>they crawl many domains (often, unbounded) instead of a specific set of sites</li>
<li>they don&#8217;t necessarily crawl domains to completion, because it would
impractical (or impossible) to do so, and instead limit the crawl by time or
number of pages crawled</li>
<li>they are simpler in logic (as opposed to very complex spiders with many
extraction rules) because data is often post-processed in a separate stage</li>
<li>they crawl many domains concurrently, which allows them to achieve faster
crawl speeds by not being limited by any particular site constraint (each site
is crawled slowly to respect politeness, but many sites are crawled in
parallel)</li>
</ul>
<p>As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarizes some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.</p>
<div class="section" id="increase-concurrency">
<h4>Increase concurrency<a class="headerlink" href="#increase-concurrency" title="Permalink to this headline">¶</a></h4>
<p>Concurrency is the number of requests that are processed in parallel. There is
a global limit and a per-domain limit.</p>
<p>The default global concurrency limit in Scrapy is not suitable for crawling
many different domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU you crawler will have available. A
good starting point is <code class="docutils literal"><span class="pre">100</span></code>, but the best way to find out is by doing some
trials and identifying at what concurrency your Scrapy process gets CPU
bounded. For optimum performance, you should pick a concurrency where CPU usage
is at 80-90%.</p>
<p>To increase the global concurrency use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>CONCURRENT_REQUESTS = 100
</pre></div>
</div>
</div>
<div class="section" id="increase-twisted-io-thread-pool-maximum-size">
<h4>Increase Twisted IO thread pool maximum size<a class="headerlink" href="#increase-twisted-io-thread-pool-maximum-size" title="Permalink to this headline">¶</a></h4>
<p>Currently Scrapy does DNS resolution in a blocking way with usage of thread
pool. With higher concurrency levels the crawling could be slow or even fail
hitting DNS resolver timeouts. Possible solution to increase the number of
threads handling DNS queries. The DNS queue will be processed faster speeding
up establishing of connection and crawling overall.</p>
<p>To increase maximum thread pool size use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>REACTOR_THREADPOOL_MAXSIZE = 20
</pre></div>
</div>
</div>
<div class="section" id="setup-your-own-dns">
<h4>Setup your own DNS<a class="headerlink" href="#setup-your-own-dns" title="Permalink to this headline">¶</a></h4>
<p>If you have multiple crawling processes and single central DNS, it can act
like DoS attack on the DNS server resulting to slow down of entire network or
even blocking your machines. To avoid this setup your own DNS server with
local cache and upstream to some large DNS like OpenDNS or Verizon.</p>
</div>
<div class="section" id="reduce-log-level">
<h4>Reduce log level<a class="headerlink" href="#reduce-log-level" title="Permalink to this headline">¶</a></h4>
<p>When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
<code class="docutils literal"><span class="pre">INFO</span></code> log level. In order to save CPU (and log storage requirements) you
should not use <code class="docutils literal"><span class="pre">DEBUG</span></code> log level when preforming large broad crawls in
production. Using <code class="docutils literal"><span class="pre">DEBUG</span></code> level when developing your (broad) crawler may fine
though.</p>
<p>To set the log level use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>LOG_LEVEL = &#39;INFO&#39;
</pre></div>
</div>
</div>
<div class="section" id="disable-cookies">
<h4>Disable cookies<a class="headerlink" href="#disable-cookies" title="Permalink to this headline">¶</a></h4>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.</p>
<p>To disable cookies use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>COOKIES_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="disable-retries">
<h4>Disable retries<a class="headerlink" href="#disable-retries" title="Permalink to this headline">¶</a></h4>
<p>Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.</p>
<p>To disable retries use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>RETRY_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="reduce-download-timeout">
<h4>Reduce download timeout<a class="headerlink" href="#reduce-download-timeout" title="Permalink to this headline">¶</a></h4>
<p>Unless you are crawling from a very slow connection (which shouldn&#8217;t be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.</p>
<p>To reduce the download timeout use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>DOWNLOAD_TIMEOUT = 15
</pre></div>
</div>
</div>
<div class="section" id="disable-redirects">
<h4>Disable redirects<a class="headerlink" href="#disable-redirects" title="Permalink to this headline">¶</a></h4>
<p>Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it&#8217;s common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.</p>
<p>To disable redirects use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>REDIRECT_ENABLED = False
</pre></div>
</div>
</div>
<div class="section" id="enable-crawling-of-ajax-crawlable-pages">
<h4>Enable crawling of &#8220;Ajax Crawlable Pages&#8221;<a class="headerlink" href="#enable-crawling-of-ajax-crawlable-pages" title="Permalink to this headline">¶</a></h4>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">ajax crawlable</a>. This means they provide plain HTML
version of content that is usually available only via AJAX.
Pages can indicate it in two ways:</p>
<ol class="arabic simple">
<li>by using <code class="docutils literal"><span class="pre">#!</span></code> in URL - this is the default way;</li>
<li>by using a special meta tag - this way is used on
&#8220;main&#8221;, &#8220;index&#8221; website pages.</li>
</ol>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a class="reference internal" href="index.html#ajaxcrawl-middleware"><span>AjaxCrawlMiddleware</span></a>:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>AJAXCRAWL_ENABLED = True
</pre></div>
</div>
<p>When doing broad crawls it&#8217;s common to crawl a lot of &#8220;index&#8221; web pages;
AjaxCrawlMiddleware helps to crawl them correctly.
It is turned OFF by default because it has some performance overhead,
and enabling it for focused crawls doesn&#8217;t make much sense.</p>
</div>
</div>
<span id="document-topics/firefox"></span><div class="section" id="using-firefox-for-scraping">
<span id="topics-firefox"></span><h3>Using Firefox for scraping<a class="headerlink" href="#using-firefox-for-scraping" title="Permalink to this headline">¶</a></h3>
<p>Here is a list of tips and advice on using Firefox for scraping, along with a
list of useful Firefox add-ons to ease the scraping process.</p>
<div class="section" id="caveats-with-inspecting-the-live-browser-dom">
<span id="topics-firefox-livedom"></span><h4>Caveats with inspecting the live browser DOM<a class="headerlink" href="#caveats-with-inspecting-the-live-browser-dom" title="Permalink to this headline">¶</a></h4>
<p>Since Firefox add-ons operate on a live browser DOM, what you&#8217;ll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing Javascript code.  Firefox,
in particular, is known for adding <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code> elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won&#8217;t be able to
extract any data if you use <code class="docutils literal"><span class="pre">&lt;tbody</span></code> in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things when working with
Firefox and XPath:</p>
<ul class="simple">
<li>Disable Firefox Javascript while inspecting the DOM looking for XPaths to be
used in Scrapy</li>
<li>Never use full XPath paths, use relative and clever ones based on attributes
(such as <code class="docutils literal"><span class="pre">id</span></code>, <code class="docutils literal"><span class="pre">class</span></code>, <code class="docutils literal"><span class="pre">width</span></code>, etc) or any identifying features like
<code class="docutils literal"><span class="pre">contains(&#64;href,</span> <span class="pre">'image')</span></code>.</li>
<li>Never include <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code> elements in your XPath expressions unless you
really know what you&#8217;re doing</li>
</ul>
</div>
<div class="section" id="useful-firefox-add-ons-for-scraping">
<span id="topics-firefox-addons"></span><h4>Useful Firefox add-ons for scraping<a class="headerlink" href="#useful-firefox-add-ons-for-scraping" title="Permalink to this headline">¶</a></h4>
<div class="section" id="firebug">
<h5>Firebug<a class="headerlink" href="#firebug" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="http://getfirebug.com">Firebug</a> is a widely known tool among web developers and it&#8217;s also very
useful for scraping. In particular, its <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> feature comes very
handy when you need to construct the XPaths for extracting data because it
allows you to view the HTML code of each page element while moving your mouse
over it.</p>
<p>See <a class="reference internal" href="index.html#topics-firebug"><span>Using Firebug for scraping</span></a> for a detailed guide on how to use Firebug with
Scrapy.</p>
</div>
<div class="section" id="xpather">
<h5>XPather<a class="headerlink" href="#xpather" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/xpather/">XPather</a> allows you to test XPath expressions directly on the pages.</p>
</div>
<div class="section" id="xpath-checker">
<h5>XPath Checker<a class="headerlink" href="#xpath-checker" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/xpath-checker/">XPath Checker</a> is another Firefox add-on for testing XPaths on your pages.</p>
</div>
<div class="section" id="tamper-data">
<h5>Tamper Data<a class="headerlink" href="#tamper-data" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/tamper-data/">Tamper Data</a> is a Firefox add-on which allows you to view and modify the HTTP
request headers sent by Firefox. Firebug also allows to view HTTP headers, but
not to modify them.</p>
</div>
<div class="section" id="firecookie">
<h5>Firecookie<a class="headerlink" href="#firecookie" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/firecookie/">Firecookie</a> makes it easier to view and manage cookies. You can use this
extension to create a new cookie, delete existing cookies, see a list of cookies
for the current site, manage cookies permissions and a lot more.</p>
</div>
</div>
</div>
<span id="document-topics/firebug"></span><div class="section" id="using-firebug-for-scraping">
<span id="topics-firebug"></span><h3>Using Firebug for scraping<a class="headerlink" href="#using-firebug-for-scraping" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Google Directory, the example website used in this guide is no longer
available as it <a class="reference external" href="https://searchenginewatch.com/sew/news/2096661/google-directory-shut">has been shut down by Google</a>. The concepts in this guide
are still valid though. If you want to update this guide to use a new
(working) site, your contribution will be more than welcome!. See <a class="reference internal" href="index.html#topics-contributing"><span>Contributing to Scrapy</span></a>
for information on how to do so.</p>
</div>
<div class="section" id="introduction">
<h4>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h4>
<p>This document explains how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> (a Firefox add-on) to make the
scraping process easier and more fun. For other useful Firefox add-ons see
<a class="reference internal" href="index.html#topics-firefox-addons"><span>Useful Firefox add-ons for scraping</span></a>. There are some caveats with using Firefox add-ons
to inspect pages, see <a class="reference internal" href="index.html#topics-firefox-livedom"><span>Caveats with inspecting the live browser DOM</span></a>.</p>
<p>In this example, we&#8217;ll show how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> to scrape data from the
<a class="reference external" href="http://directory.google.com/">Google Directory</a>, which contains the same data as the <a class="reference external" href="http://www.dmoz.org">Open Directory
Project</a> used in the <a class="reference internal" href="index.html#intro-tutorial"><span>tutorial</span></a> but with a different
face.</p>
<p>Firebug comes with a very useful feature called <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> which allows
you to inspect the HTML code of the different page elements just by hovering
your mouse over them. Otherwise you would have to search for the tags manually
through the HTML body which can be a very tedious task.</p>
<p>In the following screenshot you can see the <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> tool in action.</p>
<a class="reference internal image-reference" href="_images/firebug1.png"><img alt="Inspecting elements with Firebug" src="_images/firebug1.png" style="width: 913px; height: 600px;" /></a>
<p>At first sight, we can see that the directory is divided in categories, which
are also divided in subcategories.</p>
<p>However, it seems that there are more subcategories than the ones being shown
in this page, so we&#8217;ll keep looking:</p>
<a class="reference internal image-reference" href="_images/firebug2.png"><img alt="Inspecting elements with Firebug" src="_images/firebug2.png" style="width: 819px; height: 629px;" /></a>
<p>As expected, the subcategories contain links to other subcategories, and also
links to actual websites, which is the purpose of the directory.</p>
</div>
<div class="section" id="getting-links-to-follow">
<h4>Getting links to follow<a class="headerlink" href="#getting-links-to-follow" title="Permalink to this headline">¶</a></h4>
<p>By looking at the category URLs we can see they share a pattern:</p>
<blockquote>
<div><a class="reference external" href="http://directory.google.com/Category/Subcategory/Another_Subcategory">http://directory.google.com/Category/Subcategory/Another_Subcategory</a></div></blockquote>
<p>Once we know that, we are able to construct a regular expression to follow
those links. For example, the following one:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>directory\.google\.com/[A-Z][a-zA-Z_/]+$
</pre></div>
</div>
<p>So, based on that regular expression we can create the first crawling rule:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>Rule(LinkExtractor(allow=&#39;directory.google.com/[A-Z][a-zA-Z_/]+$&#39;, ),
    &#39;parse_category&#39;,
    follow=True,
),
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a> object instructs
<a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> based spiders how to follow the
category links. <code class="docutils literal"><span class="pre">parse_category</span></code> will be a method of the spider which will
process and extract data from those pages.</p>
<p>This is how the spider would look so far:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

class GoogleDirectorySpider(CrawlSpider):
    name = &#39;directory.google.com&#39;
    allowed_domains = [&#39;directory.google.com&#39;]
    start_urls = [&#39;http://directory.google.com/&#39;]

    rules = (
        Rule(LinkExtractor(allow=&#39;directory\.google\.com/[A-Z][a-zA-Z_/]+$&#39;),
            &#39;parse_category&#39;, follow=True,
        ),
    )

    def parse_category(self, response):
        # write the category page data extraction code here
        pass
</pre></div>
</div>
</div>
<div class="section" id="extracting-the-data">
<h4>Extracting the data<a class="headerlink" href="#extracting-the-data" title="Permalink to this headline">¶</a></h4>
<p>Now we&#8217;re going to write the code to extract data from those pages.</p>
<p>With the help of Firebug, we&#8217;ll take a look at some page containing links to
websites (say <a class="reference external" href="http://directory.google.com/Top/Arts/Awards/">http://directory.google.com/Top/Arts/Awards/</a>) and find out how we can
extract those links using <a class="reference internal" href="index.html#topics-selectors"><span>Selectors</span></a>. We&#8217;ll also
use the <a class="reference internal" href="index.html#topics-shell"><span>Scrapy shell</span></a> to test those XPath&#8217;s and make sure
they work as we expect.</p>
<a class="reference internal image-reference" href="_images/firebug3.png"><img alt="Inspecting elements with Firebug" src="_images/firebug3.png" style="width: 965px; height: 751px;" /></a>
<p>As you can see, the page markup is not very descriptive: the elements don&#8217;t
contain <code class="docutils literal"><span class="pre">id</span></code>, <code class="docutils literal"><span class="pre">class</span></code> or any attribute that clearly identifies them, so
we&#8217;ll use the ranking bars as a reference point to select the data to extract
when we construct our XPaths.</p>
<p>After using FireBug, we can see that each link is inside a <code class="docutils literal"><span class="pre">td</span></code> tag, which is
itself inside a <code class="docutils literal"><span class="pre">tr</span></code> tag that also contains the link&#8217;s ranking bar (in
another <code class="docutils literal"><span class="pre">td</span></code>).</p>
<p>So we can select the ranking bar, then find its parent (the <code class="docutils literal"><span class="pre">tr</span></code>), and then
finally, the link&#8217;s <code class="docutils literal"><span class="pre">td</span></code> (which contains the data we want to scrape).</p>
<p>This results in the following XPath:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td//a
</pre></div>
</div>
<p>It&#8217;s important to use the <a class="reference internal" href="index.html#topics-shell"><span>Scrapy shell</span></a> to test these
complex XPath expressions and make sure they work as expected.</p>
<p>Basically, that expression will look for the ranking bar&#8217;s <code class="docutils literal"><span class="pre">td</span></code> element, and
then select any <code class="docutils literal"><span class="pre">td</span></code> element who has a descendant <code class="docutils literal"><span class="pre">a</span></code> element whose
<code class="docutils literal"><span class="pre">href</span></code> attribute contains the string <code class="docutils literal"><span class="pre">#pagerank</span></code>&#8220;</p>
<p>Of course, this is not the only XPath, and maybe not the simpler one to select
that data. Another approach could be, for example, to find any <code class="docutils literal"><span class="pre">font</span></code> tags
that have that grey colour of the links,</p>
<p>Finally, we can write our <code class="docutils literal"><span class="pre">parse_category()</span></code> method:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def parse_category(self, response):
    # The path to website links in directory page
    links = response.xpath(&#39;//td[descendant::a[contains(@href, &quot;#pagerank&quot;)]]/following-sibling::td/font&#39;)

    for link in links:
        item = DirectoryItem()
        item[&#39;name&#39;] = link.xpath(&#39;a/text()&#39;).extract()
        item[&#39;url&#39;] = link.xpath(&#39;a/@href&#39;).extract()
        item[&#39;description&#39;] = link.xpath(&#39;font[2]/text()&#39;).extract()
        yield item
</pre></div>
</div>
<p>Be aware that you may find some elements which appear in Firebug but
not in the original HTML, such as the typical case of <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code>
elements.</p>
<p>or tags which Therefer   in page HTML
sources may on Firebug inspects the live DOM</p>
</div>
</div>
<span id="document-topics/leaks"></span><div class="section" id="debugging-memory-leaks">
<span id="topics-leaks"></span><h3>Debugging memory leaks<a class="headerlink" href="#debugging-memory-leaks" title="Permalink to this headline">¶</a></h3>
<p>In Scrapy, objects such as Requests, Responses and Items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it&#8217;s time to process
it. For more info see <a class="reference internal" href="index.html#topics-architecture"><span>Architecture overview</span></a>.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a &#8220;memory leak&#8221;.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a class="reference internal" href="#topics-leaks-trackrefs"><span>trackref</span></a>,
and you can also use a third-party library called <a class="reference internal" href="#topics-leaks-guppy"><span>Guppy</span></a> for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the <a class="reference internal" href="index.html#topics-telnetconsole"><span>Telnet Console</span></a>.</p>
<div class="section" id="common-causes-of-memory-leaks">
<h4>Common causes of memory leaks<a class="headerlink" href="#common-causes-of-memory-leaks" title="Permalink to this headline">¶</a></h4>
<p>It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">meta</span></code></a> attribute or the request callback function)
and that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of memory leaks
in Scrapy projects, and a quite difficult one to debug for newcomers.</p>
<p>In big projects, the spiders are typically written by different people and some
of those spiders could be &#8220;leaking&#8221; and thus affecting the rest of the other
(well-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.</p>
<p>The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, allocating resources on <a class="reference internal" href="index.html#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>
but not releasing them on <a class="reference internal" href="index.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> may cause problems if
you&#8217;re running <a class="reference internal" href="index.html#run-multiple-spiders"><span>multiple spiders per process</span></a>.</p>
<div class="section" id="too-many-requests">
<h5>Too Many Requests?<a class="headerlink" href="#too-many-requests" title="Permalink to this headline">¶</a></h5>
<p>By default Scrapy keeps the request queue in memory; it includes
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects and all objects
referenced in Request attributes (e.g. in <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">meta</span></code></a>).
While not necessarily a leak, this can take a lot of memory. Enabling
<a class="reference internal" href="index.html#topics-jobs"><span>persistent job queue</span></a> could help keeping memory usage
in control.</p>
</div>
</div>
<div class="section" id="debugging-memory-leaks-with-trackref">
<span id="topics-leaks-trackrefs"></span><h4>Debugging memory leaks with <code class="docutils literal"><span class="pre">trackref</span></code><a class="headerlink" href="#debugging-memory-leaks-with-trackref" title="Permalink to this headline">¶</a></h4>
<p><code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code> is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Requests,
Responses, Item and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the <code class="docutils literal"><span class="pre">prefs()</span></code> function which is an
alias to the <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> function:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>telnet localhost 6023

&gt;&gt;&gt; prefs()
Live References

ExampleSpider                       1   oldest: 15s ago
HtmlResponse                       10   oldest: 1s ago
Selector                            2   oldest: 0s ago
FormRequest                       878   oldest: 7s ago
</pre></div>
</div>
<p>As you can see, that report also shows the &#8220;age&#8221; of the oldest object in each
class. If you&#8217;re running multiple spiders per process chances are you can
figure out which spider is leaking by looking at the oldest request or response.
You can get the oldest object of each class using the
<a class="reference internal" href="#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><code class="xref py py-func docutils literal"><span class="pre">get_oldest()</span></code></a> function (from the telnet console).</p>
<div class="section" id="which-objects-are-tracked">
<h5>Which objects are tracked?<a class="headerlink" href="#which-objects-are-tracked" title="Permalink to this headline">¶</a></h5>
<p>The objects tracked by <code class="docutils literal"><span class="pre">trackrefs</span></code> are all from these classes (and all its
subclasses):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">scrapy.http.Request</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">scrapy.item.Item</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">scrapy.selector.Selector</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">scrapy.spiders.Spider</span></code></a></li>
</ul>
</div>
<div class="section" id="a-real-example">
<h5>A real example<a class="headerlink" href="#a-real-example" title="Permalink to this headline">¶</a></h5>
<p>Let&#8217;s see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>return Request(&quot;http://www.somenastyspider.com/product.php?pid=%d&quot; % product_id,
    callback=self.parse, meta={referer: response})
</pre></div>
</div>
<p>That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests&#8217; one, and that would definitely
cause memory leaks.</p>
<p>Let&#8217;s see how we can discover the cause (without knowing it
a-priori, of course) by using the <code class="docutils literal"><span class="pre">trackref</span></code> tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; prefs()
Live References

SomenastySpider                     1   oldest: 15s ago
HtmlResponse                     3890   oldest: 265s ago
Selector                            2   oldest: 0s ago
Request                          3878   oldest: 250s ago
</pre></div>
</div>
<p>The fact that there are so many live responses (and that they&#8217;re so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. The number of responses is similar to the number
of requests, so it looks like they are tied in a some way. We can now go
and check the code of the spider to discover the nasty line that is
generating the leaks (passing response references inside requests).</p>
<p>Sometimes extra information about live objects can be helpful.
Let&#8217;s check the oldest response:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; from scrapy.utils.trackref import get_oldest
&gt;&gt;&gt; r = get_oldest(&#39;HtmlResponse&#39;)
&gt;&gt;&gt; r.url
&#39;http://www.somenastyspider.com/product.php?pid=123&#39;
</pre></div>
</div>
<p>If you want to iterate over all objects, instead of getting the oldest one, you
can use the <a class="reference internal" href="#scrapy.utils.trackref.iter_all" title="scrapy.utils.trackref.iter_all"><code class="xref py py-func docutils literal"><span class="pre">scrapy.utils.trackref.iter_all()</span></code></a> function:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; from scrapy.utils.trackref import iter_all
&gt;&gt;&gt; [r.url for r in iter_all(&#39;HtmlResponse&#39;)]
[&#39;http://www.somenastyspider.com/product.php?pid=123&#39;,
 &#39;http://www.somenastyspider.com/product.php?pid=584&#39;,
...
</pre></div>
</div>
</div>
<div class="section" id="too-many-spiders">
<h5>Too many spiders?<a class="headerlink" href="#too-many-spiders" title="Permalink to this headline">¶</a></h5>
<p>If your project has too many spiders executed in parallel,
the output of <code class="xref py py-func docutils literal"><span class="pre">prefs()</span></code> can be difficult to read.
For this reason, that function has a <code class="docutils literal"><span class="pre">ignore</span></code> argument which can be used to
ignore a particular class (and all its subclases). For
example, this won&#8217;t show any live references to spiders:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; from scrapy.spiders import Spider
&gt;&gt;&gt; prefs(ignore=Spider)
</pre></div>
</div>
<span class="target" id="module-scrapy.utils.trackref"></span></div>
<div class="section" id="scrapy-utils-trackref-module">
<h5>scrapy.utils.trackref module<a class="headerlink" href="#scrapy-utils-trackref-module" title="Permalink to this headline">¶</a></h5>
<p>Here are the functions available in the <a class="reference internal" href="#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code></a> module.</p>
<dl class="class">
<dt id="scrapy.utils.trackref.object_ref">
<em class="property">class </em><code class="descclassname">scrapy.utils.trackref.</code><code class="descname">object_ref</code><a class="headerlink" href="#scrapy.utils.trackref.object_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Inherit from this class (instead of object) if you want to track live
instances with the <code class="docutils literal"><span class="pre">trackref</span></code> module.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.print_live_refs">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">print_live_refs</code><span class="sig-paren">(</span><em>class_name</em>, <em>ignore=NoneType</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.print_live_refs" title="Permalink to this definition">¶</a></dt>
<dd><p>Print a report of live references, grouped by class name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>ignore</strong> (<em>class or classes tuple</em>) &#8211; if given, all objects from the specified class (or tuple of
classes) will be ignored.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.get_oldest">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">get_oldest</code><span class="sig-paren">(</span><em>class_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.get_oldest" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the oldest object alive with the given class name, or <code class="docutils literal"><span class="pre">None</span></code> if
none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> first to get a list of all
tracked live objects per class name.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.iter_all">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">iter_all</code><span class="sig-paren">(</span><em>class_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.iter_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterator over all objects alive with the given class name, or
<code class="docutils literal"><span class="pre">None</span></code> if none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> first to get a list
of all tracked live objects per class name.</p>
</dd></dl>

</div>
</div>
<div class="section" id="debugging-memory-leaks-with-guppy">
<span id="topics-leaks-guppy"></span><h4>Debugging memory leaks with Guppy<a class="headerlink" href="#debugging-memory-leaks-with-guppy" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal"><span class="pre">trackref</span></code> provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks (Requests, Responses, Items, and Selectors). However, there are
other cases where the memory leaks could come from other (more or less obscure)
objects. If this is your case, and you can&#8217;t find your leaks using <code class="docutils literal"><span class="pre">trackref</span></code>,
you still have another resource: the <a class="reference external" href="https://pypi.python.org/pypi/guppy">Guppy library</a>.</p>
<p>If you use <code class="docutils literal"><span class="pre">pip</span></code>, you can install Guppy with the following command:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>pip install guppy
</pre></div>
</div>
<p>The telnet console also comes with a built-in shortcut (<code class="docutils literal"><span class="pre">hpy</span></code>) for accessing
Guppy heap objects. Here&#8217;s an example to view all Python objects available in
the heap using Guppy:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; x = hpy.heap()
&gt;&gt;&gt; x.bytype
Partition of a set of 297033 objects. Total size = 52587824 bytes.
 Index  Count   %     Size   % Cumulative  % Type
     0  22307   8 16423880  31  16423880  31 dict
     1 122285  41 12441544  24  28865424  55 str
     2  68346  23  5966696  11  34832120  66 tuple
     3    227   0  5836528  11  40668648  77 unicode
     4   2461   1  2222272   4  42890920  82 type
     5  16870   6  2024400   4  44915320  85 function
     6  13949   5  1673880   3  46589200  89 types.CodeType
     7  13422   5  1653104   3  48242304  92 list
     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern
     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers
&lt;1676 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>You can see that most space is used by dicts. Then, if you want to see from
which attribute those dicts are referenced, you could do:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&gt;&gt;&gt; x.bytype[0].byvia
Partition of a set of 22307 objects. Total size = 16423880 bytes.
 Index  Count   %     Size   % Cumulative  % Referred Via:
     0  10982  49  9416336  57   9416336  57 &#39;.__dict__&#39;
     1   1820   8  2681504  16  12097840  74 &#39;.__dict__&#39;, &#39;.func_globals&#39;
     2   3097  14  1122904   7  13220744  80
     3    990   4   277200   2  13497944  82 &quot;[&#39;cookies&#39;]&quot;
     4    987   4   276360   2  13774304  84 &quot;[&#39;cache&#39;]&quot;
     5    985   4   275800   2  14050104  86 &quot;[&#39;meta&#39;]&quot;
     6    897   4   251160   2  14301264  87 &#39;[2]&#39;
     7      1   0   196888   1  14498152  88 &quot;[&#39;moduleDict&#39;]&quot;, &quot;[&#39;modules&#39;]&quot;
     8    672   3   188160   1  14686312  89 &quot;[&#39;cb_kwargs&#39;]&quot;
     9     27   0   155016   1  14841328  90 &#39;[1]&#39;
&lt;333 more rows. Type e.g. &#39;_.more&#39; to view.&gt;
</pre></div>
</div>
<p>As you can see, the Guppy module is very powerful but also requires some deep
knowledge about Python internals. For more info about Guppy, refer to the
<a class="reference external" href="http://guppy-pe.sourceforge.net/">Guppy documentation</a>.</p>
</div>
<div class="section" id="leaks-without-leaks">
<span id="topics-leaks-without-leaks"></span><h4>Leaks without leaks<a class="headerlink" href="#leaks-without-leaks" title="Permalink to this headline">¶</a></h4>
<p>Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.evanjones.ca/python-memory.html">Python Memory Management</a></li>
<li><a class="reference external" href="http://www.evanjones.ca/python-memory-part2.html">Python Memory Management Part 2</a></li>
<li><a class="reference external" href="http://www.evanjones.ca/python-memory-part3.html">Python Memory Management Part 3</a></li>
</ul>
<p>The improvements proposed by Evan Jones, which are detailed in <a class="reference external" href="http://www.evanjones.ca/memoryallocator/">this paper</a>,
got merged in Python 2.5, but this only reduces the problem, it doesn&#8217;t fix it
completely. To quote the paper:</p>
<blockquote>
<div><em>Unfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a large
issue. An application could have many megabytes of free memory, scattered
throughout all the arenas, but it will be unable to free any of it. This is
a problem experienced by all memory allocators. The only way to solve it is
to move to a compacting garbage collector, which is able to move objects in
memory. This would require significant changes to the Python interpreter.</em></div></blockquote>
<p>To keep memory consumption reasonable you can split the job into several
smaller jobs or enable <a class="reference internal" href="index.html#topics-jobs"><span>persistent job queue</span></a>
and stop/start spider from time to time.</p>
</div>
</div>
<span id="document-topics/media-pipeline"></span><div class="section" id="downloading-and-processing-files-and-images">
<span id="topics-media-pipeline"></span><h3>Downloading and processing files and images<a class="headerlink" href="#downloading-and-processing-files-and-images" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides reusable <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>item pipelines</em></a> for
downloading files attached to a particular item (for example, when you scrape
products and also want to download their images locally). These pipelines share
a bit of functionality and structure (we refer to them as media pipelines), but
typically you&#8217;ll either use the Files Pipeline or the Images Pipeline.</p>
<p>Both pipelines implement these features:</p>
<ul class="simple">
<li>Avoid re-downloading media that was downloaded recently</li>
<li>Specifying where to store the media (filesystem directory, Amazon S3 bucket)</li>
</ul>
<p>The Images Pipeline has a few extra functions for processing images:</p>
<ul class="simple">
<li>Convert all downloaded images to a common format (JPG) and mode (RGB)</li>
<li>Thumbnail generation</li>
<li>Check images width/height to make sure they meet a minimum constraint</li>
</ul>
<p>The pipelines also keep an internal queue of those media URLs which are currently
being scheduled for download, and connect those responses that arrive containing
the same media to that queue. This avoids downloading the same media more than
once when it&#8217;s shared by several items.</p>
<div class="section" id="using-the-files-pipeline">
<h4>Using the Files Pipeline<a class="headerlink" href="#using-the-files-pipeline" title="Permalink to this headline">¶</a></h4>
<p>The typical workflow, when using the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code> goes like
this:</p>
<ol class="arabic simple">
<li>In a Spider, you scrape an item and put the URLs of the desired into a
<code class="docutils literal"><span class="pre">file_urls</span></code> field.</li>
<li>The item is returned from the spider and goes to the item pipeline.</li>
<li>When the item reaches the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code>, the URLs in the
<code class="docutils literal"><span class="pre">file_urls</span></code> field are scheduled for download using the standard
Scrapy scheduler and downloader (which means the scheduler and downloader
middlewares are reused), but with a higher priority, processing them before other
pages are scraped. The item remains &#8220;locked&#8221; at that particular pipeline stage
until the files have finish downloading (or fail for some reason).</li>
<li>When the files are downloaded, another field (<code class="docutils literal"><span class="pre">files</span></code>) will be populated
with the results. This field will contain a list of dicts with information
about the downloaded files, such as the downloaded path, the original
scraped url (taken from the <code class="docutils literal"><span class="pre">file_urls</span></code> field) , and the file checksum.
The files in the list of the <code class="docutils literal"><span class="pre">files</span></code> field will retain the same order of
the original <code class="docutils literal"><span class="pre">file_urls</span></code> field. If some file failed downloading, an
error will be logged and the file won&#8217;t be present in the <code class="docutils literal"><span class="pre">files</span></code> field.</li>
</ol>
</div>
<div class="section" id="using-the-images-pipeline">
<h4>Using the Images Pipeline<a class="headerlink" href="#using-the-images-pipeline" title="Permalink to this headline">¶</a></h4>
<p>Using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></code></a> is a lot like using the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code>,
except the default field names used are different: you use <code class="docutils literal"><span class="pre">image_urls</span></code> for
the image URLs of an item and it will populate an <code class="docutils literal"><span class="pre">images</span></code> field for the information
about the downloaded images.</p>
<p>The advantage of using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></code></a> for image files is that you
can configure some extra functions like generating thumbnails and filtering
the images based on their size.</p>
<p>The Images Pipeline uses <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> for thumbnailing and normalizing images to
JPEG/RGB format, so you need to install this library in order to use it.
<a class="reference external" href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> (PIL) should also work in most cases, but it is known
to cause troubles in some setups, so we recommend to use <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> instead of
PIL.</p>
</div>
<div class="section" id="enabling-your-media-pipeline">
<span id="topics-media-pipeline-enabling"></span><h4>Enabling your Media Pipeline<a class="headerlink" href="#enabling-your-media-pipeline" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:setting-IMAGES_STORE"></span><p id="std:setting-FILES_STORE">To enable your media pipeline you must first add it to your project
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For Images Pipeline, use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>ITEM_PIPELINES = {&#39;scrapy.pipelines.images.ImagesPipeline&#39;: 1}
</pre></div>
</div>
<p>For Files Pipeline, use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>ITEM_PIPELINES = {&#39;scrapy.pipelines.files.FilesPipeline&#39;: 1}
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also use both the Files and Images Pipeline at the same time.</p>
</div>
<p>Then, configure the target storage setting to a valid value that will be used
for storing the downloaded images. Otherwise the pipeline will remain disabled,
even if you include it in the <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For the Files Pipeline, set the <a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE</span></code></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>FILES_STORE = &#39;/path/to/valid/dir&#39;
</pre></div>
</div>
<p>For the Images Pipeline, set the <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></code></a> setting:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>IMAGES_STORE = &#39;/path/to/valid/dir&#39;
</pre></div>
</div>
</div>
<div class="section" id="supported-storage">
<h4>Supported Storage<a class="headerlink" href="#supported-storage" title="Permalink to this headline">¶</a></h4>
<p>File system is currently the only officially supported storage, but there is
also (undocumented) support for storing files in <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<div class="section" id="file-system-storage">
<h5>File system storage<a class="headerlink" href="#file-system-storage" title="Permalink to this headline">¶</a></h5>
<p>The files are stored using a <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of their URLs for the file names.</p>
<p>For example, the following image URL:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>http://www.example.com/image.jpg
</pre></div>
</div>
<p>Whose <cite>SHA1 hash</cite> is:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>3afec3b4765f8f0a07b78f98c07b83f013567a0a
</pre></div>
</div>
<p>Will be downloaded and stored in the following file:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&lt;IMAGES_STORE&gt;/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">&lt;IMAGES_STORE&gt;</span></code> is the directory defined in <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></code></a> setting
for the Images Pipeline.</li>
<li><code class="docutils literal"><span class="pre">full</span></code> is a sub-directory to separate full images from thumbnails (if
used). For more info see <a class="reference internal" href="#topics-images-thumbnails"><span>Thumbnail generation for images</span></a>.</li>
</ul>
</div>
</div>
<div class="section" id="usage-example">
<h4>Usage example<a class="headerlink" href="#usage-example" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:setting-FILES_URLS_FIELD"></span><span class="target" id="std:setting-FILES_RESULT_FIELD"></span><span class="target" id="std:setting-IMAGES_URLS_FIELD"></span><p id="std:setting-IMAGES_RESULT_FIELD">In order to use a media pipeline first, <a class="reference internal" href="#topics-media-pipeline-enabling"><span>enable it</span></a>.</p>
<p>Then, if a spider returns a dict with the URLs key (<code class="docutils literal"><span class="pre">file_urls</span></code> or
<code class="docutils literal"><span class="pre">image_urls</span></code>, for the Files or Images Pipeline respectively), the pipeline will
put the results under respective key (<code class="docutils literal"><span class="pre">files</span></code> or <code class="docutils literal"><span class="pre">images</span></code>).</p>
<p>If you prefer to use <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>, then define a custom item with the
necessary fields, like in this example for Images Pipeline:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy

class MyItem(scrapy.Item):

    # ... other item fields ...
    image_urls = scrapy.Field()
    images = scrapy.Field()
</pre></div>
</div>
<p>If you want to use another field name for the URLs key or for the results key,
it is also possible to override it.</p>
<p>For the Files Pipeline, set <a class="reference internal" href="#std:setting-FILES_URLS_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">FILES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-FILES_RESULT_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">FILES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>FILES_URLS_FIELD = &#39;field_name_for_your_files_urls&#39;
FILES_RESULT_FIELD = &#39;field_name_for_your_processed_files&#39;
</pre></div>
</div>
<p>For the Images Pipeline, set <a class="reference internal" href="#std:setting-IMAGES_URLS_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-IMAGES_RESULT_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>IMAGES_URLS_FIELD = &#39;field_name_for_your_images_urls&#39;
IMAGES_RESULT_FIELD = &#39;field_name_for_your_processed_images&#39;
</pre></div>
</div>
<p>If you need something more complex and want to override the custom pipeline
behaviour, see <a class="reference internal" href="#topics-media-pipeline-override"><span>Extending the Media Pipelines</span></a>.</p>
</div>
<div class="section" id="additional-features">
<h4>Additional features<a class="headerlink" href="#additional-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="file-expiration">
<h5>File expiration<a class="headerlink" href="#file-expiration" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-IMAGES_EXPIRES"></span><p id="std:setting-FILES_EXPIRES">The Image Pipeline avoids downloading files that were downloaded recently. To
adjust this retention delay use the <a class="reference internal" href="#std:setting-FILES_EXPIRES"><code class="xref std std-setting docutils literal"><span class="pre">FILES_EXPIRES</span></code></a> setting (or
<a class="reference internal" href="#std:setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_EXPIRES</span></code></a>, in case of Images Pipeline), which
specifies the delay in number of days:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span># 120 days of delay for files expiration
FILES_EXPIRES = 120

# 30 days of delay for images expiration
IMAGES_EXPIRES = 30
</pre></div>
</div>
<p>The default value for both settings is 90 days.</p>
</div>
<div class="section" id="thumbnail-generation-for-images">
<span id="topics-images-thumbnails"></span><h5>Thumbnail generation for images<a class="headerlink" href="#thumbnail-generation-for-images" title="Permalink to this headline">¶</a></h5>
<p>The Images Pipeline can automatically create thumbnails of the downloaded
images.</p>
<p id="std:setting-IMAGES_THUMBS">In order use this feature, you must set <a class="reference internal" href="#std:setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></code></a> to a dictionary
where the keys are the thumbnail names and the values are their dimensions.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>IMAGES_THUMBS = {
    &#39;small&#39;: (50, 50),
    &#39;big&#39;: (270, 270),
}
</pre></div>
</div>
<p>When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">&lt;size_name&gt;</span></code> is the one specified in the <a class="reference internal" href="#std:setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></code></a>
dictionary keys (<code class="docutils literal"><span class="pre">small</span></code>, <code class="docutils literal"><span class="pre">big</span></code>, etc)</li>
<li><code class="docutils literal"><span class="pre">&lt;image_id&gt;</span></code> is the <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of the image url</li>
</ul>
<p>Example of image files stored using <code class="docutils literal"><span class="pre">small</span></code> and <code class="docutils literal"><span class="pre">big</span></code> thumbnail names:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
</pre></div>
</div>
<p>The first one is the full image, as downloaded from the site.</p>
</div>
<div class="section" id="filtering-out-small-images">
<h5>Filtering out small images<a class="headerlink" href="#filtering-out-small-images" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-IMAGES_MIN_HEIGHT"></span><p id="std:setting-IMAGES_MIN_WIDTH">When using the Images Pipeline, you can drop images which are too small, by
specifying the minimum allowed size in the <a class="reference internal" href="#std:setting-IMAGES_MIN_HEIGHT"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_HEIGHT</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_MIN_WIDTH"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_WIDTH</span></code></a> settings.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The size constraints don&#8217;t affect thumbnail generation at all.</p>
</div>
<p>It is possible to set just one size constraint or both. When setting both of
them, only images that satisfy both minimum sizes will be saved. For the
above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will
all be dropped because at least one dimension is shorter than the constraint.</p>
<p>By default, there are no size constraints, so all images are processed.</p>
</div>
</div>
<div class="section" id="module-scrapy.pipelines.files">
<span id="extending-the-media-pipelines"></span><span id="topics-media-pipeline-override"></span><h4>Extending the Media Pipelines<a class="headerlink" href="#module-scrapy.pipelines.files" title="Permalink to this headline">¶</a></h4>
<p>See here the methods that you can override in your custom Files Pipeline:</p>
<dl class="class">
<dt id="scrapy.pipelines.files.FilesPipeline">
<em class="property">class </em><code class="descclassname">scrapy.pipelines.files.</code><code class="descname">FilesPipeline</code><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.pipelines.files.FilesPipeline.get_media_requests">
<code class="descname">get_media_requests</code><span class="sig-paren">(</span><em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you can override the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method and return a Request for each
file URL:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def get_media_requests(self, item, info):
    for file_url in item[&#39;file_urls&#39;]:
        yield scrapy.Request(file_url)
</pre></div>
</div>
<p>Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method, as a list of 2-element tuples.
Each tuple will contain <code class="docutils literal"><span class="pre">(success,</span> <span class="pre">file_info_or_error)</span></code> where:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">success</span></code> is a boolean which is <code class="docutils literal"><span class="pre">True</span></code> if the image was downloaded
successfully or <code class="docutils literal"><span class="pre">False</span></code> if it failed for some reason</li>
<li><code class="docutils literal"><span class="pre">file_info_or_error</span></code> is a dict containing the following keys (if success
is <code class="docutils literal"><span class="pre">True</span></code>) or a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> if there was a problem.<ul>
<li><code class="docutils literal"><span class="pre">url</span></code> - the url where the file was downloaded from. This is the url of
the request returned from the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a>
method.</li>
<li><code class="docutils literal"><span class="pre">path</span></code> - the path (relative to <a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE</span></code></a>) where the file
was stored</li>
<li><code class="docutils literal"><span class="pre">checksum</span></code> - a <a class="reference external" href="https://en.wikipedia.org/wiki/MD5">MD5 hash</a> of the image contents</li>
</ul>
</li>
</ul>
<p>The list of tuples received by <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> is
guaranteed to retain the same order of the requests returned from the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method.</p>
<p>Here&#8217;s a typical value of the <code class="docutils literal"><span class="pre">results</span></code> argument:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>[(True,
  {&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,
   &#39;path&#39;: &#39;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&#39;,
   &#39;url&#39;: &#39;http://www.example.com/files/product1.pdf&#39;}),
 (False,
  Failure(...))]
</pre></div>
</div>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method returns <code class="docutils literal"><span class="pre">None</span></code> which
means there are no files to download for the item.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.files.FilesPipeline.item_completed">
<code class="descname">item_completed</code><span class="sig-paren">(</span><em>results</em>, <em>items</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.item_completed()</span></code></a> method called when all file
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.</p>
<p>Here is an example of the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method where we
store the downloaded file paths (passed in results) in the <code class="docutils literal"><span class="pre">file_paths</span></code>
item field, and we drop the item if it doesn&#8217;t contain any files:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    file_paths = [x[&#39;path&#39;] for ok, x in results if ok]
    if not file_paths:
        raise DropItem(&quot;Item contains no files&quot;)
    item[&#39;file_paths&#39;] = file_paths
    return item
</pre></div>
</div>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-scrapy.pipelines.images"></span><p>See here the methods that you can override in your custom Images Pipeline:</p>
<dl class="class">
<dt id="scrapy.pipelines.images.ImagesPipeline">
<em class="property">class </em><code class="descclassname">scrapy.pipelines.images.</code><code class="descname">ImagesPipeline</code><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></code></a> is an extension of the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code>,
customizing the field names and adding custom behavior for images.</div></blockquote>
<dl class="method">
<dt id="scrapy.pipelines.images.ImagesPipeline.get_media_requests">
<code class="descname">get_media_requests</code><span class="sig-paren">(</span><em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>Works the same way as <code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.get_media_requests()</span></code> method,
but using a different field name for image urls.</p>
<p>Must return a Request for each image URL.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.images.ImagesPipeline.item_completed">
<code class="descname">item_completed</code><span class="sig-paren">(</span><em>results</em>, <em>items</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">ImagesPipeline.item_completed()</span></code></a> method is called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>Works the same way as <code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.item_completed()</span></code> method,
but using a different field names for storing image downloading results.</p>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="custom-images-pipeline-example">
<h4>Custom Images pipeline example<a class="headerlink" href="#custom-images-pipeline-example" title="Permalink to this headline">¶</a></h4>
<p>Here is a full example of the Images Pipeline whose methods are examplified
above:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import scrapy
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item[&#39;image_urls&#39;]:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x[&#39;path&#39;] for ok, x in results if ok]
        if not image_paths:
            raise DropItem(&quot;Item contains no images&quot;)
        item[&#39;image_paths&#39;] = image_paths
        return item
</pre></div>
</div>
</div>
</div>
<span id="document-topics/ubuntu"></span><div class="section" id="ubuntu-packages">
<span id="topics-ubuntu"></span><h3>Ubuntu packages<a class="headerlink" href="#ubuntu-packages" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p><a class="reference external" href="http://scrapinghub.com/">Scrapinghub</a> publishes apt-gettable packages which are generally fresher than
those in Ubuntu, and more stable too since they&#8217;re continuously built from
<a class="reference external" href="https://github.com/scrapy/scrapy">GitHub repo</a> (master &amp; stable branches) and so they contain the latest bug
fixes.</p>
<p>To use the packages:</p>
<ol class="arabic">
<li><p class="first">Import the GPG key used to sign Scrapy packages into APT keyring:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7
</pre></div>
</div>
</li>
<li><p class="first">Create <cite>/etc/apt/sources.list.d/scrapy.list</cite> file using the following command:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>echo &#39;deb http://archive.scrapy.org/ubuntu scrapy main&#39; | sudo tee /etc/apt/sources.list.d/scrapy.list
</pre></div>
</div>
</li>
<li><p class="first">Update package lists and install the scrapy package:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>sudo apt-get update &amp;&amp; sudo apt-get install scrapy
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Repeat step 3 if you are trying to upgrade Scrapy.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><cite>python-scrapy</cite> is a different package provided by official debian
repositories, it&#8217;s very outdated and it isn&#8217;t supported by Scrapy team.</p>
</div>
</div>
<span id="document-topics/deploy"></span><div class="section" id="deploying-spiders">
<span id="topics-deploy"></span><h3>Deploying Spiders<a class="headerlink" href="#deploying-spiders" title="Permalink to this headline">¶</a></h3>
<p>This section describes the different options you have for deploying your Scrapy
spiders to run them on a regular basis. Running Scrapy spiders in your local
machine is very convenient for the (early) development stage, but not so much
when you need to execute long-running spiders or move spiders to run in
production continuously. This is where the solutions for deploying Scrapy
spiders come in.</p>
<p>Popular choices for deploying Scrapy spiders are:</p>
<ul class="simple">
<li><a class="reference internal" href="#deploy-scrapyd"><span>Scrapyd</span></a> (open source)</li>
<li><a class="reference internal" href="#deploy-scrapy-cloud"><span>Scrapy Cloud</span></a> (cloud-based)</li>
</ul>
<div class="section" id="deploying-to-a-scrapyd-server">
<span id="deploy-scrapyd"></span><h4>Deploying to a Scrapyd Server<a class="headerlink" href="#deploying-to-a-scrapyd-server" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://github.com/scrapy/scrapyd">Scrapyd</a> is an open source application to run Scrapy spiders. It provides
a server with HTTP API, capable of running and monitoring Scrapy spiders.</p>
<p>To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by
the <a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a> package. Please refer to the <a class="reference external" href="http://scrapyd.readthedocs.org/en/latest/deploy.html">scrapyd-deploy
documentation</a> for more information.</p>
<p>Scrapyd is maintained by some of the Scrapy developers.</p>
</div>
<div class="section" id="deploying-to-scrapy-cloud">
<span id="deploy-scrapy-cloud"></span><h4>Deploying to Scrapy Cloud<a class="headerlink" href="#deploying-to-scrapy-cloud" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://scrapinghub.com/scrapy-cloud/">Scrapy Cloud</a> is a hosted, cloud-based service by <a class="reference external" href="http://scrapinghub.com/">Scrapinghub</a>,
the company behind Scrapy.</p>
<p>Scrapy Cloud removes the need to setup and monitor servers
and provides a nice UI to manage spiders and review scraped items,
logs and stats.</p>
<p>To deploy spiders to Scrapy Cloud you can use the <a class="reference external" href="http://doc.scrapinghub.com/shub.html">shub</a> command line tool.
Please refer to the <a class="reference external" href="http://doc.scrapinghub.com/scrapy-cloud.html">Scrapy Cloud documentation</a> for more information.</p>
<p>Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> file
just like <code class="docutils literal"><span class="pre">scrapyd-deploy</span></code>.</p>
</div>
</div>
<span id="document-topics/autothrottle"></span><div class="section" id="autothrottle-extension">
<span id="topics-autothrottle"></span><h3>AutoThrottle extension<a class="headerlink" href="#autothrottle-extension" title="Permalink to this headline">¶</a></h3>
<p>This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.</p>
<div class="section" id="design-goals">
<h4>Design goals<a class="headerlink" href="#design-goals" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>be nicer to sites instead of using default download delay of zero</li>
<li>automatically adjust scrapy to the optimum crawling speed, so the user
doesn&#8217;t have to tune the download delays to find the optimum one.
The user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.</li>
</ol>
</div>
<div class="section" id="how-it-works">
<span id="autothrottle-algorithm"></span><h4>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h4>
<p>AutoThrottle extension adjusts download delays dynamically to make spider send
<a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> concurrent requests on average
to each remote website.</p>
<p>It uses download latency to compute the delays. The main idea is the
following: if a server needs <code class="docutils literal"><span class="pre">latency</span></code> seconds to respond, a client
should send a request each <code class="docutils literal"><span class="pre">latency/N</span></code> seconds to have <code class="docutils literal"><span class="pre">N</span></code> requests
processed in parallel.</p>
<p>Instead of adjusting the delays one can just set a small fixed
download delay and impose hard limits on concurrency using
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options. It will provide a similar
effect, but there are some important differences:</p>
<ul class="simple">
<li>because the download delay is small there will be occasional bursts
of requests;</li>
<li>often non-200 (error) responses can be returned faster than regular
responses, so with a small download delay and a hard concurrency limit
crawler will be sending requests to server faster when server starts to
return errors. But this is an opposite of what crawler should do - in case
of errors it makes more sense to slow down: these errors may be caused by
the high request rate.</li>
</ul>
<p>AutoThrottle doesn&#8217;t have these issues.</p>
</div>
<div class="section" id="throttling-algorithm">
<h4>Throttling algorithm<a class="headerlink" href="#throttling-algorithm" title="Permalink to this headline">¶</a></h4>
<p>AutoThrottle algorithm adjusts download delays based on the following rules:</p>
<ol class="arabic simple">
<li>spiders always start with a download delay of
<a class="reference internal" href="#std:setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a>;</li>
<li>when a response is received, the target download delay is calculated as
<code class="docutils literal"><span class="pre">latency</span> <span class="pre">/</span> <span class="pre">N</span></code> where <code class="docutils literal"><span class="pre">latency</span></code> is a latency of the response,
and <code class="docutils literal"><span class="pre">N</span></code> is <a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a>.</li>
<li>download delay for next requests is set to the average of previous
download delay and the target download delay;</li>
<li>latencies of non-200 responses are not allowed to decrease the delay;</li>
<li>download delay can&#8217;t become less than <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> or greater
than <a class="reference internal" href="#std:setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options and
never set a download delay lower than <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
</div>
<p id="download-latency">In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.</p>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>The settings used to control the AutoThrottle extension are:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_DEBUG</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a></li>
</ul>
<p>For more information see <a class="reference internal" href="#autothrottle-algorithm"><span>How it works</span></a>.</p>
<div class="section" id="autothrottle-enabled">
<span id="std:setting-AUTOTHROTTLE_ENABLED"></span><h5>AUTOTHROTTLE_ENABLED<a class="headerlink" href="#autothrottle-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enables the AutoThrottle extension.</p>
</div>
<div class="section" id="autothrottle-start-delay">
<span id="std:setting-AUTOTHROTTLE_START_DELAY"></span><h5>AUTOTHROTTLE_START_DELAY<a class="headerlink" href="#autothrottle-start-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">5.0</span></code></p>
<p>The initial download delay (in seconds).</p>
</div>
<div class="section" id="autothrottle-max-delay">
<span id="std:setting-AUTOTHROTTLE_MAX_DELAY"></span><h5>AUTOTHROTTLE_MAX_DELAY<a class="headerlink" href="#autothrottle-max-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">60.0</span></code></p>
<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>
</div>
<div class="section" id="autothrottle-target-concurrency">
<span id="std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"></span><h5>AUTOTHROTTLE_TARGET_CONCURRENCY<a class="headerlink" href="#autothrottle-target-concurrency" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">1.0</span></code></p>
<p>Average number of requests Scrapy should be sending in parallel to remote
websites.</p>
<p>By default, AutoThrottle adjusts the delay to send a single
concurrent request to each of the remote websites. Set this option to
a higher value (e.g. <code class="docutils literal"><span class="pre">2.0</span></code>) to increase the throughput and the load on remote
servers. A lower <code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> value
(e.g. <code class="docutils literal"><span class="pre">0.5</span></code>) makes the crawler more conservative and polite.</p>
<p>Note that <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>
and <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options are still respected
when AutoThrottle extension is enabled. This means that if
<code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> is set to a value higher than
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>, the crawler won&#8217;t reach this number
of concurrent requests.</p>
<p>At every given time point Scrapy can be sending more or less concurrent
requests than <code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>; it is a suggested
value the crawler tries to approach, not a hard limit.</p>
</div>
<div class="section" id="autothrottle-debug">
<span id="std:setting-AUTOTHROTTLE_DEBUG"></span><h5>AUTOTHROTTLE_DEBUG<a class="headerlink" href="#autothrottle-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.</p>
</div>
</div>
</div>
<span id="document-topics/benchmarking"></span><div class="section" id="benchmarking">
<span id="id1"></span><h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.</p>
<p>To run it use:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy bench
</pre></div>
</div>
<p>You should see an output like this:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>2013-05-16 13:08:46-0300 [scrapy] INFO: Scrapy 0.17.0 started (bot: scrapybot)
2013-05-16 13:08:47-0300 [scrapy] INFO: Spider opened
2013-05-16 13:08:47-0300 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:48-0300 [scrapy] INFO: Crawled 74 pages (at 4440 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:49-0300 [scrapy] INFO: Crawled 143 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:50-0300 [scrapy] INFO: Crawled 210 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:51-0300 [scrapy] INFO: Crawled 274 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:52-0300 [scrapy] INFO: Crawled 343 pages (at 4140 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:53-0300 [scrapy] INFO: Crawled 410 pages (at 4020 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:54-0300 [scrapy] INFO: Crawled 474 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:55-0300 [scrapy] INFO: Crawled 538 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:56-0300 [scrapy] INFO: Crawled 602 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [scrapy] INFO: Closing spider (closespider_timeout)
2013-05-16 13:08:57-0300 [scrapy] INFO: Crawled 666 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2013-05-16 13:08:57-0300 [scrapy] INFO: Dumping Scrapy stats:
    {&#39;downloader/request_bytes&#39;: 231508,
     &#39;downloader/request_count&#39;: 682,
     &#39;downloader/request_method_count/GET&#39;: 682,
     &#39;downloader/response_bytes&#39;: 1172802,
     &#39;downloader/response_count&#39;: 682,
     &#39;downloader/response_status_count/200&#39;: 682,
     &#39;finish_reason&#39;: &#39;closespider_timeout&#39;,
     &#39;finish_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 57, 985539),
     &#39;log_count/INFO&#39;: 14,
     &#39;request_depth_max&#39;: 34,
     &#39;response_received_count&#39;: 682,
     &#39;scheduler/dequeued&#39;: 682,
     &#39;scheduler/dequeued/memory&#39;: 682,
     &#39;scheduler/enqueued&#39;: 12767,
     &#39;scheduler/enqueued/memory&#39;: 12767,
     &#39;start_time&#39;: datetime.datetime(2013, 5, 16, 16, 8, 47, 676539)}
2013-05-16 13:08:57-0300 [scrapy] INFO: Spider closed (closespider_timeout)
</pre></div>
</div>
<p>That tells you that Scrapy is able to crawl about 3900 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it&#8217;s written.</p>
<p>In the future, more cases will be added to the benchmarking suite to cover
other common scenarios.</p>
</div>
<span id="document-topics/jobs"></span><div class="section" id="jobs-pausing-and-resuming-crawls">
<span id="topics-jobs"></span><h3>Jobs: pausing and resuming crawls<a class="headerlink" href="#jobs-pausing-and-resuming-crawls" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, for big sites, it&#8217;s desirable to pause crawls and be able to resume
them later.</p>
<p>Scrapy supports this functionality out of the box by providing the following
facilities:</p>
<ul class="simple">
<li>a scheduler that persists scheduled requests on disk</li>
<li>a duplicates filter that persists visited requests on disk</li>
<li>an extension that keeps some spider state (key/value pairs) persistent
between batches</li>
</ul>
<div class="section" id="job-directory">
<h4>Job directory<a class="headerlink" href="#job-directory" title="Permalink to this headline">¶</a></h4>
<p>To enable persistence support you just need to define a <em>job directory</em> through
the <code class="docutils literal"><span class="pre">JOBDIR</span></code> setting. This directory will be for storing all required data to
keep the state of a single job (ie. a spider run).  It&#8217;s important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it&#8217;s meant to be used for storing the state of
a <em>single</em> job.</p>
</div>
<div class="section" id="how-to-use-it">
<h4>How to use it<a class="headerlink" href="#how-to-use-it" title="Permalink to this headline">¶</a></h4>
<p>To start a spider with persistence supported enabled, run it like this:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending
a signal), and resume it later by issuing the same command:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy crawl somespider -s JOBDIR=crawls/somespider-1
</pre></div>
</div>
</div>
<div class="section" id="keeping-persistent-state-between-batches">
<h4>Keeping persistent state between batches<a class="headerlink" href="#keeping-persistent-state-between-batches" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you&#8217;ll want to keep some persistent spider state between pause/resume
batches. You can use the <code class="docutils literal"><span class="pre">spider.state</span></code> attribute for that, which should be a
dict. There&#8217;s a built-in extension that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.</p>
<p>Here&#8217;s an example of a callback that uses the spider state (other spider code
is omitted for brevity):</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def parse_item(self, response):
    # parse item here
    self.state[&#39;items_count&#39;] = self.state.get(&#39;items_count&#39;, 0) + 1
</pre></div>
</div>
</div>
<div class="section" id="persistence-gotchas">
<h4>Persistence gotchas<a class="headerlink" href="#persistence-gotchas" title="Permalink to this headline">¶</a></h4>
<p>There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:</p>
<div class="section" id="cookies-expiration">
<h5>Cookies expiration<a class="headerlink" href="#cookies-expiration" title="Permalink to this headline">¶</a></h5>
<p>Cookies may expire. So, if you don&#8217;t resume your spider quickly the requests
scheduled may no longer work. This won&#8217;t be an issue if you spider doesn&#8217;t rely
on cookies.</p>
</div>
<div class="section" id="request-serialization">
<h5>Request serialization<a class="headerlink" href="#request-serialization" title="Permalink to this headline">¶</a></h5>
<p>Requests must be serializable by the <cite>pickle</cite> module, in order for persistence
to work, so you should make sure that your requests are serializable.</p>
<p>The most common issue here is to use <code class="docutils literal"><span class="pre">lambda</span></code> functions on request callbacks that
can&#8217;t be persisted.</p>
<p>So, for example, this won&#8217;t work:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, callback=lambda r: self.other_callback(r, somearg))

def other_callback(self, response, somearg):
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
<p>But this will:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def some_callback(self, response):
    somearg = &#39;test&#39;
    return scrapy.Request(&#39;http://www.example.com&#39;, callback=self.other_callback, meta={&#39;somearg&#39;: somearg})

def other_callback(self, response):
    somearg = response.meta[&#39;somearg&#39;]
    print &quot;the argument passed is:&quot;, somearg
</pre></div>
</div>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-faq"><em>Frequently Asked Questions</em></a></dt>
<dd>Get answers to most frequently asked questions.</dd>
<dt><a class="reference internal" href="index.html#document-topics/debug"><em>Debugging Spiders</em></a></dt>
<dd>Learn how to debug common problems of your scrapy spider.</dd>
<dt><a class="reference internal" href="index.html#document-topics/contracts"><em>Spiders Contracts</em></a></dt>
<dd>Learn how to use contracts for testing your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/practices"><em>Common Practices</em></a></dt>
<dd>Get familiar with some Scrapy common practices.</dd>
<dt><a class="reference internal" href="index.html#document-topics/broad-crawls"><em>Broad Crawls</em></a></dt>
<dd>Tune Scrapy for crawling a lot domains in parallel.</dd>
<dt><a class="reference internal" href="index.html#document-topics/firefox"><em>Using Firefox for scraping</em></a></dt>
<dd>Learn how to scrape with Firefox and some useful add-ons.</dd>
<dt><a class="reference internal" href="index.html#document-topics/firebug"><em>Using Firebug for scraping</em></a></dt>
<dd>Learn how to scrape efficiently using Firebug.</dd>
<dt><a class="reference internal" href="index.html#document-topics/leaks"><em>Debugging memory leaks</em></a></dt>
<dd>Learn how to find and get rid of memory leaks in your crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/media-pipeline"><em>Downloading and processing files and images</em></a></dt>
<dd>Download files and/or images associated with your scraped items.</dd>
<dt><a class="reference internal" href="index.html#document-topics/ubuntu"><em>Ubuntu packages</em></a></dt>
<dd>Install latest Scrapy packages easily on Ubuntu</dd>
<dt><a class="reference internal" href="index.html#document-topics/deploy"><em>Deploying Spiders</em></a></dt>
<dd>Deploying your Scrapy spiders and run them in a remote server.</dd>
<dt><a class="reference internal" href="index.html#document-topics/autothrottle"><em>AutoThrottle extension</em></a></dt>
<dd>Adjust crawl rate dynamically based on load.</dd>
<dt><a class="reference internal" href="index.html#document-topics/benchmarking"><em>Benchmarking</em></a></dt>
<dd>Check how Scrapy performs on your hardware.</dd>
<dt><a class="reference internal" href="index.html#document-topics/jobs"><em>Jobs: pausing and resuming crawls</em></a></dt>
<dd>Learn how to pause and resume crawls for large spiders.</dd>
</dl>
</div>
<div class="section" id="extending-scrapy">
<span id="id5"></span><h2>Extending Scrapy<a class="headerlink" href="#extending-scrapy" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id6">
<span id="document-topics/architecture"></span><div class="section" id="architecture-overview">
<span id="topics-architecture"></span><h3>Architecture overview<a class="headerlink" href="#architecture-overview" title="Permalink to this headline">¶</a></h3>
<p>This document describes the architecture of Scrapy and how its components
interact.</p>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<p>The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the green arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.</p>
<a class="reference internal image-reference" href="_images/scrapy_architecture.png"><img alt="Scrapy architecture" src="_images/scrapy_architecture.png" style="width: 700px; height: 494px;" /></a>
</div>
<div class="section" id="components">
<h4>Components<a class="headerlink" href="#components" title="Permalink to this headline">¶</a></h4>
<div class="section" id="scrapy-engine">
<h5>Scrapy Engine<a class="headerlink" href="#scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the Data
Flow section below for more details.</p>
</div>
<div class="section" id="scheduler">
<h5>Scheduler<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>The Scheduler receives requests from the engine and enqueues them for feeding
them later (also to the engine) when the engine requests them.</p>
</div>
<div class="section" id="downloader">
<h5>Downloader<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.</p>
</div>
<div class="section" id="spiders">
<h5>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h5>
<p>Spiders are custom classes written by Scrapy users to parse responses and
extract items (aka scraped items) from them or additional URLs (requests) to
follow. For more information see <a class="reference internal" href="index.html#topics-spiders"><span>Spiders</span></a>.</p>
</div>
<div class="section" id="item-pipeline">
<h5>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h5>
<p>The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>.</p>
</div>
<div class="section" id="downloader-middlewares">
<h5>Downloader middlewares<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine.</p>
<p>Use a Downloader middleware if you need to do one of the following:</p>
<ul class="simple">
<li>process a request just before it is sent to the Downloader
(i.e. right before Scrapy sends the request to the website);</li>
<li>change received response before passing it to a spider;</li>
<li>send a new Request instead of passing received response to a spider;</li>
<li>pass response to a spider without fetching a web page;</li>
<li>silently drop some requests.</li>
</ul>
<p>For more information see <a class="reference internal" href="index.html#topics-downloader-middleware"><span>Downloader Middleware</span></a>.</p>
</div>
<div class="section" id="spider-middlewares">
<h5>Spider middlewares<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests).</p>
<p>Use a Spider middleware if you need to</p>
<ul class="simple">
<li>post-process output of spider callbacks - change/add/remove requests or items;</li>
<li>post-process start_requests;</li>
<li>handle spider exceptions;</li>
<li>call errback instead of callback for some of the requests based on response
content.</li>
</ul>
<p>For more information see <a class="reference internal" href="index.html#topics-spider-middleware"><span>Spider Middleware</span></a>.</p>
</div>
</div>
<div class="section" id="data-flow">
<h4>Data flow<a class="headerlink" href="#data-flow" title="Permalink to this headline">¶</a></h4>
<p>The data flow in Scrapy is controlled by the execution engine, and goes like
this:</p>
<ol class="arabic simple">
<li>The Engine gets the first URLs to crawl from the Spider and schedules them
in the Scheduler, as Requests.</li>
<li>The Engine asks the Scheduler for the next URLs to crawl.</li>
<li>The Scheduler returns the next URLs to crawl to the Engine and the Engine
sends them to the Downloader, passing through the Downloader Middleware
(request direction).</li>
<li>Once the page finishes downloading the Downloader generates a Response (with
that page) and sends it to the Engine, passing through the Downloader
Middleware (response direction).</li>
<li>The Engine receives the Response from the Downloader and sends it to the
Spider for processing, passing through the Spider Middleware (input direction).</li>
<li>The Spider processes the Response and returns scraped items and new Requests
(to follow) to the Engine.</li>
<li>The Engine passes scraped items and new Requests returned by a spider
through Spider Middleware (output direction), and then sends processed
items to Item Pipelines and processed Requests to the Scheduler.</li>
<li>The process repeats (from step 1) until there are no more requests from the
Scheduler.</li>
</ol>
</div>
<div class="section" id="event-driven-networking">
<h4>Event-driven networking<a class="headerlink" href="#event-driven-networking" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is written with <a class="reference external" href="https://twistedmatrix.com/trac/">Twisted</a>, a popular event-driven networking framework
for Python. Thus, it&#8217;s implemented using a non-blocking (aka asynchronous) code
for concurrency.</p>
<p>For more information about asynchronous programming and Twisted see these
links:</p>
<ul class="simple">
<li><a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Introduction to Deferreds in Twisted</a></li>
<li><a class="reference external" href="http://jessenoller.com/2009/02/11/twisted-hello-asynchronous-programming/">Twisted - hello, asynchronous programming</a></li>
<li><a class="reference external" href="http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/">Twisted Introduction - Krondo</a></li>
</ul>
</div>
</div>
<span id="document-topics/downloader-middleware"></span><div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h3>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this headline">¶</a></h3>
<p>The downloader middleware is a framework of hooks into Scrapy&#8217;s
request/response processing.  It&#8217;s a light, low-level system for globally
altering Scrapy&#8217;s requests and responses.</p>
<div class="section" id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h4>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here&#8217;s an example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project&#8217;s <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <cite>None</cite>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>DOWNLOADER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomDownloaderMiddleware&#39;: 543,
    &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: None,
}
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-downloader-middleware">
<h4>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>Each middleware component is a Python class that defines one or
more of the following methods:</p>
<span class="target" id="module-scrapy.downloadermiddlewares"></span><dl class="class">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.</code><code class="descname">DownloaderMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Any of the downloader middleware methods may also return a deferred.</p>
</div>
<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">
<code class="descname">process_request</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> should either: return <code class="docutils literal"><span class="pre">None</span></code>, return a
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object, or raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a>.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, Scrapy won&#8217;t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> or <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods,
or the appropriate download function; it&#8217;ll return that response. The <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<code class="docutils literal"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">
<code class="descname">process_response</code><span class="sig-paren">(</span><em>request</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> should either: return a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>
object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object or
raise a <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> of the next middleware in the chain.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a>.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the errback
function of the request (<code class="docutils literal"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that originated the response</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">
<code class="descname">process_exception</code><span class="sig-paren">(</span><em>request</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception)</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> should return: either <code class="docutils literal"><span class="pre">None</span></code>,
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, or a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a>
method chain of installed middleware is started, and Scrapy won&#8217;t bother calling
any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of middleware.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of the middleware the same as returning a
response would.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that generated the exception</li>
<li><strong>exception</strong> (an <code class="docutils literal"><span class="pre">Exception</span></code> object) &#8211; the raised exception</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h4>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="#topics-downloader-middleware"><span>downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<div class="section" id="module-scrapy.downloadermiddlewares.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h5>CookiesMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.cookies" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.cookies.</code><code class="descname">CookiesMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a></li>
</ul>
<div class="section" id="multiple-cookie-sessions-per-spider">
<span id="std:reqmeta-cookiejar"></span><h6>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>for i, url in enumerate(urls):
    yield scrapy.Request(url, meta={&#39;cookiejar&#39;: i},
        callback=self.parse_page)
</pre></div>
</div>
<p>Keep in mind that the <a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> meta key is not &#8220;sticky&#8221;. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>def parse_page(self, response):
    # do some processing
    return scrapy.Request(&quot;http://www.example.com/otherpage&quot;,
        meta={&#39;cookiejar&#39;: response.meta[&#39;cookiejar&#39;]},
        callback=self.parse_other_page)
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h6>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this headline">¶</a></h6>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h6>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this headline">¶</a></h6>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <code class="docutils literal"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (ie. <code class="docutils literal"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here&#8217;s an example of a log with <a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>2011-04-06 14:35:10-0300 [scrapy] INFO: Spider opened
2011-04-06 14:35:10-0300 [scrapy] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
        Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [scrapy] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
        Set-Cookie: ip_isocode=US
        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [scrapy] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.defaultheaders">
<span id="defaultheadersmiddleware"></span><h5>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.defaultheaders" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.defaultheaders.</code><code class="descname">DefaultHeadersMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h5>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.downloadtimeout" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.downloadtimeout.</code><code class="descname">DownloadTimeoutMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class="xref py py-attr docutils literal"><span class="pre">download_timeout</span></code>
spider attribute.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also set download timeout per-request using
<a class="reference internal" href="index.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a> Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.</p>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpauth">
<span id="httpauthmiddleware"></span><h5>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpauth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpauth.</code><code class="descname">HttpAuthMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <code class="docutils literal"><span class="pre">http_user</span></code>
and <code class="docutils literal"><span class="pre">http_pass</span></code> attributes of those spiders.</p>
<p>Example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.spiders import CrawlSpider

class SomeIntranetSiteSpider(CrawlSpider):

    http_user = &#39;someuser&#39;
    http_pass = &#39;somepass&#39;
    name = &#39;intranet.example.com&#39;

    # .. rest of the spider code omitted ...
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpcache">
<span id="httpcachemiddleware"></span><h5>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcache" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpcache.</code><code class="descname">HttpCacheMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with two HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-storage-fs"><span>Filesystem storage backend (default)</span></a></li>
<li><a class="reference internal" href="#httpcache-storage-dbm"><span>DBM storage backend</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also implement your own storage backend.</p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-policy-rfc2616"><span>RFC2616 policy</span></a></li>
<li><a class="reference internal" href="#httpcache-policy-dummy"><span>Dummy policy (default)</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
<p id="std:reqmeta-dont_cache">You can also avoid caching a response on every policy using <a class="reference internal" href="#std:reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_cache</span></code></a> meta key equals <cite>True</cite>.</p>
</dd></dl>

<div class="section" id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h6>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this headline">¶</a></h6>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
&#8220;replay&#8221; a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.extensions.httpcache.DummyPolicy</span></code></li>
</ul>
</div>
<div class="section" id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h6>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this headline">¶</a></h6>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<ul>
<li><p class="first">Do not attempt to store responses/requests with <cite>no-store</cite> cache-control directive set</p>
</li>
<li><p class="first">Do not serve responses from cache if <cite>no-cache</cite> cache-control directive is set even for fresh responses</p>
</li>
<li><p class="first">Compute freshness lifetime from <cite>max-age</cite> cache-control directive</p>
</li>
<li><p class="first">Compute freshness lifetime from <cite>Expires</cite> response header</p>
</li>
<li><p class="first">Compute freshness lifetime from <cite>Last-Modified</cite> response header (heuristic used by Firefox)</p>
</li>
<li><p class="first">Compute current age from <cite>Age</cite> response header</p>
</li>
<li><p class="first">Compute current age from <cite>Date</cite> header</p>
</li>
<li><p class="first">Revalidate stale responses based on <cite>Last-Modified</cite> response header</p>
</li>
<li><p class="first">Revalidate stale responses based on <cite>ETag</cite> response header</p>
</li>
<li><p class="first">Set <cite>Date</cite> header for any received response missing it</p>
</li>
<li><p class="first">Support <cite>max-stale</cite> cache-control directive in requests</p>
<p>This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.</p>
<p>Example:</p>
<p>Add <cite>Cache-Control: max-stale=600</cite> to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.</p>
<p>See also: RFC2616, 14.9.3</p>
</li>
</ul>
<p>what is missing:</p>
<ul class="simple">
<li><cite>Pragma: no-cache</cite> support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1">https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</a></li>
<li><cite>Vary</cite> header support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>Invalidation after updates or deletes <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>... probably others ..</li>
</ul>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.extensions.httpcache.RFC2616Policy</span></code></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h6>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this headline">¶</a></h6>
<p>File system storage backend is available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.extensions.httpcache.FilesystemCacheStorage</span></code></li>
</ul>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">request_body</span></code> - the plain request body</li>
<li><code class="docutils literal"><span class="pre">request_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal"><span class="pre">response_body</span></code> - the plain response body</li>
<li><code class="docutils literal"><span class="pre">response_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal"><span class="pre">meta</span></code> - some metadata of this cache resource in Python <code class="docutils literal"><span class="pre">repr()</span></code> format
(grep-friendly format)</li>
<li><code class="docutils literal"><span class="pre">pickled_meta</span></code> - the same metadata in <code class="docutils literal"><span class="pre">meta</span></code> but pickled for more
efficient deserialization</li>
</ul>
</div></blockquote>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
</pre></div>
</div>
</div>
<div class="section" id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h6>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="https://docs.python.org/2/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="#std:setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.extensions.httpcache.DbmCacheStorage</span></code></li>
</ul>
</div>
<div class="section" id="leveldb-storage-backend">
<span id="httpcache-storage-leveldb"></span><h6>LevelDB storage backend<a class="headerlink" href="#leveldb-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.23.</span></p>
</div>
<p>A <a class="reference external" href="https://github.com/google/leveldb">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p>
<p>This backend is not recommended for development because only one process can
access LevelDB databases at the same time, so you can&#8217;t run a crawl and open
the scrapy shell in parallel for the same spider.</p>
<p>In order to use this storage backend:</p>
<ul class="simple">
<li>set <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal"><span class="pre">scrapy.extensions.httpcache.LeveldbCacheStorage</span></code></li>
<li>install <a class="reference external" href="https://pypi.python.org/pypi/leveldb">LevelDB python bindings</a> like <code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">leveldb</span></code></li>
</ul>
</div>
<div class="section" id="httpcache-middleware-settings">
<h6>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this headline">¶</a></h6>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></code></a> can be configured through the following
settings:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h7>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="#std:setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></code></a> was used to enable cache.</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h7>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h7>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="index.html#topics-project-structure"><span>Default structure of Scrapy projects</span></a>.</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h7>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>Don&#8217;t cache response with these HTTP codes.</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h7>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h7>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">['file']</span></code></p>
<p>Don&#8217;t cache responses with these URI schemes.</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h7>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h7>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">'anydbm'</span></code></p>
<p>The database module to use in the <a class="reference internal" href="#httpcache-storage-dbm"><span>DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h7>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
</div>
<div class="section" id="httpcache-gzip">
<span id="std:setting-HTTPCACHE_GZIP"></span><h7>HTTPCACHE_GZIP<a class="headerlink" href="#httpcache-gzip" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.</p>
</div>
<div class="section" id="httpcache-always-store">
<span id="std:setting-HTTPCACHE_ALWAYS_STORE"></span><h7>HTTPCACHE_ALWAYS_STORE<a class="headerlink" href="#httpcache-always-store" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with <cite>Cache-Control: max-stale</cite>, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects <cite>Cache-Control: no-store</cite> directives in responses.
If you don&#8217;t want that, filter <cite>no-store</cite> out of the Cache-Control headers in
responses you feedto the cache middleware.</p>
</div>
<div class="section" id="httpcache-ignore-response-cache-controls">
<span id="std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"></span><h7>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS<a class="headerlink" href="#httpcache-ignore-response-cache-controls" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set &#8220;no-store&#8221;, &#8220;no-cache&#8221;, &#8220;must-revalidate&#8221;, etc., but get
upset at the traffic a spider can generate if it respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpcompression">
<span id="httpcompressionmiddleware"></span><h5>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcompression" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpcompression.</code><code class="descname">HttpCompressionMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h6>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h7>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.chunked">
<span id="chunkedtransfermiddleware"></span><h5>ChunkedTransferMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.chunked" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.chunked.</code><code class="descname">ChunkedTransferMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware adds support for <a class="reference external" href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a></p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpproxy">
<span id="httpproxymiddleware"></span><h5>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpproxy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.8.</span></p>
</div>
<span class="target" id="std:reqmeta-proxy"></span><dl class="class">
<dt id="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpproxy.</code><code class="descname">HttpProxyMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal"><span class="pre">proxy</span></code> meta value for <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="https://docs.python.org/2/library/urllib.html">urllib</a> and <a class="reference external" href="https://docs.python.org/2/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">http_proxy</span></code></li>
<li><code class="docutils literal"><span class="pre">https_proxy</span></code></li>
<li><code class="docutils literal"><span class="pre">no_proxy</span></code></li>
</ul>
<p>You can also set the meta key <code class="docutils literal"><span class="pre">proxy</span></code> per-request, to a value like
<code class="docutils literal"><span class="pre">http://some_proxy_server:port</span></code>.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.redirect">
<span id="redirectmiddleware"></span><h5>RedirectMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.redirect" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.redirect.</code><code class="descname">RedirectMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal"><span class="pre">redirect_urls</span></code> <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> key.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></code></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal"><span class="pre">dont_redirect</span></code>
key set to True, the request will be ignored by this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(CrawlSpider):
    handle_httpstatus_list = [301, 302]
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
<code class="docutils literal"><span class="pre">handle_httpstatus_all</span></code> to <code class="docutils literal"><span class="pre">True</span></code> if you want to allow any response code
for a request.</p>
<div class="section" id="redirectmiddleware-settings">
<h6>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h7>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h7>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be follow for a single request.</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h5>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.redirect.</code><code class="descname">MetaRefreshMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-METAREFRESH_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-METAREFRESH_MAXDELAY"><code class="xref std std-setting docutils literal"><span class="pre">METAREFRESH_MAXDELAY</span></code></a></li>
</ul>
<p>This middleware obey <a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="reference internal" href="#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></code></a>
and <a class="reference internal" href="#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></code></a> request meta keys as described for <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a></p>
<div class="section" id="metarefreshmiddleware-settings">
<h6>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h7>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</div>
<div class="section" id="metarefresh-maxdelay">
<span id="std:setting-METAREFRESH_MAXDELAY"></span><h7>METAREFRESH_MAXDELAY<a class="headerlink" href="#metarefresh-maxdelay" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.retry">
<span id="retrymiddleware"></span><h5>RetryMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.retry" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.retry.RetryMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.retry.</code><code class="descname">RetryMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a></li>
</ul>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal"><span class="pre">dont_retry</span></code> key
set to True, the request will be ignored by this middleware.</p>
<div class="section" id="retrymiddleware-settings">
<h6>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h7>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h7>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h7>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">408]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a> because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h5>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.robotstxt" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.robotstxt.</code><code class="descname">RobotsTxtMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
</dd></dl>

<p id="std:reqmeta-dont_obey_robotstxt">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has
<code class="docutils literal"><span class="pre">dont_obey_robotstxt</span></code> key set to True
the request will be ignored by this middleware even if
<a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.stats">
<span id="downloaderstats"></span><h5>DownloaderStats<a class="headerlink" href="#module-scrapy.downloadermiddlewares.stats" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.stats.DownloaderStats">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.stats.</code><code class="descname">DownloaderStats</code><a class="headerlink" href="#scrapy.downloadermiddlewares.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.useragent">
<span id="useragentmiddleware"></span><h5>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.useragent" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.useragent.</code><code class="descname">UserAgentMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <cite>user_agent</cite>
attribute must be set.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h5>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.ajaxcrawl" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.ajaxcrawl.</code><code class="descname">AjaxCrawlMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds &#8216;AJAX crawlable&#8217; page variants based
on meta-fragment html tag. See
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
for more info.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scrapy finds &#8216;AJAX crawlable&#8217; pages for URLs like
<code class="docutils literal"><span class="pre">'http://example.com/!#foo=bar'</span></code> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn&#8217;t contain <code class="docutils literal"><span class="pre">'!#'</span></code>.
This is often a case for &#8216;index&#8217; or &#8216;main&#8217; website pages.</p>
</div>
</dd></dl>

<div class="section" id="ajaxcrawlmiddleware-settings">
<h6>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="ajaxcrawl-enabled">
<span id="std:setting-AJAXCRAWL_ENABLED"></span><h7>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.21.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="reference internal" href="index.html#topics-broad-crawls"><span>broad crawls</span></a>.</p>
</div>
</div>
<div class="section" id="httpproxymiddleware-settings">
<h6>HttpProxyMiddleware settings<a class="headerlink" href="#httpproxymiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="httpproxy-auth-encoding">
<span id="std:setting-HTTPPROXY_AUTH_ENCODING"></span><h7>HTTPPROXY_AUTH_ENCODING<a class="headerlink" href="#httpproxy-auth-encoding" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">&quot;latin-1&quot;</span></code></p>
<p>The default encoding for proxy authentication on <code class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/spider-middleware"></span><div class="section" id="spider-middleware">
<span id="topics-spider-middleware"></span><h3>Spider Middleware<a class="headerlink" href="#spider-middleware" title="Permalink to this headline">¶</a></h3>
<p>The spider middleware is a framework of hooks into Scrapy&#8217;s spider processing
mechanism where you can plug custom functionality to process the responses that
are sent to <a class="reference internal" href="index.html#topics-spiders"><span>Spiders</span></a> for processing and to process the requests
and items that are generated from spiders.</p>
<div class="section" id="activating-a-spider-middleware">
<span id="topics-spider-middleware-setting"></span><h4>Activating a spider middleware<a class="headerlink" href="#activating-a-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a spider middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.</p>
<p>Here&#8217;s an example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
}
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a>, and enabled by default) you must define it
in your project <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting and assign <cite>None</cite> as its
value.  For example, if you want to disable the off-site middleware:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>SPIDER_MIDDLEWARES = {
    &#39;myproject.middlewares.CustomSpiderMiddleware&#39;: 543,
    &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;: None,
}
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-spider-middleware">
<h4>Writing your own spider middleware<a class="headerlink" href="#writing-your-own-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>Each middleware component is a Python class that defines one or more of the
following methods:</p>
<span class="target" id="module-scrapy.spidermiddlewares"></span><dl class="class">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.</code><code class="descname">SpiderMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input">
<code class="descname">process_spider_input</code><span class="sig-paren">(</span><em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each response that goes through the spider
middleware and into the spider, for processing.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a> should return <code class="docutils literal"><span class="pre">None</span></code> or raise an
exception.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.</p>
<p>If it raises an exception, Scrapy won&#8217;t bother calling any other spider
middleware <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a> and will call the request
errback.  The output of the errback is chained back in the other
direction for <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> to process it, or
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> if it raised an exception.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">
<code class="descname">process_spider_output</code><span class="sig-paren">(</span><em>response</em>, <em>result</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called with the results returned from the Spider, after
it has processed the response.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> must return an iterable of
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>
objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response which generated this output from the
spider</li>
<li><strong>result</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, dict
or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects) &#8211; the result returned by the spider</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider whose result is being processed</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception">
<code class="descname">process_spider_exception</code><span class="sig-paren">(</span><em>response</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when when a spider or <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a>
method (from other spider middleware) raises an exception.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> should return either <code class="docutils literal"><span class="pre">None</span></code> or an
iterable of <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>, dict or
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it&#8217;s logged and discarded).</p>
<p>If it returns an iterable the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> pipeline
kicks in, and no other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> will be called.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response being processed when the exception was
raised</li>
<li><strong>exception</strong> (<a class="reference external" href="https://docs.python.org/2/library/exceptions.html#exceptions.Exception">Exception</a> object) &#8211; the exception raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests">
<code class="descname">process_start_requests</code><span class="sig-paren">(</span><em>start_requests</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This method is called with the start requests of the spider, and works
similarly to the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> method, except that it
doesn&#8217;t have a response associated and must return only requests (not
items).</p>
<p>It receives an iterable (in the <code class="docutils literal"><span class="pre">start_requests</span></code> parameter) and must
return another iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all <code class="docutils literal"><span class="pre">start_requests</span></code> iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start_requests</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>) &#8211; the start requests</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider to whom the start requests belong</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-spider-middleware-reference">
<span id="topics-spider-middleware-ref"></span><h4>Built-in spider middleware reference<a class="headerlink" href="#built-in-spider-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the <a class="reference internal" href="#topics-spider-middleware"><span>spider middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<div class="section" id="module-scrapy.spidermiddlewares.depth">
<span id="depthmiddleware"></span><h5>DepthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.depth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.depth.DepthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.depth.</code><code class="descname">DepthMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>DepthMiddleware is a scrape middleware used for tracking the depth of each
Request inside the site being scraped. It can be used to limit the maximum
depth to scrape or things like that.</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="scrapy.spidermiddlewares.depth.DepthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">DepthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT"><code class="xref std std-setting docutils literal"><span class="pre">DEPTH_LIMIT</span></code></a> - The maximum depth that will be allowed to
crawl for any site. If zero, no limit will be imposed.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS"><code class="xref std std-setting docutils literal"><span class="pre">DEPTH_STATS</span></code></a> - Whether to collect depth stats.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY"><code class="xref std std-setting docutils literal"><span class="pre">DEPTH_PRIORITY</span></code></a> - Whether to prioritize the requests based on
their depth.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-scrapy.spidermiddlewares.httperror">
<span id="httperrormiddleware"></span><h5>HttpErrorMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.httperror" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.httperror.</code><code class="descname">HttpErrorMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don&#8217;t
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.</p>
</dd></dl>

<p>According to the <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP standard</a>, successful responses are those whose
status codes are in the 200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
<code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> spider attribute or
<a class="reference internal" href="#std:setting-HTTPERROR_ALLOWED_CODES"><code class="xref std std-setting docutils literal"><span class="pre">HTTPERROR_ALLOWED_CODES</span></code></a> setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(CrawlSpider):
    handle_httpstatus_list = [404]
</pre></div>
</div>
<span class="target" id="std:reqmeta-handle_httpstatus_list"></span><p id="std:reqmeta-handle_httpstatus_all">The <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key <code class="docutils literal"><span class="pre">handle_httpstatus_all</span></code>
to <code class="docutils literal"><span class="pre">True</span></code> if you want to allow any response code for a request.</p>
<p>Keep in mind, however, that it&#8217;s usually a bad idea to handle non-200
responses, unless you really know what you&#8217;re doing.</p>
<p>For more information see: <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code Definitions</a>.</p>
<div class="section" id="httperrormiddleware-settings">
<h6>HttpErrorMiddleware settings<a class="headerlink" href="#httperrormiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="httperror-allowed-codes">
<span id="std:setting-HTTPERROR_ALLOWED_CODES"></span><h7>HTTPERROR_ALLOWED_CODES<a class="headerlink" href="#httperror-allowed-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>Pass all responses with non-200 status codes contained in this list.</p>
</div>
<div class="section" id="httperror-allow-all">
<span id="std:setting-HTTPERROR_ALLOW_ALL"></span><h7>HTTPERROR_ALLOW_ALL<a class="headerlink" href="#httperror-allow-all" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Pass all responses, regardless of its status code.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.spidermiddlewares.offsite">
<span id="offsitemiddleware"></span><h5>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.offsite" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.offsite.OffsiteMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.offsite.</code><code class="descname">OffsiteMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren&#8217;t in the
spider&#8217;s <a class="reference internal" href="index.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></code></a> attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule <code class="docutils literal"><span class="pre">www.example.org</span></code> will also allow <code class="docutils literal"><span class="pre">bob.www.example.org</span></code>
but not <code class="docutils literal"><span class="pre">www2.example.com</span></code> nor <code class="docutils literal"><span class="pre">example.com</span></code>.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>DEBUG: Filtered offsite request to &#39;www.othersite.com&#39;: &lt;GET http://www.othersite.com/some/page.html&gt;
</pre></div>
</div>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <code class="docutils literal"><span class="pre">www.othersite.com</span></code> is filtered, no log message will be
printed. But if a request for <code class="docutils literal"><span class="pre">someothersite.com</span></code> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn&#8217;t define an
<a class="reference internal" href="index.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></code></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <code class="xref py py-attr docutils literal"><span class="pre">dont_filter</span></code> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.spidermiddlewares.referer">
<span id="referermiddleware"></span><h5>RefererMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.referer" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.RefererMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">RefererMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.RefererMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates Request <code class="docutils literal"><span class="pre">Referer</span></code> header, based on the URL of the Response which
generated it.</p>
</dd></dl>

<div class="section" id="referermiddleware-settings">
<h6>RefererMiddleware settings<a class="headerlink" href="#referermiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="referer-enabled">
<span id="std:setting-REFERER_ENABLED"></span><h7>REFERER_ENABLED<a class="headerlink" href="#referer-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable referer middleware.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.spidermiddlewares.urllength">
<span id="urllengthmiddleware"></span><h5>UrlLengthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.urllength" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.urllength.</code><code class="descname">UrlLengthMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">UrlLengthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT"><code class="xref std std-setting docutils literal"><span class="pre">URLLENGTH_LIMIT</span></code></a> - The maximum URL length to allow for crawled URLs.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/extensions"></span><div class="section" id="extensions">
<span id="topics-extensions"></span><h3>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h3>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes that are instantiated at Scrapy startup,
when extensions are initialized.</p>
<div class="section" id="extension-settings">
<h4>Extension settings<a class="headerlink" href="#extension-settings" title="Permalink to this headline">¶</a></h4>
<p>Extensions use the <a class="reference internal" href="index.html#topics-settings"><span>Scrapy settings</span></a> to manage their
settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, a
hypothetic extension to handle <a class="reference external" href="https://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a> would use settings like
<cite>GOOGLESITEMAP_ENABLED</cite>, <cite>GOOGLESITEMAP_DEPTH</cite>, and so on.</p>
</div>
<div class="section" id="loading-activating-extensions">
<h4>Loading &amp; activating extensions<a class="headerlink" href="#loading-activating-extensions" title="Permalink to this headline">¶</a></h4>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class. Therefore, all the extension initialization
code must be performed in the class constructor (<code class="docutils literal"><span class="pre">__init__</span></code> method).</p>
<p>To make an extension available, add it to the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting in
your Scrapy settings. In <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a>, each extension is represented
by a string: the full Python path to the extension&#8217;s class name. For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>EXTENSIONS = {
    &#39;scrapy.extensions.corestats.CoreStats&#39;: 500,
    &#39;scrapy.extensions.telnet.TelnetConsole&#39;: 500,
}
</pre></div>
</div>
<p>As you can see, the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension <em>loading</em> order. The <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting defined in Scrapy (and not meant to be
overridden) and then sorted by order to get the final sorted list of enabled
extensions.</p>
<p>As extensions typically do not depend on each other, their loading order is
irrelevant in most cases. This is why the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting
defines all extensions with the same order (<code class="docutils literal"><span class="pre">0</span></code>). However, this feature can
be exploited if you need to add an extension which depends on other extensions
already loaded.</p>
</div>
<div class="section" id="available-enabled-and-disabled-extensions">
<h4>Available, enabled and disabled extensions<a class="headerlink" href="#available-enabled-and-disabled-extensions" title="Permalink to this headline">¶</a></h4>
<p>Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the <a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_ENABLED</span></code></a> setting is set.</p>
</div>
<div class="section" id="disabling-an-extension">
<h4>Disabling an extension<a class="headerlink" href="#disabling-an-extension" title="Permalink to this headline">¶</a></h4>
<p>In order to disable an extension that comes enabled by default (ie. those
included in the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting) you must set its order to
<code class="docutils literal"><span class="pre">None</span></code>. For example:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>EXTENSIONS = {
    &#39;scrapy.extensions.corestats.CoreStats&#39;: None,
}
</pre></div>
</div>
</div>
<div class="section" id="writing-your-own-extension">
<h4>Writing your own extension<a class="headerlink" href="#writing-your-own-extension" title="Permalink to this headline">¶</a></h4>
<p>Each extension is a Python class. The main entry point for a Scrapy extension
(this also includes middlewares and pipelines) is the <code class="docutils literal"><span class="pre">from_crawler</span></code>
class method which receives a <code class="docutils literal"><span class="pre">Crawler</span></code> instance. Through the Crawler object
you can access settings, signals, stats, and also control the crawling behaviour.</p>
<p>Typically, extensions connect to <a class="reference internal" href="index.html#topics-signals"><span>signals</span></a> and perform
tasks triggered by them.</p>
<p>Finally, if the <code class="docutils literal"><span class="pre">from_crawler</span></code> method raises the
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal"><span class="pre">NotConfigured</span></code></a> exception, the extension will be
disabled. Otherwise, the extension will be enabled.</p>
<div class="section" id="sample-extension">
<h5>Sample extension<a class="headerlink" href="#sample-extension" title="Permalink to this headline">¶</a></h5>
<p>Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:</p>
<ul class="simple">
<li>a spider is opened</li>
<li>a spider is closed</li>
<li>a specific number of items are scraped</li>
</ul>
<p>The extension will be enabled through the <code class="docutils literal"><span class="pre">MYEXT_ENABLED</span></code> setting and the
number of items will be specified through the <code class="docutils literal"><span class="pre">MYEXT_ITEMCOUNT</span></code> setting.</p>
<p>Here is the code of such extension:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import logging
from scrapy import signals
from scrapy.exceptions import NotConfigured

logger = logging.getLogger(__name__)

class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise
        # NotConfigured otherwise
        if not crawler.settings.getbool(&#39;MYEXT_ENABLED&#39;):
            raise NotConfigured

        # get the number of items from settings
        item_count = crawler.settings.getint(&#39;MYEXT_ITEMCOUNT&#39;, 1000)

        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        logger.info(&quot;opened spider %s&quot;, spider.name)

    def spider_closed(self, spider):
        logger.info(&quot;closed spider %s&quot;, spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped % self.item_count == 0:
            logger.info(&quot;scraped %d items&quot;, self.items_scraped)
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-extensions-reference">
<span id="topics-extensions-ref"></span><h4>Built-in extensions reference<a class="headerlink" href="#built-in-extensions-reference" title="Permalink to this headline">¶</a></h4>
<div class="section" id="general-purpose-extensions">
<h5>General purpose extensions<a class="headerlink" href="#general-purpose-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="module-scrapy.extensions.logstats">
<span id="log-stats-extension"></span><h6>Log Stats extension<a class="headerlink" href="#module-scrapy.extensions.logstats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.logstats.LogStats">
<em class="property">class </em><code class="descclassname">scrapy.extensions.logstats.</code><code class="descname">LogStats</code><a class="headerlink" href="#scrapy.extensions.logstats.LogStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Log basic stats like crawled pages and scraped items.</p>
</div>
<div class="section" id="module-scrapy.extensions.corestats">
<span id="core-stats-extension"></span><h6>Core Stats extension<a class="headerlink" href="#module-scrapy.extensions.corestats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.corestats.CoreStats">
<em class="property">class </em><code class="descclassname">scrapy.extensions.corestats.</code><code class="descname">CoreStats</code><a class="headerlink" href="#scrapy.extensions.corestats.CoreStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Enable the collection of core statistics, provided the stats collection is
enabled (see <a class="reference internal" href="index.html#topics-stats"><span>Stats Collection</span></a>).</p>
</div>
<div class="section" id="module-scrapy.extensions.telnet">
<span id="telnet-console-extension"></span><span id="topics-extensions-ref-telnetconsole"></span><h6>Telnet console extension<a class="headerlink" href="#module-scrapy.extensions.telnet" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.telnet.scrapy.extensions.telnet.TelnetConsole">
<em class="property">class </em><code class="descclassname">scrapy.extensions.telnet.</code><code class="descname">TelnetConsole</code><a class="headerlink" href="#scrapy.extensions.telnet.scrapy.extensions.telnet.TelnetConsole" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.</p>
<p>The telnet console must be enabled by the <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a>
setting, and the server will listen in the port specified in
<a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></code></a>.</p>
</div>
<div class="section" id="module-scrapy.extensions.memusage">
<span id="memory-usage-extension"></span><span id="topics-extensions-ref-memusage"></span><h6>Memory usage extension<a class="headerlink" href="#module-scrapy.extensions.memusage" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.memusage.scrapy.extensions.memusage.MemoryUsage">
<em class="property">class </em><code class="descclassname">scrapy.extensions.memusage.</code><code class="descname">MemoryUsage</code><a class="headerlink" href="#scrapy.extensions.memusage.scrapy.extensions.memusage.MemoryUsage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This extension does not work in Windows.</p>
</div>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<ol class="arabic simple">
<li>sends a notification e-mail when it exceeds a certain value</li>
<li>closes the spider when it exceeds a certain value</li>
</ol>
<p>The notification e-mails can be triggered when a certain warning value is
reached (<a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>) and when the maximum value is reached
(<a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>) which will also cause the spider to be closed
and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_ENABLED</span></code></a> setting and
can be configured with the following settings:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_REPORT"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_REPORT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a></li>
</ul>
</div>
<div class="section" id="module-scrapy.extensions.memdebug">
<span id="memory-debugger-extension"></span><h6>Memory debugger extension<a class="headerlink" href="#module-scrapy.extensions.memdebug" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.memdebug.scrapy.extensions.memdebug.MemoryDebugger">
<em class="property">class </em><code class="descclassname">scrapy.extensions.memdebug.</code><code class="descname">MemoryDebugger</code><a class="headerlink" href="#scrapy.extensions.memdebug.scrapy.extensions.memdebug.MemoryDebugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>An extension for debugging memory usage. It collects information about:</p>
<ul class="simple">
<li>objects uncollected by the Python garbage collector</li>
<li>objects left alive that shouldn&#8217;t. For more info, see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span>Debugging memory leaks with trackref</span></a></li>
</ul>
<p>To enable this extension, turn on the <a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">MEMDEBUG_ENABLED</span></code></a> setting. The
info will be stored in the stats.</p>
</div>
<div class="section" id="module-scrapy.extensions.closespider">
<span id="close-spider-extension"></span><h6>Close spider extension<a class="headerlink" href="#module-scrapy.extensions.closespider" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.closespider.scrapy.extensions.closespider.CloseSpider">
<em class="property">class </em><code class="descclassname">scrapy.extensions.closespider.</code><code class="descname">CloseSpider</code><a class="headerlink" href="#scrapy.extensions.closespider.scrapy.extensions.closespider.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the following
settings:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a></li>
</ul>
<div class="section" id="closespider-timeout">
<span id="std:setting-CLOSESPIDER_TIMEOUT"></span><h7>CLOSESPIDER_TIMEOUT<a class="headerlink" href="#closespider-timeout" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason <code class="docutils literal"><span class="pre">closespider_timeout</span></code>. If zero (or non set), spiders won&#8217;t be closed by
timeout.</p>
</div>
<div class="section" id="closespider-itemcount">
<span id="std:setting-CLOSESPIDER_ITEMCOUNT"></span><h7>CLOSESPIDER_ITEMCOUNT<a class="headerlink" href="#closespider-itemcount" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of items. If the spider scrapes more than
that amount if items and those items are passed by the item pipeline, the
spider will be closed with the reason <code class="docutils literal"><span class="pre">closespider_itemcount</span></code>. If zero (or
non set), spiders won&#8217;t be closed by number of passed items.</p>
</div>
<div class="section" id="closespider-pagecount">
<span id="std:setting-CLOSESPIDER_PAGECOUNT"></span><h7>CLOSESPIDER_PAGECOUNT<a class="headerlink" href="#closespider-pagecount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
<code class="docutils literal"><span class="pre">closespider_pagecount</span></code>. If zero (or non set), spiders won&#8217;t be closed by
number of crawled responses.</p>
</div>
<div class="section" id="closespider-errorcount">
<span id="std:setting-CLOSESPIDER_ERRORCOUNT"></span><h7>CLOSESPIDER_ERRORCOUNT<a class="headerlink" href="#closespider-errorcount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason <code class="docutils literal"><span class="pre">closespider_errorcount</span></code>. If zero (or non
set), spiders won&#8217;t be closed by number of errors.</p>
</div>
</div>
<div class="section" id="module-scrapy.extensions.statsmailer">
<span id="statsmailer-extension"></span><h6>StatsMailer extension<a class="headerlink" href="#module-scrapy.extensions.statsmailer" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.statsmailer.scrapy.extensions.statsmailer.StatsMailer">
<em class="property">class </em><code class="descclassname">scrapy.extensions.statsmailer.</code><code class="descname">StatsMailer</code><a class="headerlink" href="#scrapy.extensions.statsmailer.scrapy.extensions.statsmailer.StatsMailer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This simple extension can be used to send a notification e-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the <a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS"><code class="xref std std-setting docutils literal"><span class="pre">STATSMAILER_RCPTS</span></code></a>
setting.</p>
<span class="target" id="module-scrapy.extensions.debug"></span></div>
</div>
<div class="section" id="debugging-extensions">
<h5>Debugging extensions<a class="headerlink" href="#debugging-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="stack-trace-dump-extension">
<h6>Stack trace dump extension<a class="headerlink" href="#stack-trace-dump-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.debug.scrapy.extensions.debug.StackTraceDump">
<em class="property">class </em><code class="descclassname">scrapy.extensions.debug.</code><code class="descname">StackTraceDump</code><a class="headerlink" href="#scrapy.extensions.debug.scrapy.extensions.debug.StackTraceDump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Dumps information about the running process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. The information dumped is the following:</p>
<ol class="arabic simple">
<li>engine status (using <code class="docutils literal"><span class="pre">scrapy.utils.engine.get_engine_status()</span></code>)</li>
<li>live references (see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span>Debugging memory leaks with trackref</span></a>)</li>
<li>stack trace of all threads</li>
</ol>
<p>After the stack trace and engine status is dumped, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows),
because the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a> signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> signal:</p>
<ol class="arabic">
<li><p class="first">By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>
</li>
<li><p class="first">By running this command (assuming <code class="docutils literal"><span class="pre">&lt;pid&gt;</span></code> is the process id of the Scrapy
process):</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>kill -QUIT &lt;pid&gt;
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="debugger-extension">
<h6>Debugger extension<a class="headerlink" href="#debugger-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.debug.scrapy.extensions.debug.Debugger">
<em class="property">class </em><code class="descclassname">scrapy.extensions.debug.</code><code class="descname">Debugger</code><a class="headerlink" href="#scrapy.extensions.debug.scrapy.extensions.debug.Debugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Invokes a <a class="reference external" href="https://docs.python.org/2/library/pdb.html">Python debugger</a> inside a running Scrapy process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. After the debugger is exited, the Scrapy process continues
running normally.</p>
<p>For more info see <cite>Debugging in Python</cite>.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows).</p>
</div>
</div>
</div>
</div>
<span id="document-topics/api"></span><div class="section" id="core-api">
<span id="topics-api"></span><h3>Core API<a class="headerlink" href="#core-api" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This section documents the Scrapy core API, and it&#8217;s intended for developers of
extensions and middlewares.</p>
<div class="section" id="crawler-api">
<span id="topics-api-crawler"></span><h4>Crawler API<a class="headerlink" href="#crawler-api" title="Permalink to this headline">¶</a></h4>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
object, passed to extensions through the <code class="docutils literal"><span class="pre">from_crawler</span></code> class method. This
object provides access to all Scrapy core components, and it&#8217;s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<span class="target" id="module-scrapy.crawler"></span><p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it&#8217;s configured through the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span>configure the downloader middlewares</span></a>.</p>
<dl class="class">
<dt id="scrapy.crawler.Crawler">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">Crawler</code><span class="sig-paren">(</span><em>spidercls</em>, <em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>The Crawler object must be instantiated with a
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">scrapy.spiders.Spider</span></code></a> subclass and a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></code></a> object.</p>
<dl class="attribute">
<dt id="scrapy.crawler.Crawler.settings">
<code class="descname">settings</code><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="reference internal" href="index.html#topics-settings"><span>Settings</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.signals">
<code class="descname">signals</code><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="Permalink to this definition">¶</a></dt>
<dd><p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="reference internal" href="index.html#topics-signals"><span>Signals</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><code class="xref py py-class docutils literal"><span class="pre">SignalManager</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.stats">
<code class="descname">stats</code><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="reference internal" href="index.html#topics-stats"><span>Stats Collection</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.extensions">
<code class="descname">extensions</code><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="Permalink to this definition">¶</a></dt>
<dd><p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won&#8217;t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="reference internal" href="index.html#topics-extensions"><span>Extensions</span></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.engine">
<code class="descname">engine</code><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="Permalink to this definition">¶</a></dt>
<dd><p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to inspect  or
modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.spider">
<code class="descname">spider</code><a class="headerlink" href="#scrapy.crawler.Crawler.spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Spider currently being crawled. This is an instance of the spider class
provided while constructing the crawler, and it is created after the
arguments given in the <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts the crawler by instantiating its spider class with the given
<cite>args</cite> and <cite>kwargs</cite> arguments, while setting the execution engine in
motion.</p>
<p>Returns a deferred that is fired when the crawl is finished.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerRunner">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">CrawlerRunner</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a convenient helper class that keeps track of, manages and runs
crawlers inside an already setup Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>.</p>
<p>The CrawlerRunner object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn&#8217;t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="index.html#run-from-script"><span>Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler&#8217;s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn&#8217;t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> subclass or string) &#8211; already created crawler, or a spider class
or spider&#8217;s name inside the project to create it</li>
<li><strong>args</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; arguments to initialize the spider</li>
<li><strong>kwargs</strong> (<em>dict</em>) &#8211; keyword arguments to initialize the spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.CrawlerRunner.crawlers">
<code class="descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.create_crawler">
<code class="descname">create_crawler</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li>If <cite>crawler_or_spidercls</cite> is a Crawler, it is returned as-is.</li>
<li>If <cite>crawler_or_spidercls</cite> is a Spider subclass, a new Crawler
is constructed for it.</li>
<li>If <cite>crawler_or_spidercls</cite> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.join">
<code class="descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawlers" title="scrapy.crawler.CrawlerRunner.crawlers"><code class="xref py py-attr docutils literal"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.stop">
<code class="descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerProcess">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">CrawlerProcess</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a></p>
<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>
<p>This class extends <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a> by adding support
for starting a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> and handling shutdown signals, like the
keyboard interrupt command Ctrl-C. It also configures top-level logging.</p>
<p>This utility should be a better fit than
<a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a> if you aren&#8217;t running another
Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> within your application.</p>
<p>The CrawlerProcess object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn&#8217;t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="index.html#run-from-script"><span>Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler&#8217;s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn&#8217;t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> subclass or string) &#8211; already created crawler, or a spider class
or spider&#8217;s name inside the project to create it</li>
<li><strong>args</strong> (<a class="reference internal" href="index.html#scrapy.loader.SpiderLoader.list" title="scrapy.loader.SpiderLoader.list"><em>list</em></a>) &#8211; arguments to initialize the spider</li>
<li><strong>kwargs</strong> (<em>dict</em>) &#8211; keyword arguments to initialize the spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.CrawlerProcess.crawlers">
<code class="descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.create_crawler">
<code class="descname">create_crawler</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li>If <cite>crawler_or_spidercls</cite> is a Crawler, it is returned as-is.</li>
<li>If <cite>crawler_or_spidercls</cite> is a Spider subclass, a new Crawler
is constructed for it.</li>
<li>If <cite>crawler_or_spidercls</cite> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.join">
<code class="descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawlers" title="scrapy.crawler.CrawlerProcess.crawlers"><code class="xref py py-attr docutils literal"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.start">
<code class="descname">start</code><span class="sig-paren">(</span><em>stop_after_crawl=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.start" title="Permalink to this definition">¶</a></dt>
<dd><p>This method starts a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>, adjusts its pool size to
<a class="reference internal" href="index.html#std:setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache based
on <a class="reference internal" href="index.html#std:setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">DNSCACHE_ENABLED</span></code></a> and <a class="reference internal" href="index.html#std:setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal"><span class="pre">DNSCACHE_SIZE</span></code></a>.</p>
<p>If <cite>stop_after_crawl</cite> is True, the reactor will be stopped after all
crawlers have finished, using <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.join" title="scrapy.crawler.CrawlerProcess.join"><code class="xref py py-meth docutils literal"><span class="pre">join()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stop_after_crawl</strong> (<em>boolean</em>) &#8211; stop or not the reactor when all
crawlers have finished</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.stop">
<code class="descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h4>Settings API<a class="headerlink" href="#module-scrapy.settings" title="Permalink to this headline">¶</a></h4>
<dl class="attribute">
<dt id="scrapy.settings.SETTINGS_PRIORITIES">
<code class="descclassname">scrapy.settings.</code><code class="descname">SETTINGS_PRIORITIES</code><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;default&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;command&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s1">&#39;project&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s1">&#39;spider&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s1">&#39;cmdline&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="index.html#topics-settings"><span>Settings</span></a>.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.settings.get_settings_priority">
<code class="descclassname">scrapy.settings.</code><code class="descname">get_settings_priority</code><span class="sig-paren">(</span><em>priority</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Small helper function that looks up a given string priority in the
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its
numerical value, or directly returns a given numerical priority.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.Settings">
<em class="property">class </em><code class="descclassname">scrapy.settings.</code><code class="descname">Settings</code><span class="sig-paren">(</span><em>values=None</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">scrapy.settings.BaseSettings</span></code></a></p>
<p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>It is a direct subclass and supports all methods of
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a>. Additionally, after instantiation
of this class, the new object will have the global default settings
described on <a class="reference internal" href="index.html#topics-settings-ref"><span>Built-in settings reference</span></a> already populated.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.BaseSettings">
<em class="property">class </em><code class="descclassname">scrapy.settings.</code><code class="descname">BaseSettings</code><span class="sig-paren">(</span><em>values=None</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instances of this class behave like dictionaries, but store priorities
along with their <code class="docutils literal"><span class="pre">(key,</span> <span class="pre">value)</span></code> pairs, and can be frozen (i.e. marked
immutable).</p>
<p>Key-value entries can be passed on initialization with the <code class="docutils literal"><span class="pre">values</span></code>
argument, and they would take the <code class="docutils literal"><span class="pre">priority</span></code> level (unless <code class="docutils literal"><span class="pre">values</span></code> is
already an instance of <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a>, in which
case the existing priority levels will be kept).  If the <code class="docutils literal"><span class="pre">priority</span></code>
argument is a string, the priority name will be looked up in
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer
should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> method, and can be accessed with
the square bracket notation of dictionaries, or with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal"><span class="pre">get()</span></code></a> method of the instance and its
value conversion variants. When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a deep copy of current settings.</p>
<p>This method returns a new instance of the <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class,
populated with the same values and their priorities.</p>
<p>Modifications to the new object won&#8217;t be reflected on the original
settings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy_to_dict">
<code class="descname">copy_to_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy_to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a copy of current settings and convert to a dict.</p>
<p>This method returns a new dict populated with the same values
and their priorities as the current settings.</p>
<p>Modifications to the returned dict won&#8217;t be reflected on the original
settings.</p>
<p>This method can be useful for example for printing settings
in Scrapy shell.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable further changes to the current settings.</p>
<p>After calling this method, the present state of the settings will become
immutable. Trying to change values through the <a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> method and
its variants won&#8217;t be possible and will be alerted.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.frozencopy">
<code class="descname">frozencopy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.frozencopy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an immutable copy of the current settings.</p>
<p>Alias for a <a class="reference internal" href="#scrapy.settings.BaseSettings.freeze" title="scrapy.settings.BaseSettings.freeze"><code class="xref py py-meth docutils literal"><span class="pre">freeze()</span></code></a> call in the object returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.copy" title="scrapy.settings.BaseSettings.copy"><code class="xref py py-meth docutils literal"><span class="pre">copy()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.get">
<code class="descname">get</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value without affecting its original type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getbool">
<code class="descname">getbool</code><span class="sig-paren">(</span><em>name</em>, <em>default=False</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getbool" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a boolean.</p>
<p><code class="docutils literal"><span class="pre">1</span></code>, <code class="docutils literal"><span class="pre">'1'</span></code>, and <code class="docutils literal"><span class="pre">True</span></code> return <code class="docutils literal"><span class="pre">True</span></code>, while <code class="docutils literal"><span class="pre">0</span></code>, <code class="docutils literal"><span class="pre">'0'</span></code>,
<code class="docutils literal"><span class="pre">False</span></code> and <code class="docutils literal"><span class="pre">None</span></code> return <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal"><span class="pre">'0'</span></code> will return <code class="docutils literal"><span class="pre">False</span></code> when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getdict">
<code class="descname">getdict</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a dictionary. If the setting original type is a
dictionary, a copy of it will be returned. If it is a string it will be
evaluated as a JSON dictionary. In the case that it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a> instance itself, it will be
converted to a dictionary, containing all its current settings values
as they would be returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal"><span class="pre">get()</span></code></a>,
and losing all information about priority and mutability.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getfloat">
<code class="descname">getfloat</code><span class="sig-paren">(</span><em>name</em>, <em>default=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getfloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a float.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getint">
<code class="descname">getint</code><span class="sig-paren">(</span><em>name</em>, <em>default=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getint" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as an int.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getlist">
<code class="descname">getlist</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a list. If the setting original type is a list, a
copy of it will be returned. If it&#8217;s a string it will be split by &#8221;,&#8221;.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal"><span class="pre">'one,two'</span></code> will return a list [&#8216;one&#8217;, &#8216;two&#8217;] when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>default</strong> (<em>any</em>) &#8211; the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getpriority">
<code class="descname">getpriority</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the current numerical priority value of a setting, or <code class="docutils literal"><span class="pre">None</span></code> if
the given <code class="docutils literal"><span class="pre">name</span></code> does not exist.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>string</em>) &#8211; the setting name</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getwithbase">
<code class="descname">getwithbase</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getwithbase" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>
counterpart.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>string</em>) &#8211; name of the dictionary-like setting</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.maxpriority">
<code class="descname">maxpriority</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.maxpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the numerical value of the highest priority present throughout
all settings, or the numerical value for <code class="docutils literal"><span class="pre">default</span></code> from
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> if there are no settings
stored.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.set">
<code class="descname">set</code><span class="sig-paren">(</span><em>name</em>, <em>value</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <code class="xref py py-meth docutils literal"><span class="pre">configure()</span></code> method),
otherwise they won&#8217;t have any effect.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) &#8211; the setting name</li>
<li><strong>value</strong> (<em>any</em>) &#8211; the value to associate with the setting</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the setting. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.setmodule">
<code class="descname">setmodule</code><span class="sig-paren">(</span><em>module</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.setmodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> for every globally declared
uppercase variable of <code class="docutils literal"><span class="pre">module</span></code> with the provided <code class="docutils literal"><span class="pre">priority</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<em>module object or string</em>) &#8211; the module or the path of the module</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>values</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> for every item of <code class="docutils literal"><span class="pre">values</span></code>
with the provided <code class="docutils literal"><span class="pre">priority</span></code>.</p>
<p>If <code class="docutils literal"><span class="pre">values</span></code> is a string, it is assumed to be JSON-encoded and parsed
into a dict with <code class="docutils literal"><span class="pre">json.loads()</span></code> first. If it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a> instance, the per-key priorities
will be used and the <code class="docutils literal"><span class="pre">priority</span></code> parameter ignored. This allows
inserting/updating settings with different priorities with a single
command.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>values</strong> (dict or string or <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a>) &#8211; the settings names and values</li>
<li><strong>priority</strong> (<em>string or int</em>) &#8211; the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.loader">
<span id="spiderloader-api"></span><span id="topics-api-spiderloader"></span><h4>SpiderLoader API<a class="headerlink" href="#module-scrapy.loader" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.loader.SpiderLoader">
<em class="property">class </em><code class="descclassname">scrapy.loader.</code><code class="descname">SpiderLoader</code><a class="headerlink" href="#scrapy.loader.SpiderLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is in charge of retrieving and handling the spider classes
defined across the project.</p>
<p>Custom spider loaders can be employed by specifying their path in the
<a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement
the <code class="xref py py-class docutils literal"><span class="pre">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an
errorless execution.</p>
<dl class="method">
<dt id="scrapy.loader.SpiderLoader.from_settings">
<code class="descname">from_settings</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This class method is used by Scrapy to create an instance of the class.
It&#8217;s called with the current project settings, and it loads the spiders
found in the modules of the <a class="reference internal" href="index.html#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MODULES</span></code></a> setting.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> instance) &#8211; project settings</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>spider_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Spider class with the given name. It&#8217;ll look into the previously
loaded spiders for a spider class with name <cite>spider_name</cite> and will raise
a KeyError if not found.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider_name</strong> (<em>str</em>) &#8211; spider class name</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.list">
<code class="descname">list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the names of the available spiders in the project.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.SpiderLoader.find_by_request">
<code class="descname">find_by_request</code><span class="sig-paren">(</span><em>request</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.SpiderLoader.find_by_request" title="Permalink to this definition">¶</a></dt>
<dd><p>List the spiders&#8217; names that can handle the given request. Will try to
match the request&#8217;s url against the domains of the spiders.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> instance) &#8211; queried request</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h4>Signals API<a class="headerlink" href="#module-scrapy.signalmanager" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.signalmanager.SignalManager">
<em class="property">class </em><code class="descclassname">scrapy.signalmanager.</code><code class="descname">SignalManager</code><span class="sig-paren">(</span><em>sender=_Anonymous</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.signalmanager.SignalManager.connect">
<code class="descname">connect</code><span class="sig-paren">(</span><em>receiver</em>, <em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="reference internal" href="index.html#topics-signals"><span>Signals</span></a>
section.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>receiver</strong> (<em>callable</em>) &#8211; the function to be connected</li>
<li><strong>signal</strong> (<em>object</em>) &#8211; the signal to connect to</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect">
<code class="descname">disconnect</code><span class="sig-paren">(</span><em>receiver</em>, <em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect a receiver function from a signal. This has the
opposite effect of the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method, and the arguments
are the same.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect_all">
<code class="descname">disconnect_all</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect all receivers from the given signal.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>signal</strong> (<em>object</em>) &#8211; the signal to disconnect from</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log">
<code class="descname">send_catch_log</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<code class="descname">send_catch_log_deferred</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="Permalink to this definition">¶</a></dt>
<dd><p>Like <a class="reference internal" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><code class="xref py py-meth docutils literal"><span class="pre">send_catch_log()</span></code></a> but supports returning <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> from
signal handlers.</p>
<p>Returns a Deferred that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stats-collector-api">
<span id="topics-api-stats"></span><h4>Stats Collector API<a class="headerlink" href="#stats-collector-api" title="Permalink to this headline">¶</a></h4>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="index.html#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.statscollectors</span></code></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code></a>
class (which they all inherit from).</p>
<span class="target" id="module-scrapy.statscollectors"></span><dl class="class">
<dt id="scrapy.statscollectors.StatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">StatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_value">
<code class="descname">get_value</code><span class="sig-paren">(</span><em>key</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the value for the given stats key or default if it doesn&#8217;t exist.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_stats">
<code class="descname">get_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all stats from the currently running spider as a dict.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_value">
<code class="descname">set_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given stats key.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_stats">
<code class="descname">set_stats</code><span class="sig-paren">(</span><em>stats</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Override the current stats with the dict passed in <code class="docutils literal"><span class="pre">stats</span></code> argument.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.inc_value">
<code class="descname">inc_value</code><span class="sig-paren">(</span><em>key</em>, <em>count=1</em>, <em>start=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.inc_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it&#8217;s not set).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.max_value">
<code class="descname">max_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.max_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.min_value">
<code class="descname">min_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.min_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.clear_stats">
<code class="descname">clear_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.clear_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all stats.</p>
</dd></dl>

<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.open_spider">
<code class="descname">open_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Open the given spider for stats collection.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.close_spider">
<code class="descname">close_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-topics/signals"></span><div class="section" id="signals">
<span id="topics-signals"></span><h3>Signals<a class="headerlink" href="#signals" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an <a class="reference internal" href="index.html#topics-extensions"><span>extension</span></a>, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch them
don&#8217;t need to accept all of them - the signal dispatching mechanism will only
deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the
<a class="reference internal" href="index.html#topics-api-signals"><span>Signals API</span></a>.</p>
<div class="section" id="deferred-signal-handlers">
<h4>Deferred signal handlers<a class="headerlink" href="#deferred-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted deferreds</a> from their handlers, see
the <a class="reference internal" href="#topics-signals-ref"><span>Built-in signals reference</span></a> below to know which ones.</p>
</div>
<div class="section" id="module-scrapy.signals">
<span id="built-in-signals-reference"></span><span id="topics-signals-ref"></span><h4>Built-in signals reference<a class="headerlink" href="#module-scrapy.signals" title="Permalink to this headline">¶</a></h4>
<p>Here&#8217;s the list of Scrapy built-in signals and their meaning.</p>
<div class="section" id="engine-started">
<h5>engine_started<a class="headerlink" href="#engine-started" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_started"></span><dl class="function">
<dt id="scrapy.signals.engine_started">
<code class="descclassname">scrapy.signals.</code><code class="descname">engine_started</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_started" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine has started crawling.</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This signal may be fired <em>after</em> the <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a> signal,
depending on how the spider was started. So <strong>don&#8217;t</strong> rely on this signal
getting fired before <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>.</p>
</div>
</div>
<div class="section" id="engine-stopped">
<h5>engine_stopped<a class="headerlink" href="#engine-stopped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_stopped"></span><dl class="function">
<dt id="scrapy.signals.engine_stopped">
<code class="descclassname">scrapy.signals.</code><code class="descname">engine_stopped</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_stopped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

</div>
<div class="section" id="item-scraped">
<h5>item_scraped<a class="headerlink" href="#item-scraped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_scraped"></span><dl class="function">
<dt id="scrapy.signals.item_scraped">
<code class="descclassname">scrapy.signals.</code><code class="descname">item_scraped</code><span class="sig-paren">(</span><em>item</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_scraped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when an item has been scraped, after it has passed all the
<a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a> stages (without being dropped).</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object) &#8211; the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response from where the item was scraped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-dropped">
<h5>item_dropped<a class="headerlink" href="#item-dropped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_dropped"></span><dl class="function">
<dt id="scrapy.signals.item_dropped">
<code class="descclassname">scrapy.signals.</code><code class="descname">item_dropped</code><span class="sig-paren">(</span><em>item</em>, <em>response</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after an item has been dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a>
when some stage raised a <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object) &#8211; the item dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><span>Item Pipeline</span></a></li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response from where the item was dropped</li>
<li><strong>exception</strong> (<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> exception) &#8211; the exception (which must be a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> subclass) which caused the item
to be dropped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-closed">
<h5>spider_closed<a class="headerlink" href="#spider-closed" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_closed"></span><dl class="function">
<dt id="scrapy.signals.spider_closed">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_closed</code><span class="sig-paren">(</span><em>spider</em>, <em>reason</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been closed. This can be used to release per-spider
resources reserved on <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which has been closed</li>
<li><strong>reason</strong> (<em>str</em>) &#8211; a string which describes the reason why the spider was closed. If
it was closed because the spider has completed scraping, the reason
is <code class="docutils literal"><span class="pre">'finished'</span></code>. Otherwise, if the spider was manually closed by
calling the <code class="docutils literal"><span class="pre">close_spider</span></code> engine method, then the reason is the one
passed in the <code class="docutils literal"><span class="pre">reason</span></code> argument of that method (which defaults to
<code class="docutils literal"><span class="pre">'cancelled'</span></code>). If the engine was shutdown (for example, by hitting
Ctrl-C to stop it) the reason will be <code class="docutils literal"><span class="pre">'shutdown'</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-opened">
<h5>spider_opened<a class="headerlink" href="#spider-opened" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_opened"></span><dl class="function">
<dt id="scrapy.signals.spider_opened">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_opened</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been opened for crawling. This is typically used to
reserve per-spider resources, but can be used for any task that needs to be
performed when a spider is opened.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which has been opened</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-idle">
<h5>spider_idle<a class="headerlink" href="#spider-idle" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_idle"></span><dl class="function">
<dt id="scrapy.signals.spider_idle">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_idle</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider has gone idle, which means the spider has no further:</p>
<blockquote>
<div><ul class="simple">
<li>requests waiting to be downloaded</li>
<li>requests scheduled</li>
<li>items being processed in the item pipeline</li>
</ul>
</div></blockquote>
<p>If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the <a class="reference internal" href="#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> signal is sent.</p>
<p>You can, for example, schedule some requests in your <a class="reference internal" href="#std:signal-spider_idle"><code class="xref std std-signal docutils literal"><span class="pre">spider_idle</span></code></a>
handler to prevent the spider from being closed.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which has gone idle</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-error">
<h5>spider_error<a class="headerlink" href="#spider-error" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_error"></span><dl class="function">
<dt id="scrapy.signals.spider_error">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_error</code><span class="sig-paren">(</span><em>failure</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider callback generates an error (ie. raises an exception).</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>failure</strong> (<a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object) &#8211; the exception raised as a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response being processed when the exception was raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-scheduled">
<h5>request_scheduled<a class="headerlink" href="#request-scheduled" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_scheduled"></span><dl class="function">
<dt id="scrapy.signals.request_scheduled">
<code class="descclassname">scrapy.signals.</code><code class="descname">request_scheduled</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine schedules a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, to be
downloaded later.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that reached the scheduler</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-dropped">
<h5>request_dropped<a class="headerlink" href="#request-dropped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_dropped"></span><dl class="function">
<dt id="scrapy.signals.request_dropped">
<code class="descclassname">scrapy.signals.</code><code class="descname">request_dropped</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, scheduled by the engine to be
downloaded later, is rejected by the scheduler.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that reached the scheduler</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-received">
<h5>response_received<a class="headerlink" href="#response-received" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_received"></span><dl class="function">
<dt id="scrapy.signals.response_received">
<code class="descclassname">scrapy.signals.</code><code class="descname">response_received</code><span class="sig-paren">(</span><em>response</em>, <em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine receives a new <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> from the
downloader.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response received</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-downloaded">
<h5>response_downloaded<a class="headerlink" href="#response-downloaded" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_downloaded"></span><dl class="function">
<dt id="scrapy.signals.response_downloaded">
<code class="descclassname">scrapy.signals.</code><code class="descname">response_downloaded</code><span class="sig-paren">(</span><em>response</em>, <em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the downloader right after a <code class="docutils literal"><span class="pre">HTTPResponse</span></code> is downloaded.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object) &#8211; the response downloaded</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object) &#8211; the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> object) &#8211; the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/exporters"></span><div class="section" id="module-scrapy.exporters">
<span id="item-exporters"></span><span id="topics-exporters"></span><h3>Item Exporters<a class="headerlink" href="#module-scrapy.exporters" title="Permalink to this headline">¶</a></h3>
<p>Once you have scraped your items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.</p>
<div class="section" id="using-item-exporters">
<h4>Using Item Exporters<a class="headerlink" href="#using-item-exporters" title="Permalink to this headline">¶</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the <a class="reference internal" href="index.html#topics-feed-exports"><span>Feed exports</span></a>. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.</p>
<p>In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in <a class="reference internal" href="#topics-exporters-reference"><span>Built-in Item Exporters reference</span></a>. After you have
instantiated your exporter, you have to:</p>
<p>1. call the method <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting"><code class="xref py py-meth docutils literal"><span class="pre">start_exporting()</span></code></a> in order to
signal the beginning of the exporting process</p>
<p>2. call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item"><code class="xref py py-meth docutils literal"><span class="pre">export_item()</span></code></a> method for each item you want
to export</p>
<p>3. and finally call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting"><code class="xref py py-meth docutils literal"><span class="pre">finish_exporting()</span></code></a> to signal
the end of the exporting process</p>
<p>Here you can see an <a class="reference internal" href="index.html#document-topics/item-pipeline"><em>Item Pipeline</em></a> which uses an Item
Exporter to export scraped items to different files, one per spider:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>from scrapy import signals
from scrapy.exporters import XmlItemExporter

class XmlExportPipeline(object):

    def __init__(self):
        self.files = {}

     @classmethod
     def from_crawler(cls, crawler):
         pipeline = cls()
         crawler.signals.connect(pipeline.spider_opened, signals.spider_opened)
         crawler.signals.connect(pipeline.spider_closed, signals.spider_closed)
         return pipeline

    def spider_opened(self, spider):
        file = open(&#39;%s_products.xml&#39; % spider.name, &#39;w+b&#39;)
        self.files[spider] = file
        self.exporter = XmlItemExporter(file)
        self.exporter.start_exporting()

    def spider_closed(self, spider):
        self.exporter.finish_exporting()
        file = self.files.pop(spider)
        file.close()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item
</pre></div>
</div>
</div>
<div class="section" id="serialization-of-item-fields">
<span id="topics-exporters-field-serialization"></span><h4>Serialization of item fields<a class="headerlink" href="#serialization-of-item-fields" title="Permalink to this headline">¶</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it is
passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which are
described next.</p>
<div class="section" id="declaring-a-serializer-in-the-field">
<span id="topics-exporters-serializers"></span><h5>1. Declaring a serializer in the field<a class="headerlink" href="#declaring-a-serializer-in-the-field" title="Permalink to this headline">¶</a></h5>
<p>If you use <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> you can declare a serializer in the
<a class="reference internal" href="index.html#topics-items-fields"><span>field metadata</span></a>. The serializer must be
a callable which receives a value and returns its serialized form.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">def</span> <span class="nf">serialize_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="n">serialize_price</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="overriding-the-serialize-field-method">
<h5>2. Overriding the serialize_field() method<a class="headerlink" href="#overriding-the-serialize-field-method" title="Permalink to this headline">¶</a></h5>
<p>You can also override the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code></a> method to
customize how your field value will be exported.</p>
<p>Make sure you call the base class <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code></a> method
after your custom code.</p>
<p>Example:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exporter</span> <span class="kn">import</span> <span class="n">XmlItemExporter</span>

<span class="k">class</span> <span class="nc">ProductXmlExporter</span><span class="p">(</span><span class="n">XmlItemExporter</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">serialize_field</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span> <span class="o">==</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Product</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">serialize_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-item-exporters-reference">
<span id="topics-exporters-reference"></span><h4>Built-in Item Exporters reference<a class="headerlink" href="#built-in-item-exporters-reference" title="Permalink to this headline">¶</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you&#8217;re exporting these two items:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Color TV&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s1">&#39;1200&#39;</span><span class="p">)</span>
<span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;DVD player&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s1">&#39;200&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="baseitemexporter">
<h5>BaseItemExporter<a class="headerlink" href="#baseitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.BaseItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">BaseItemExporter</code><span class="sig-paren">(</span><em>fields_to_export=None</em>, <em>export_empty_fields=False</em>, <em>encoding='utf-8'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.</p>
<p>These features can be configured through the constructor arguments which
populate their respective instance attributes: <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></code></a>,
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="scrapy.exporters.BaseItemExporter.export_empty_fields"><code class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal"><span class="pre">encoding</span></code></a>.</p>
<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.export_item">
<code class="descname">export_item</code><span class="sig-paren">(</span><em>item</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the given item. This method must be implemented in subclasses.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.serialize_field">
<code class="descname">serialize_field</code><span class="sig-paren">(</span><em>field</em>, <em>name</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.</p>
<p>By default, this method looks for a serializer <a class="reference internal" href="#topics-exporters-serializers"><span>declared in the item
field</span></a> and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged except for <code class="docutils literal"><span class="pre">unicode</span></code> values which are encoded to
<code class="docutils literal"><span class="pre">str</span></code> using the encoding declared in the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal"><span class="pre">encoding</span></code></a> attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>field</strong> (<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> object or an empty dict) &#8211; the field being serialized. If a raw dict is being
exported (not <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>) <em>field</em> value is an empty dict.</li>
<li><strong>name</strong> (<em>str</em>) &#8211; the name of the field being serialized</li>
<li><strong>value</strong> &#8211; the value being serialized</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.start_exporting">
<code class="descname">start_exporting</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a>). You must call this method before exporting any
items.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.finish_exporting">
<code class="descname">finish_exporting</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a>). You must always call this method after you
have no more items to export.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.fields_to_export">
<code class="descname">fields_to_export</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="Permalink to this definition">¶</a></dt>
<dd><p>A list with the name of the fields that will be exported, or None if you
want to export all fields. Defaults to None.</p>
<p>Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></code></a>) respect the order of the
fields defined in this attribute.</p>
<p>Some exporters may require fields_to_export list in order to export the
data properly when spiders return dicts (not <code class="xref py py-class docutils literal"><span class="pre">Item</span></code> instances).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.export_empty_fields">
<code class="descname">export_empty_fields</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether to include empty/unpopulated item fields in the exported data.
Defaults to <code class="docutils literal"><span class="pre">False</span></code>. Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></code></a>)
ignore this attribute and always export all empty fields.</p>
<p>This option is ignored for dict items.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.encoding">
<code class="descname">encoding</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>The encoding that will be used to encode unicode values. This only
affects unicode values (which are always serialized to str using this
encoding). Other value types are passed unchanged to the specific
serialization library.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xmlitemexporter">
<h5>XmlItemExporter<a class="headerlink" href="#xmlitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.XmlItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">XmlItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>item_element='item'</em>, <em>root_element='items'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.XmlItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in XML format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>root_element</strong> (<em>str</em>) &#8211; The name of root element in the exported XML.</li>
<li><strong>item_element</strong> (<em>str</em>) &#8211; The name of each item element in the exported XML.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;Color TV&lt;/name&gt;
    &lt;price&gt;1200&lt;/price&gt;
 &lt;/item&gt;
  &lt;item&gt;
    &lt;name&gt;DVD player&lt;/name&gt;
    &lt;price&gt;200&lt;/price&gt;
 &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
<p>Unless overridden in the <code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code> method, multi-valued fields are
exported by serializing each value inside a <code class="docutils literal"><span class="pre">&lt;value&gt;</span></code> element. This is for
convenience, as multi-valued fields are very common.</p>
<p>For example, the item:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>Item(name=[&#39;John&#39;, &#39;Doe&#39;], age=&#39;23&#39;)
</pre></div>
</div>
<p>Would be serialized as:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;
      &lt;value&gt;John&lt;/value&gt;
      &lt;value&gt;Doe&lt;/value&gt;
    &lt;/name&gt;
    &lt;age&gt;23&lt;/age&gt;
  &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="csvitemexporter">
<h5>CsvItemExporter<a class="headerlink" href="#csvitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.CsvItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">CsvItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>include_headers_line=True</em>, <em>join_multivalued='</em>, <em>'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.CsvItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in CSV format to the given file-like object. If the
<code class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></code> attribute is set, it will be used to define the
CSV columns and their order. The <code class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></code> attribute has
no effect on this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>include_headers_line</strong> (<em>str</em>) &#8211; If enabled, makes the exporter output a header
line with the field names taken from
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal"><span class="pre">BaseItemExporter.fields_to_export</span></code></a> or the first exported item fields.</li>
<li><strong>join_multivalued</strong> &#8211; The char (or chars) that will be used for joining
multi-valued fields, if found.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to the
<a class="reference external" href="https://docs.python.org/2/library/csv.html#csv.writer">csv.writer</a> constructor, so you can use any <cite>csv.writer</cite> constructor
argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>product,price
Color TV,1200
DVD player,200
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pickleitemexporter">
<h5>PickleItemExporter<a class="headerlink" href="#pickleitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.PickleItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">PickleItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>protocol=0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PickleItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pickle format to the given file-like object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> &#8211; the file-like object to use for exporting the data.</li>
<li><strong>protocol</strong> (<em>int</em>) &#8211; The pickle protocol to use.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For more information, refer to the <a class="reference external" href="https://docs.python.org/2/library/pickle.html">pickle module documentation</a>.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>Pickle isn&#8217;t a human readable format, so no output examples are provided.</p>
</dd></dl>

</div>
<div class="section" id="pprintitemexporter">
<h5>PprintItemExporter<a class="headerlink" href="#pprintitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.PprintItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">PprintItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PprintItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pretty print format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>{&#39;name&#39;: &#39;Color TV&#39;, &#39;price&#39;: &#39;1200&#39;}
{&#39;name&#39;: &#39;DVD player&#39;, &#39;price&#39;: &#39;200&#39;}
</pre></div>
</div>
<p>Longer lines (when present) are pretty-formatted.</p>
</dd></dl>

</div>
<div class="section" id="jsonitemexporter">
<h5>JsonItemExporter<a class="headerlink" href="#jsonitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.JsonItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">JsonItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing all
objects as a list of objects. The additional constructor arguments are
passed to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover
arguments to the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any
<a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>[{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
</pre></div>
</div>
<div class="admonition warning" id="json-with-large-data">
<p class="first admonition-title">Warning</p>
<p class="last">JSON is very simple and flexible serialization format, but it
doesn&#8217;t scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream-friendly format, consider using <a class="reference internal" href="#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></code></a>
instead, or splitting the output in multiple chunks.</p>
</div>
</dd></dl>

</div>
<div class="section" id="jsonlinesitemexporter">
<h5>JsonLinesItemExporter<a class="headerlink" href="#jsonlinesitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.JsonLinesItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">JsonLinesItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonLinesItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing one
JSON-encoded item per line. The additional constructor arguments are passed
to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to
the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a>
constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> &#8211; the file-like object to use for exporting the data.</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
</pre></div>
</div>
<p>Unlike the one produced by <a class="reference internal" href="#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a>, the format produced by
this exporter is well suited for serializing large amounts of data.</p>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/architecture"><em>Architecture overview</em></a></dt>
<dd>Understand the Scrapy architecture.</dd>
<dt><a class="reference internal" href="index.html#document-topics/downloader-middleware"><em>Downloader Middleware</em></a></dt>
<dd>Customize how pages get requested and downloaded.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spider-middleware"><em>Spider Middleware</em></a></dt>
<dd>Customize the input and output of your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/extensions"><em>Extensions</em></a></dt>
<dd>Extend Scrapy with your custom functionality</dd>
<dt><a class="reference internal" href="index.html#document-topics/api"><em>Core API</em></a></dt>
<dd>Use it on extensions and middlewares to extend Scrapy functionality</dd>
<dt><a class="reference internal" href="index.html#document-topics/signals"><em>Signals</em></a></dt>
<dd>See all available signals and how to work with them.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exporters"><em>Item Exporters</em></a></dt>
<dd>Quickly export your scraped items to a file (XML, CSV, etc).</dd>
</dl>
</div>
<div class="section" id="all-the-rest">
<h2>All the rest<a class="headerlink" href="#all-the-rest" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound" id="id7">
<span id="document-news"></span><div class="section" id="release-notes">
<span id="news"></span><h3>Release notes<a class="headerlink" href="#release-notes" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>1.1.0 (2016-05-11)<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<ul class="simple">
<li>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5). See
<a class="reference internal" href="#news-betapy3"><span>Beta Python 3 Support</span></a> for more details and some limitations.</li>
<li>Hot new features:<ul>
<li>Item loaders now support nested loaders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1467">issue 1467</a>).</li>
<li><code class="docutils literal"><span class="pre">FormRequest.from_response</span></code> improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1382">issue 1382</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1137">issue 1137</a>).</li>
<li>Added setting <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> and improved
AutoThrottle docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1324">issue 1324</a>).</li>
<li>Added <code class="docutils literal"><span class="pre">response.text</span></code> to get body as unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1730">issue 1730</a>).</li>
<li>Anonymous S3 connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1358">issue 1358</a>).</li>
<li>Deferreds in downloader middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1473">issue 1473</a>). This enables better
robots.txt handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1471">issue 1471</a>).</li>
<li>HTTP caching now follows RFC2616 more closely, added settings
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_ALWAYS_STORE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_ALWAYS_STORE</span></code></a> and
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1151">issue 1151</a>).</li>
<li>Selectors were extracted to the <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a> library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1409">issue 1409</a>). This means
you can use Scrapy Selectors without Scrapy and also upgrade the
selectors engine without needing to upgrade Scrapy.</li>
<li>HTTPS downloader now does TLS protocol negotiation by default,
instead of forcing TLS 1.0. You can also set the SSL/TLS method
using the new <a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>.</li>
</ul>
</li>
<li>These bug fixes may require your attention:<ul>
<li>Don&#8217;t retry bad requests (HTTP 400) by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>).
If you need the old behavior, add <code class="docutils literal"><span class="pre">400</span></code> to <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a>.</li>
<li>Fix shell files argument handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).
If you try <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> it will try to load the URL <a class="reference external" href="http://index.html">http://index.html</a>,
use <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">./index.html</span></code> to load a local file.</li>
<li>Robots.txt compliance is now enabled by default for newly-created projects
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1724">issue 1724</a>). Scrapy will also wait for robots.txt to be downloaded
before proceeding with the crawl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1735">issue 1735</a>). If you want to disable
this behavior, update <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> in <code class="docutils literal"><span class="pre">settings.py</span></code> file
after creating a new project.</li>
<li>Exporters now work on unicode, instead of bytes by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1080">issue 1080</a>).
If you use <code class="docutils literal"><span class="pre">PythonItemExporter</span></code>, you may want to update your code to
disable binary mode which is now deprecated.</li>
<li>Accept XML node names containing dots as valid (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1533">issue 1533</a>).</li>
<li>When uploading files or images to S3 (with <code class="docutils literal"><span class="pre">FilesPipeline</span></code> or
<code class="docutils literal"><span class="pre">ImagesPipeline</span></code>), the default ACL policy is now &#8220;private&#8221; instead
of &#8220;public&#8221; <strong>Warning: backwards incompatible!</strong>.
You can use <a class="reference internal" href="index.html#std:setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE_S3_ACL</span></code></a> to change it.</li>
<li>We&#8217;ve reimplemented <code class="docutils literal"><span class="pre">canonicalize_url()</span></code> for more correct output,
especially for URLs with non-ASCII characters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1947">issue 1947</a>).
This could change link extractors output compared to previous scrapy versions.
This may also invalidate some cache entries you could still have from pre-1.1 runs.
<strong>Warning: backwards incompatible!</strong>.</li>
</ul>
</li>
</ul>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<div class="section" id="beta-python-3-support">
<span id="news-betapy3"></span><h5>Beta Python 3 Support<a class="headerlink" href="#beta-python-3-support" title="Permalink to this headline">¶</a></h5>
<p>We have been <a class="reference external" href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python 3</a>. As a result, now
you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted &gt;= 15.5 required). Some
features are still missing (and some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work.
However, we are aware of some limitations in Python 3:</p>
<ul class="simple">
<li>Scrapy does not work on Windows with Python 3</li>
<li>Sending emails is not supported</li>
<li>FTP download handler is not supported</li>
<li>Telnet console is not supported</li>
</ul>
</div>
<div class="section" id="additional-new-features-and-enhancements">
<h5>Additional New Features and Enhancements<a class="headerlink" href="#additional-new-features-and-enhancements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy now has a <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md">Code of Conduct</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1681">issue 1681</a>).</li>
<li>Command line tool now has completion for zsh (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/934">issue 934</a>).</li>
<li>Improvements to <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></code>:<ul>
<li>Support for bpython and configure preferred Python shell via
<code class="docutils literal"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1100">issue 1100</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1444">issue 1444</a>).</li>
<li>Support URLs without scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1498">issue 1498</a>)
<strong>Warning: backwards incompatible!</strong></li>
<li>Bring back support for relative file path (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).</li>
</ul>
</li>
<li>Added <a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a> setting to change default check
interval (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1282">issue 1282</a>).</li>
<li>Download handlers are now lazy-loaded on first request using their
scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1390">issue 1390</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1421">issue 1421</a>).</li>
<li>HTTPS download handlers do not force TLS 1.0 anymore; instead,
OpenSSL&#8217;s <code class="docutils literal"><span class="pre">SSLv23_method()/TLS_method()</span></code> is used allowing to try
negotiating with the remote hosts the highest TLS protocol version
it can (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1794">issue 1794</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1629">issue 1629</a>).</li>
<li><code class="docutils literal"><span class="pre">RedirectMiddleware</span></code> now skips the status codes from
<code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> on spider attribute
or in <code class="docutils literal"><span class="pre">Request</span></code>&#8216;s <code class="docutils literal"><span class="pre">meta</span></code> key (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1334">issue 1334</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1364">issue 1364</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1447">issue 1447</a>).</li>
<li>Form submission:<ul>
<li>now works with <code class="docutils literal"><span class="pre">&lt;button&gt;</span></code> elements too (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1469">issue 1469</a>).</li>
<li>an empty string is now used for submit buttons without a value
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1472">issue 1472</a>)</li>
</ul>
</li>
<li>Dict-like settings now have per-key priorities
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1135">issue 1135</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1149">issue 1149</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1586">issue 1586</a>).</li>
<li>Sending non-ASCII emails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1662">issue 1662</a>)</li>
<li><code class="docutils literal"><span class="pre">CloseSpider</span></code> and <code class="docutils literal"><span class="pre">SpiderState</span></code> extensions now get disabled if no relevant
setting is set (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1723">issue 1723</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1725">issue 1725</a>).</li>
<li>Added method <code class="docutils literal"><span class="pre">ExecutionEngine.close</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1423">issue 1423</a>).</li>
<li>Added method <code class="docutils literal"><span class="pre">CrawlerRunner.create_crawler</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1528">issue 1528</a>).</li>
<li>Scheduler priority queue can now be customized via
<code class="xref std std-setting docutils literal"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1822">issue 1822</a>).</li>
<li><code class="docutils literal"><span class="pre">.pps</span></code> links are now ignored by default in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1835">issue 1835</a>).</li>
<li>temporary data folder for FTP and S3 feed storages can be customized
using a new <a class="reference internal" href="index.html#std:setting-FEED_TEMPDIR"><code class="xref std std-setting docutils literal"><span class="pre">FEED_TEMPDIR</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1847">issue 1847</a>).</li>
<li><code class="docutils literal"><span class="pre">FilesPipeline</span></code> and <code class="docutils literal"><span class="pre">ImagesPipeline</span></code> settings are now instance attributes
instead of class attributes, enabling spider-specific behaviors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1891">issue 1891</a>).</li>
<li><code class="docutils literal"><span class="pre">JsonItemExporter</span></code> now formats opening and closing square brackets
on their own line (first and last lines of output file) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1950">issue 1950</a>).</li>
<li>If available, <code class="docutils literal"><span class="pre">botocore</span></code> is used for <code class="docutils literal"><span class="pre">S3FeedStorage</span></code>, <code class="docutils literal"><span class="pre">S3DownloadHandler</span></code>
and <code class="docutils literal"><span class="pre">S3FilesStore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1761">issue 1761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1883">issue 1883</a>).</li>
<li>Tons of documentation updates and related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1291">issue 1291</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1302">issue 1302</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1335">issue 1335</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1683">issue 1683</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1660">issue 1660</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1642">issue 1642</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1721">issue 1721</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1727">issue 1727</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1879">issue 1879</a>).</li>
<li>Other refactoring, optimizations and cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1476">issue 1476</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1481">issue 1481</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1477">issue 1477</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1315">issue 1315</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1290">issue 1290</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1750">issue 1750</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1881">issue 1881</a>).</li>
</ul>
</div>
<div class="section" id="deprecations-and-removals">
<h5>Deprecations and Removals<a class="headerlink" href="#deprecations-and-removals" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added <code class="docutils literal"><span class="pre">to_bytes</span></code> and <code class="docutils literal"><span class="pre">to_unicode</span></code>, deprecated <code class="docutils literal"><span class="pre">str_to_unicode</span></code> and
<code class="docutils literal"><span class="pre">unicode_to_str</span></code> functions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/778">issue 778</a>).</li>
<li><code class="docutils literal"><span class="pre">binary_is_text</span></code> is introduced, to replace use of <code class="docutils literal"><span class="pre">isbinarytext</span></code>
(but with inverse return value) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1851">issue 1851</a>)</li>
<li>The <code class="docutils literal"><span class="pre">optional_features</span></code> set has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1359">issue 1359</a>).</li>
<li>The <code class="docutils literal"><span class="pre">--lsprof</span></code> command line option has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1689">issue 1689</a>).
<strong>Warning: backward incompatible</strong>, but doesn&#8217;t break user code.</li>
<li>The following datatypes were deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1720">issue 1720</a>):<ul>
<li><code class="docutils literal"><span class="pre">scrapy.utils.datatypes.MultiValueDictKeyError</span></code></li>
<li><code class="docutils literal"><span class="pre">scrapy.utils.datatypes.MultiValueDict</span></code></li>
<li><code class="docutils literal"><span class="pre">scrapy.utils.datatypes.SiteNode</span></code></li>
</ul>
</li>
<li>The previously bundled <code class="docutils literal"><span class="pre">scrapy.xlib.pydispatch</span></code> library was deprecated and
replaced by <a class="reference external" href="https://pypi.python.org/pypi/PyDispatcher">pydispatcher</a>.</li>
</ul>
</div>
<div class="section" id="relocations">
<h5>Relocations<a class="headerlink" href="#relocations" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">telnetconsole</span></code> was relocated to <code class="docutils literal"><span class="pre">extensions/</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1524">issue 1524</a>).<ul>
<li>Note: telnet is not enabled on Python 3
(<a class="reference external" href="https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>)</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="bugfixes">
<h5>Bugfixes<a class="headerlink" href="#bugfixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy does not retry requests that got a <code class="docutils literal"><span class="pre">HTTP</span> <span class="pre">400</span> <span class="pre">Bad</span> <span class="pre">Request</span></code>
response anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>). <strong>Warning: backwards incompatible!</strong></li>
<li>Support empty password for http_proxy config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1274">issue 1274</a>).</li>
<li>Interpret <code class="docutils literal"><span class="pre">application/x-json</span></code> as <code class="docutils literal"><span class="pre">TextResponse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1333">issue 1333</a>).</li>
<li>Support link rel attribute with multiple values (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1201">issue 1201</a>).</li>
<li>Fixed <code class="docutils literal"><span class="pre">scrapy.http.FormRequest.from_response</span></code> when there is a <code class="docutils literal"><span class="pre">&lt;base&gt;</span></code>
tag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1564">issue 1564</a>).</li>
<li>Fixed <a class="reference internal" href="index.html#std:setting-TEMPLATES_DIR"><code class="xref std std-setting docutils literal"><span class="pre">TEMPLATES_DIR</span></code></a> handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1575">issue 1575</a>).</li>
<li>Various <code class="docutils literal"><span class="pre">FormRequest</span></code> fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1595">issue 1595</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1596">issue 1596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1597">issue 1597</a>).</li>
<li>Makes <code class="docutils literal"><span class="pre">_monkeypatches</span></code> more robust (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1634">issue 1634</a>).</li>
<li>Fixed bug on <code class="docutils literal"><span class="pre">XMLItemExporter</span></code> with non-string fields in
items (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1738">issue 1738</a>).</li>
<li>Fixed startproject command in OS X (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1635">issue 1635</a>).</li>
<li>Fixed PythonItemExporter and CSVExporter for non-string item
types (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1737">issue 1737</a>).</li>
<li>Various logging related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1294">issue 1294</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1419">issue 1419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1263">issue 1263</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1624">issue 1624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1654">issue 1654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1722">issue 1722</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1726">issue 1726</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1303">issue 1303</a>).</li>
<li>Fixed bug in <code class="docutils literal"><span class="pre">utils.template.render_templatefile()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1212">issue 1212</a>).</li>
<li>sitemaps extraction from <code class="docutils literal"><span class="pre">robots.txt</span></code> is now case-insensitive (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1902">issue 1902</a>).</li>
<li>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
to same remote host (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1912">issue 1912</a>).</li>
</ul>
</div>
</div>
<div class="section" id="id2">
<h4>1.0.6 (2016-05-04)<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>FIX: RetryMiddleware is now robust to non-standard HTTP status codes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1857">issue 1857</a>)</li>
<li>FIX: Filestorage HTTP cache was checking wrong modified time (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1875">issue 1875</a>)</li>
<li>DOC: Support for Sphinx 1.4+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1893">issue 1893</a>)</li>
<li>DOC: Consistency in selectors examples (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1869">issue 1869</a>)</li>
</ul>
</div>
<div class="section" id="id3">
<h4>1.0.5 (2016-02-04)<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/907">issue 907</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/108195e">commit 108195e</a>)</li>
<li>TST: Changed buildbot makefile to use &#8216;pytest&#8217; (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1f3d90a">commit 1f3d90a</a>)</li>
<li>DOC: Fixed typos in tutorial and media-pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/808a9ea">commit 808a9ea</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/commit/803bd87">commit 803bd87</a>)</li>
<li>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in settings docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa94121">commit aa94121</a>)</li>
</ul>
</div>
<div class="section" id="id4">
<h4>1.0.4 (2015-12-30)<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Ignoring xlib/tx folder, depending on Twisted version. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7dfa979">commit 7dfa979</a>)</li>
<li>Run on new travis-ci infra (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6e42f0b">commit 6e42f0b</a>)</li>
<li>Spelling fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/823a1cc">commit 823a1cc</a>)</li>
<li>escape nodename in xmliter regex (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/da3c155">commit da3c155</a>)</li>
<li>test xml nodename with dots (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4418fc3">commit 4418fc3</a>)</li>
<li>TST don&#8217;t use broken Pillow version in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a55078c">commit a55078c</a>)</li>
<li>disable log on version command. closes #1426 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86fc330">commit 86fc330</a>)</li>
<li>disable log on startproject command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db4c9fe">commit db4c9fe</a>)</li>
<li>Add PyPI download stats badge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/df2b944">commit df2b944</a>)</li>
<li>don&#8217;t run tests twice on Travis if a PR is made from a scrapy/scrapy branch (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a83ab41">commit a83ab41</a>)</li>
<li>Add Python 3 porting status badge to the README (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/73ac80d">commit 73ac80d</a>)</li>
<li>fixed RFPDupeFilter persistence (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97d080e">commit 97d080e</a>)</li>
<li>TST a test to show that dupefilter persistence is not working (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97f2fb3">commit 97f2fb3</a>)</li>
<li>explicit close file on <a class="reference external" href="file://">file://</a> scheme handler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d9b4850">commit d9b4850</a>)</li>
<li>Disable dupefilter in shell (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c0d0734">commit c0d0734</a>)</li>
<li>DOC: Add captions to toctrees which appear in sidebar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa239ad">commit aa239ad</a>)</li>
<li>DOC Removed pywin32 from install instructions as it&#8217;s already declared as dependency. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10eb400">commit 10eb400</a>)</li>
<li>Added installation notes about using Conda for Windows and other OSes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1c3600a">commit 1c3600a</a>)</li>
<li>Fixed minor grammar issues. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7f4ddd5">commit 7f4ddd5</a>)</li>
<li>fixed a typo in the documentation. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b71f677">commit b71f677</a>)</li>
<li>Version 1 now exists (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5456c0e">commit 5456c0e</a>)</li>
<li>fix another invalid xpath error (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a1366e">commit 0a1366e</a>)</li>
<li>fix ValueError: Invalid XPath: //div/[id=&#8221;not-exists&#8221;]/text() on selectors.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ca8d60f">commit ca8d60f</a>)</li>
<li>Typos corrections (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7067117">commit 7067117</a>)</li>
<li>fix typos in downloader-middleware.rst and exceptions.rst, middlware -&gt; middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/32f115c">commit 32f115c</a>)</li>
<li>Add note to ubuntu install section about debian compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/23fda69">commit 23fda69</a>)</li>
<li>Replace alternative OSX install workaround with virtualenv (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98b63ee">commit 98b63ee</a>)</li>
<li>Reference Homebrew&#8217;s homepage for installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1925db1">commit 1925db1</a>)</li>
<li>Add oldest supported tox version to contributing docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d10d6d">commit 5d10d6d</a>)</li>
<li>Note in install docs about pip being already included in python&gt;=2.7.9 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/85c980e">commit 85c980e</a>)</li>
<li>Add non-python dependencies to Ubuntu install section in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbd010d">commit fbd010d</a>)</li>
<li>Add OS X installation section to docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8f4cba">commit d8f4cba</a>)</li>
<li>DOC(ENH): specify path to rtd theme explicitly (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de73b1a">commit de73b1a</a>)</li>
<li>minor: scrapy.Spider docs grammar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ddcc7b">commit 1ddcc7b</a>)</li>
<li>Make common practices sample code match the comments (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1b85bcf">commit 1b85bcf</a>)</li>
<li>nextcall repetitive calls (heartbeats). (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/55f7104">commit 55f7104</a>)</li>
<li>Backport fix compatibility with Twisted 15.4.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b262411">commit b262411</a>)</li>
<li>pin pytest to 2.7.3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a6535c2">commit a6535c2</a>)</li>
<li>Merge pull request #1512 from mgedmin/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8876111">commit 8876111</a>)</li>
<li>Merge pull request #1513 from mgedmin/patch-2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d4daf8">commit 5d4daf8</a>)</li>
<li>Typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d0682">commit f8d0682</a>)</li>
<li>Fix list formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f83a93">commit 5f83a93</a>)</li>
<li>fix scrapy squeue tests after recent changes to queuelib (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3365c01">commit 3365c01</a>)</li>
<li>Merge pull request #1475 from rweindl/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d688cd">commit 2d688cd</a>)</li>
<li>Update tutorial.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbc1f25">commit fbc1f25</a>)</li>
<li>Merge pull request #1449 from rhoekman/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7d6538c">commit 7d6538c</a>)</li>
<li>Small grammatical change (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8752294">commit 8752294</a>)</li>
<li>Add openssl version to version command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c45ac">commit 13c45ac</a>)</li>
</ul>
</div>
<div class="section" id="id5">
<h4>1.0.3 (2015-08-11)<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>add service_identity to scrapy install_requires (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cbc2501">commit cbc2501</a>)</li>
<li>Workaround for travis#296 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/66af9cd">commit 66af9cd</a>)</li>
</ul>
</div>
<div class="section" id="id6">
<h4>1.0.2 (2015-08-06)<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Twisted 15.3.0 does not raises PicklingError serializing lambda functions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b04dd7d">commit b04dd7d</a>)</li>
<li>Minor method name fix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f85c7f">commit 6f85c7f</a>)</li>
<li>minor: scrapy.Spider grammar and clarity (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9c9d2e0">commit 9c9d2e0</a>)</li>
<li>Put a blurb about support channels in CONTRIBUTING (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c63882b">commit c63882b</a>)</li>
<li>Fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a9ae7b0">commit a9ae7b0</a>)</li>
<li>Fix doc reference. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7c8a4fe">commit 7c8a4fe</a>)</li>
</ul>
</div>
<div class="section" id="id7">
<h4>1.0.1 (2015-07-01)<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Unquote request path before passing to FTPClient, it already escape paths (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc00ad2">commit cc00ad2</a>)</li>
<li>include tests/ to source distribution in MANIFEST.in (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eca227e">commit eca227e</a>)</li>
<li>DOC Fix SelectJmes documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b8567bc">commit b8567bc</a>)</li>
<li>DOC Bring Ubuntu and Archlinux outside of Windows subsection (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/392233f">commit 392233f</a>)</li>
<li>DOC remove version suffix from ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5303c66">commit 5303c66</a>)</li>
<li>DOC Update release date for 1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c89fa29">commit c89fa29</a>)</li>
</ul>
</div>
<div class="section" id="id8">
<h4>1.0.0 (2015-06-19)<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>You will find a lot of new features and bugfixes in this major release.  Make
sure to check our updated <a class="reference internal" href="index.html#intro-overview"><span>overview</span></a> to get a glance of
some of the changes, along with our brushed <a class="reference internal" href="index.html#intro-tutorial"><span>tutorial</span></a>.</p>
<div class="section" id="support-for-returning-dictionaries-in-spiders">
<h5>Support for returning dictionaries in spiders<a class="headerlink" href="#support-for-returning-dictionaries-in-spiders" title="Permalink to this headline">¶</a></h5>
<p>Declaring and returning Scrapy Items is no longer necessary to collect the
scraped data from your spider, you can now return explicit dictionaries
instead.</p>
<p><em>Classic version</em></p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MyItem(scrapy.Item):
    url = scrapy.Field()

class MySpider(scrapy.Spider):
    def parse(self, response):
        return MyItem(url=response.url)
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(scrapy.Spider):
    def parse(self, response):
        return {&#39;url&#39;: response.url}
</pre></div>
</div>
</div>
<div class="section" id="per-spider-settings-gsoc-2014">
<h5>Per-spider settings (GSoC 2014)<a class="headerlink" href="#per-spider-settings-gsoc-2014" title="Permalink to this headline">¶</a></h5>
<p>Last Google Summer of Code project accomplished an important redesign of the
mechanism used for populating settings, introducing explicit priorities to
override any given setting. As an extension of that goal, we included a new
level of priority for settings that act exclusively for a single spider,
allowing them to redefine project settings.</p>
<p>Start using it by defining a <a class="reference internal" href="index.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal"><span class="pre">custom_settings</span></code></a>
class variable in your spider:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(scrapy.Spider):
    custom_settings = {
        &quot;DOWNLOAD_DELAY&quot;: 5.0,
        &quot;RETRY_ENABLED&quot;: False,
    }
</pre></div>
</div>
<p>Read more about settings population: <a class="reference internal" href="index.html#topics-settings"><span>Settings</span></a></p>
</div>
<div class="section" id="python-logging">
<h5>Python Logging<a class="headerlink" href="#python-logging" title="Permalink to this headline">¶</a></h5>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built in’s
as default logging system. We’re maintaining backward compatibility for most
of the old custom interface to call logging functions, but you’ll get
warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy import log
log.msg(&#39;MESSAGE&#39;, log.INFO)
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-none"><div class="highlight"><pre><span></span>import logging
logging.info(&#39;MESSAGE&#39;)
</pre></div>
</div>
<p>Logging with spiders remains the same, but on top of the
<a class="reference internal" href="index.html#scrapy.spiders.Spider.log" title="scrapy.spiders.Spider.log"><code class="xref py py-meth docutils literal"><span class="pre">log()</span></code></a> method you’ll have access to a custom
<a class="reference internal" href="index.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal"><span class="pre">logger</span></code></a> created for the spider to issue log
events:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>class MySpider(scrapy.Spider):
    def parse(self, response):
        self.logger.info(&#39;Response received&#39;)
</pre></div>
</div>
<p>Read more in the logging documentation: <a class="reference internal" href="index.html#topics-logging"><span>Logging</span></a></p>
</div>
<div class="section" id="crawler-api-refactoring-gsoc-2014">
<h5>Crawler API refactoring (GSoC 2014)<a class="headerlink" href="#crawler-api-refactoring-gsoc-2014" title="Permalink to this headline">¶</a></h5>
<p>Another milestone for last Google Summer of Code was a refactoring of the
internal API, seeking a simpler and easier usage. Check new core interface
in: <a class="reference internal" href="index.html#topics-api"><span>Core API</span></a></p>
<p>A common situation where you will face these changes is while running Scrapy
from scripts. Here’s a quick example of how to run a Spider manually with the
new API:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>from scrapy.crawler import CrawlerProcess

process = CrawlerProcess({
    &#39;USER_AGENT&#39;: &#39;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#39;
})
process.crawl(MySpider)
process.start()
</pre></div>
</div>
<p>Bear in mind this feature is still under development and its API may change
until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a class="reference internal" href="index.html#topics-practices"><span>Common Practices</span></a></p>
</div>
<div class="section" id="module-relocations">
<h5>Module Relocations<a class="headerlink" href="#module-relocations" title="Permalink to this headline">¶</a></h5>
<p>There’s been a large rearrangement of modules trying to improve the general
structure of Scrapy. Main changes were separating various subpackages into
new projects and dissolving both <cite>scrapy.contrib</cite> and <cite>scrapy.contrib_exp</cite>
into top level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings indicating
their new place.</p>
<div class="section" id="full-list-of-relocations">
<h6>Full list of relocations<a class="headerlink" href="#full-list-of-relocations" title="Permalink to this headline">¶</a></h6>
<p>Outsourced packages</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These extensions went through some minor changes, e.g. some setting names
were changed. Please check the documentation in each new repository to
get familiar with the new usage.</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.commands.deploy</td>
<td><a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a>
(See other alternatives here:
<a class="reference internal" href="index.html#topics-deploy"><span>Deploying Spiders</span></a>)</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.djangoitem</td>
<td><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-djangoitem">scrapy-djangoitem</a></td>
</tr>
<tr class="row-even"><td>scrapy.webservice</td>
<td><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-jsonrpc">scrapy-jsonrpc</a></td>
</tr>
</tbody>
</table>
<p><cite>scrapy.contrib_exp</cite> and <cite>scrapy.contrib</cite> dissolutions</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.contrib_exp.downloadermiddleware.decompression</td>
<td>scrapy.downloadermiddlewares.decompression</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib_exp.iterators</td>
<td>scrapy.utils.iterators</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.downloadermiddleware</td>
<td>scrapy.downloadermiddlewares</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.exporter</td>
<td>scrapy.exporters</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.linkextractors</td>
<td>scrapy.linkextractors</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.loader</td>
<td>scrapy.loader</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.loader.processor</td>
<td>scrapy.loader.processors</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.pipeline</td>
<td>scrapy.pipelines</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.spidermiddleware</td>
<td>scrapy.spidermiddlewares</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.spiders</td>
<td>scrapy.spiders</td>
</tr>
<tr class="row-even"><td><ul class="first last simple">
<li>scrapy.contrib.closespider</li>
<li>scrapy.contrib.corestats</li>
<li>scrapy.contrib.debug</li>
<li>scrapy.contrib.feedexport</li>
<li>scrapy.contrib.httpcache</li>
<li>scrapy.contrib.logstats</li>
<li>scrapy.contrib.memdebug</li>
<li>scrapy.contrib.memusage</li>
<li>scrapy.contrib.spiderstate</li>
<li>scrapy.contrib.statsmailer</li>
<li>scrapy.contrib.throttle</li>
</ul>
</td>
<td>scrapy.extensions.*</td>
</tr>
</tbody>
</table>
<p>Plural renames and Modules unification</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.command</td>
<td>scrapy.commands</td>
</tr>
<tr class="row-odd"><td>scrapy.dupefilter</td>
<td>scrapy.dupefilters</td>
</tr>
<tr class="row-even"><td>scrapy.linkextractor</td>
<td>scrapy.linkextractors</td>
</tr>
<tr class="row-odd"><td>scrapy.spider</td>
<td>scrapy.spiders</td>
</tr>
<tr class="row-even"><td>scrapy.squeue</td>
<td>scrapy.squeues</td>
</tr>
<tr class="row-odd"><td>scrapy.statscol</td>
<td>scrapy.statscollectors</td>
</tr>
<tr class="row-even"><td>scrapy.utils.decorator</td>
<td>scrapy.utils.decorators</td>
</tr>
</tbody>
</table>
<p>Class renames</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.spidermanager.SpiderManager</td>
<td>scrapy.spiderloader.SpiderLoader</td>
</tr>
</tbody>
</table>
<p>Settings renames</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>SPIDER_MANAGER_CLASS</td>
<td>SPIDER_LOADER_CLASS</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="changelog">
<h5>Changelog<a class="headerlink" href="#changelog" title="Permalink to this headline">¶</a></h5>
<p>New Features and Enhancements</p>
<ul class="simple">
<li>Python logging (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1060">issue 1060</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1235">issue 1235</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1236">issue 1236</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1240">issue 1240</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1259">issue 1259</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1278">issue 1278</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1286">issue 1286</a>)</li>
<li>FEED_EXPORT_FIELDS option (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1159">issue 1159</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1224">issue 1224</a>)</li>
<li>Dns cache size and timeout options (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1132">issue 1132</a>)</li>
<li>support namespace prefix in xmliter_lxml (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/963">issue 963</a>)</li>
<li>Reactor threadpool max size setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1123">issue 1123</a>)</li>
<li>Allow spiders to return dicts. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1081">issue 1081</a>)</li>
<li>Add Response.urljoin() helper (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1086">issue 1086</a>)</li>
<li>look in ~/.config/scrapy.cfg for user config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1098">issue 1098</a>)</li>
<li>handle TLS SNI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1101">issue 1101</a>)</li>
<li>Selectorlist extract first (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/624">issue 624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1145">issue 1145</a>)</li>
<li>Added JmesSelect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1016">issue 1016</a>)</li>
<li>add gzip compression to filesystem http cache backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1020">issue 1020</a>)</li>
<li>CSS support in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/983">issue 983</a>)</li>
<li>httpcache dont_cache meta #19 #689 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/821">issue 821</a>)</li>
<li>add signal to be sent when request is dropped by the scheduler
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/961">issue 961</a>)</li>
<li>avoid download large response (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/946">issue 946</a>)</li>
<li>Allow to specify the quotechar in CSVFeedSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/882">issue 882</a>)</li>
<li>Add referer to &#8220;Spider error processing&#8221; log message (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/795">issue 795</a>)</li>
<li>process robots.txt once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/896">issue 896</a>)</li>
<li>GSoC Per-spider settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/854">issue 854</a>)</li>
<li>Add project name validation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/817">issue 817</a>)</li>
<li>GSoC API cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/816">issue 816</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1128">issue 1128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1147">issue 1147</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1148">issue 1148</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1156">issue 1156</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1185">issue 1185</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1187">issue 1187</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1258">issue 1258</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1268">issue 1268</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1276">issue 1276</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1285">issue 1285</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1284">issue 1284</a>)</li>
<li>Be more responsive with IO operations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1074">issue 1074</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1075">issue 1075</a>)</li>
<li>Do leveldb compaction for httpcache on closing (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1297">issue 1297</a>)</li>
</ul>
<p>Deprecations and Removals</p>
<ul class="simple">
<li>Deprecate htmlparser link extractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1205">issue 1205</a>)</li>
<li>remove deprecated code from FeedExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1155">issue 1155</a>)</li>
<li>a leftover for.15 compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/925">issue 925</a>)</li>
<li>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/895">issue 895</a>)</li>
<li>Drop old engine code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/911">issue 911</a>)</li>
<li>Deprecate SgmlLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/777">issue 777</a>)</li>
</ul>
<p>Relocations</p>
<ul class="simple">
<li>Move exporters/__init__.py to exporters.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1242">issue 1242</a>)</li>
<li>Move base classes to their packages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1218">issue 1218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1233">issue 1233</a>)</li>
<li>Module relocation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1181">issue 1181</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1210">issue 1210</a>)</li>
<li>rename SpiderManager to SpiderLoader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1166">issue 1166</a>)</li>
<li>Remove djangoitem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1177">issue 1177</a>)</li>
<li>remove scrapy deploy command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1102">issue 1102</a>)</li>
<li>dissolve contrib_exp (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1134">issue 1134</a>)</li>
<li>Deleted bin folder from root, fixes #913 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/914">issue 914</a>)</li>
<li>Remove jsonrpc based webservice (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/859">issue 859</a>)</li>
<li>Move Test cases under project root dir (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/827">issue 827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/841">issue 841</a>)</li>
<li>Fix backward incompatibility for relocated paths in settings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1267">issue 1267</a>)</li>
</ul>
<p>Documentation</p>
<ul class="simple">
<li>CrawlerProcess documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1190">issue 1190</a>)</li>
<li>Favoring web scraping over screen scraping in the descriptions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1188">issue 1188</a>)</li>
<li>Some improvements for Scrapy tutorial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1180">issue 1180</a>)</li>
<li>Documenting Files Pipeline together with Images Pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1150">issue 1150</a>)</li>
<li>deployment docs tweaks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1164">issue 1164</a>)</li>
<li>Added deployment section covering scrapyd-deploy and shub (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1124">issue 1124</a>)</li>
<li>Adding more settings to project template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1073">issue 1073</a>)</li>
<li>some improvements to overview page (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1106">issue 1106</a>)</li>
<li>Updated link in docs/topics/architecture.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/647">issue 647</a>)</li>
<li>DOC reorder topics (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1022">issue 1022</a>)</li>
<li>updating list of Request.meta special keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1071">issue 1071</a>)</li>
<li>DOC document download_timeout (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/898">issue 898</a>)</li>
<li>DOC simplify extension docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/893">issue 893</a>)</li>
<li>Leaks docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/894">issue 894</a>)</li>
<li>DOC document from_crawler method for item pipelines (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/904">issue 904</a>)</li>
<li>Spider_error doesn&#8217;t support deferreds (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1292">issue 1292</a>)</li>
<li>Corrections &amp; Sphinx related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1220">issue 1220</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1219">issue 1219</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1196">issue 1196</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1172">issue 1172</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1171">issue 1171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1169">issue 1169</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1160">issue 1160</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1154">issue 1154</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1127">issue 1127</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1112">issue 1112</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1105">issue 1105</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1041">issue 1041</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1082">issue 1082</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1033">issue 1033</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/944">issue 944</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/866">issue 866</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/864">issue 864</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/796">issue 796</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1260">issue 1260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1271">issue 1271</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1293">issue 1293</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1298">issue 1298</a>)</li>
</ul>
<p>Bugfixes</p>
<ul class="simple">
<li>Item multi inheritance fix (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/353">issue 353</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1228">issue 1228</a>)</li>
<li>ItemLoader.load_item: iterate over copy of fields (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/722">issue 722</a>)</li>
<li>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1131">issue 1131</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1197">issue 1197</a>)</li>
<li>Force to read DOWNLOAD_TIMEOUT as int (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/954">issue 954</a>)</li>
<li>scrapy.utils.misc.load_object should print full traceback (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/902">issue 902</a>)</li>
<li>Fix bug for &#8221;.local&#8221; host name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/878">issue 878</a>)</li>
<li>Fix for Enabled extensions, middlewares, pipelines info not printed
anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/879">issue 879</a>)</li>
<li>fix dont_merge_cookies bad behaviour when set to false on meta
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/846">issue 846</a>)</li>
</ul>
<p>Python 3 In Progress Support</p>
<ul class="simple">
<li>disable scrapy.telnet if twisted.conch is not available (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1161">issue 1161</a>)</li>
<li>fix Python 3 syntax errors in ajaxcrawl.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1162">issue 1162</a>)</li>
<li>more python3 compatibility changes for urllib (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1121">issue 1121</a>)</li>
<li>assertItemsEqual was renamed to assertCountEqual in Python 3.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1070">issue 1070</a>)</li>
<li>Import unittest.mock if available. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1066">issue 1066</a>)</li>
<li>updated deprecated cgi.parse_qsl to use six&#8217;s parse_qsl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/909">issue 909</a>)</li>
<li>Prevent Python 3 port regressions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/830">issue 830</a>)</li>
<li>PY3: use MutableMapping for python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/810">issue 810</a>)</li>
<li>PY3: use six.BytesIO and six.moves.cStringIO (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/803">issue 803</a>)</li>
<li>PY3: fix xmlrpclib and email imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/801">issue 801</a>)</li>
<li>PY3: use six for robotparser and urlparse (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/800">issue 800</a>)</li>
<li>PY3: use six.iterkeys, six.iteritems, and tempfile (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/799">issue 799</a>)</li>
<li>PY3: fix has_key and use six.moves.configparser (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/798">issue 798</a>)</li>
<li>PY3: use six.moves.cPickle (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/797">issue 797</a>)</li>
<li>PY3 make it possible to run some tests in Python3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/776">issue 776</a>)</li>
</ul>
<p>Tests</p>
<ul class="simple">
<li>remove unnecessary lines from py3-ignores (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1243">issue 1243</a>)</li>
<li>Fix remaining warnings from pytest while collecting tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1206">issue 1206</a>)</li>
<li>Add docs build to travis (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1234">issue 1234</a>)</li>
<li>TST don&#8217;t collect tests from deprecated modules. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1165">issue 1165</a>)</li>
<li>install service_identity package in tests to prevent warnings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1168">issue 1168</a>)</li>
<li>Fix deprecated settings API in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1152">issue 1152</a>)</li>
<li>Add test for webclient with POST method and no body given (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1089">issue 1089</a>)</li>
<li>py3-ignores.txt supports comments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1044">issue 1044</a>)</li>
<li>modernize some of the asserts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/835">issue 835</a>)</li>
<li>selector.__repr__ test (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/779">issue 779</a>)</li>
</ul>
<p>Code refactoring</p>
<ul class="simple">
<li>CSVFeedSpider cleanup: use iterate_spider_output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1079">issue 1079</a>)</li>
<li>remove unnecessary check from scrapy.utils.spider.iter_spider_output
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1078">issue 1078</a>)</li>
<li>Pydispatch pep8 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/992">issue 992</a>)</li>
<li>Removed unused &#8216;load=False&#8217; parameter from walk_modules() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/871">issue 871</a>)</li>
<li>For consistency, use <cite>job_dir</cite> helper in <cite>SpiderState</cite> extension.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/805">issue 805</a>)</li>
<li>rename &#8220;sflo&#8221; local variables to less cryptic &#8220;log_observer&#8221; (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/775">issue 775</a>)</li>
</ul>
</div>
</div>
<div class="section" id="id9">
<h4>0.24.6 (2015-04-20)<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>encode invalid xpath with unicode_escape under PY2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/07cb3e5">commit 07cb3e5</a>)</li>
<li>fix IPython shell scope issue and load IPython user config (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2c8e573">commit 2c8e573</a>)</li>
<li>Fix small typo in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d694019">commit d694019</a>)</li>
<li>Fix small typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f92fa83">commit f92fa83</a>)</li>
<li>Converted sel.xpath() calls to response.xpath() in Extracting the data (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c2c6d15">commit c2c6d15</a>)</li>
</ul>
</div>
<div class="section" id="id10">
<h4>0.24.5 (2015-02-25)<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Support new _getEndpoint Agent signatures on Twisted 15.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/540b9bc">commit 540b9bc</a>)</li>
<li>DOC a couple more references are fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4c454b">commit b4c454b</a>)</li>
<li>DOC fix a reference (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3c1260">commit e3c1260</a>)</li>
<li>t.i.b.ThreadedResolver is now a new-style class (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9e13f42">commit 9e13f42</a>)</li>
<li>S3DownloadHandler: fix auth for requests with quoted paths/query params (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cdb9a0b">commit cdb9a0b</a>)</li>
<li>fixed the variable types in mailsender documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb3a848">commit bb3a848</a>)</li>
<li>Reset items_scraped instead of item_count (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/edb07a4">commit edb07a4</a>)</li>
<li>Tentative attention message about what document to read for contributions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7ee6f7a">commit 7ee6f7a</a>)</li>
<li>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/874fcdd">commit 874fcdd</a>)</li>
<li>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6b21f0">commit c6b21f0</a>)</li>
<li>Test the parse command locally instead of against an external url (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3a6628">commit c3a6628</a>)</li>
<li>Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0bf957">commit d0bf957</a>)</li>
<li>Updates documentation on dynamic item classes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eeb589a">commit eeb589a</a>)</li>
<li>Merge pull request #943 from Lazar-T/patch-3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5fdab02">commit 5fdab02</a>)</li>
<li>typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b0ae199">commit b0ae199</a>)</li>
<li>pywin32 is required by Twisted. closes #937 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5cb0cfb">commit 5cb0cfb</a>)</li>
<li>Update install.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/781286b">commit 781286b</a>)</li>
<li>Merge pull request #928 from Lazar-T/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b415d04">commit b415d04</a>)</li>
<li>comma instead of fullstop (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/627b9ba">commit 627b9ba</a>)</li>
<li>Merge pull request #885 from jsma/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de909ad">commit de909ad</a>)</li>
<li>Update request-response.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3f3263d">commit 3f3263d</a>)</li>
<li>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode present (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/49b40f0">commit 49b40f0</a>)</li>
</ul>
</div>
<div class="section" id="id11">
<h4>0.24.4 (2014-08-09)<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>pem file is used by mockserver and required by scrapy bench (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5eddc68">commit 5eddc68</a>)</li>
<li>scrapy bench needs scrapy.tests* (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d6cb999">commit d6cb999</a>)</li>
</ul>
</div>
<div class="section" id="id12">
<h4>0.24.3 (2014-08-09)<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>no need to waste travis-ci time on py3 for 0.24 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e080c1">commit 8e080c1</a>)</li>
<li>Update installation docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1d0c096">commit 1d0c096</a>)</li>
<li>There is a trove classifier for Scrapy framework! (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4c701d7">commit 4c701d7</a>)</li>
<li>update other places where w3lib version is mentioned (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d109c13">commit d109c13</a>)</li>
<li>Update w3lib requirement to 1.8.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/39d2ce5">commit 39d2ce5</a>)</li>
<li>Use w3lib.html.replace_entities() (remove_entities() is deprecated) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/180d3ad">commit 180d3ad</a>)</li>
<li>set zip_safe=False (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a51ee8b">commit a51ee8b</a>)</li>
<li>do not ship tests package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ee3b371">commit ee3b371</a>)</li>
<li>scrapy.bat is not needed anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3861cf">commit c3861cf</a>)</li>
<li>Modernize setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/362e322">commit 362e322</a>)</li>
<li>headers can not handle non-string values (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/94a5c65">commit 94a5c65</a>)</li>
<li>fix ftp test cases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274a7f">commit a274a7f</a>)</li>
<li>The sum up of travis-ci builds are taking like 50min to complete (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ae1e2cc">commit ae1e2cc</a>)</li>
<li>Update shell.rst typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e49c96a">commit e49c96a</a>)</li>
<li>removes weird indentation in the shell results (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ca489d">commit 1ca489d</a>)</li>
<li>improved explanations, clarified blog post as source, added link for XPath string functions in the spec (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/65c8f05">commit 65c8f05</a>)</li>
<li>renamed UserTimeoutError and ServerTimeouterror #583 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/037f6ab">commit 037f6ab</a>)</li>
<li>adding some xpath tips to selectors docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d103e0">commit 2d103e0</a>)</li>
<li>fix tests to account for <a class="reference external" href="https://github.com/scrapy/w3lib/pull/23">https://github.com/scrapy/w3lib/pull/23</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d366a">commit f8d366a</a>)</li>
<li>get_func_args maximum recursion fix #728 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/81344ea">commit 81344ea</a>)</li>
<li>Updated input/ouput processor example according to #560. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f7c4ea8">commit f7c4ea8</a>)</li>
<li>Fixed Python syntax in tutorial. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db59ed9">commit db59ed9</a>)</li>
<li>Add test case for tunneling proxy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f090260">commit f090260</a>)</li>
<li>Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8793af">commit d8793af</a>)</li>
<li>Extract links from XHTML documents with MIME-Type &#8220;application/xml&#8221; (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ed1f376">commit ed1f376</a>)</li>
<li>Merge pull request #793 from roysc/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/91a1106">commit 91a1106</a>)</li>
<li>Fix typo in commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/743e1e2">commit 743e1e2</a>)</li>
<li>better testcase for settings.overrides.setdefault (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e22daaf">commit e22daaf</a>)</li>
<li>Using CRLF as line marker according to http 1.1 definition (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ec430b">commit 5ec430b</a>)</li>
</ul>
</div>
<div class="section" id="id13">
<h4>0.24.2 (2014-07-08)<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e5e8133">commit e5e8133</a>)</li>
<li>there is not support for python3 yet (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3cd6146">commit 3cd6146</a>)</li>
<li>Update python compatible version set to debian packages (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa5d76b">commit fa5d76b</a>)</li>
<li>DOC fix formatting in release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6a9e20">commit c6a9e20</a>)</li>
</ul>
</div>
<div class="section" id="id14">
<h4>0.24.1 (2014-06-27)<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix deprecated CrawlerSettings and increase backwards compatibility with
.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e3f20a">commit 8e3f20a</a>)</li>
</ul>
</div>
<div class="section" id="id15">
<h4>0.24.0 (2014-06-26)<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h4>
<div class="section" id="enhancements">
<h5>Enhancements<a class="headerlink" href="#enhancements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Improve Scrapy top-level namespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/494">issue 494</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/684">issue 684</a>)</li>
<li>Add selector shortcuts to responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/554">issue 554</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/690">issue 690</a>)</li>
<li>Add new lxml based LinkExtractor to replace unmantained SgmlLinkExtractor
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/559">issue 559</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/761">issue 761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/763">issue 763</a>)</li>
<li>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>)</li>
<li>Add UTF8 encoding header to templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/688">issue 688</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/762">issue 762</a>)</li>
<li>Telnet console now binds to 127.0.0.1 by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/699">issue 699</a>)</li>
<li>Update debian/ubuntu install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/509">issue 509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/549">issue 549</a>)</li>
<li>Disable smart strings in lxml XPath evaluations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/535">issue 535</a>)</li>
<li>Restore filesystem based cache as default for http
cache middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/571">issue 571</a>)</li>
<li>Expose current crawler in Scrapy shell (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/557">issue 557</a>)</li>
<li>Improve testsuite comparing CSV and XML exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/570">issue 570</a>)</li>
<li>New <cite>offsite/filtered</cite> and <cite>offsite/domains</cite> stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>)</li>
<li>Support process_links as generator in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/555">issue 555</a>)</li>
<li>Verbose logging and new stats counters for DupeFilter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/553">issue 553</a>)</li>
<li>Add a mimetype parameter to <cite>MailSender.send()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/602">issue 602</a>)</li>
<li>Generalize file pipeline log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/622">issue 622</a>)</li>
<li>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/565">issue 565</a>)</li>
<li>Converted SEP documents to rst format (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/629">issue 629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/630">issue 630</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/638">issue 638</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/632">issue 632</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/636">issue 636</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/640">issue 640</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/635">issue 635</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/634">issue 634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/639">issue 639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/637">issue 637</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/631">issue 631</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/633">issue 633</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/641">issue 641</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/642">issue 642</a>)</li>
<li>Tests and docs for clickdata&#8217;s nr index in FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/646">issue 646</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/645">issue 645</a>)</li>
<li>Allow to disable a downloader handler just like any other component (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>)</li>
<li>Log when a request is discarded after too many redirections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>)</li>
<li>Log error responses if they are not handled by spider callbacks
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/656">issue 656</a>)</li>
<li>Add content-type check to http compression mw (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/193">issue 193</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/660">issue 660</a>)</li>
<li>Run pypy tests using latest pypi from ppa (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/674">issue 674</a>)</li>
<li>Run test suite using pytest instead of trial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/679">issue 679</a>)</li>
<li>Build docs and check for dead links in tox environment (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/687">issue 687</a>)</li>
<li>Make scrapy.version_info a tuple of integers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/681">issue 681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/692">issue 692</a>)</li>
<li>Infer exporter&#8217;s output format from filename extensions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/546">issue 546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/659">issue 659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/760">issue 760</a>)</li>
<li>Support case-insensitive domains in <cite>url_is_from_any_domain()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>)</li>
<li>Remove pep8 warnings in project and spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/698">issue 698</a>)</li>
<li>Tests and docs for <cite>request_fingerprint</cite> function (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/597">issue 597</a>)</li>
<li>Update SEP-19 for GSoC project <cite>per-spider settings</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>)</li>
<li>Set exit code to non-zero when contracts fails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/727">issue 727</a>)</li>
<li>Add a setting to control what class is instanciated as Downloader component
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/738">issue 738</a>)</li>
<li>Pass response in <cite>item_dropped</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/724">issue 724</a>)</li>
<li>Improve <cite>scrapy check</cite> contracts command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/733">issue 733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/752">issue 752</a>)</li>
<li>Document <cite>spider.closed()</cite> shortcut (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/719">issue 719</a>)</li>
<li>Document <cite>request_scheduled</cite> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/746">issue 746</a>)</li>
<li>Add a note about reporting security issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/697">issue 697</a>)</li>
<li>Add LevelDB http cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/626">issue 626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>)</li>
<li>Sort spider list output of <cite>scrapy list</cite> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>)</li>
<li>Multiple documentation enhancemens and fixes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/575">issue 575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/587">issue 587</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/590">issue 590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/596">issue 596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/610">issue 610</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/617">issue 617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/618">issue 618</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/627">issue 627</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/613">issue 613</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/643">issue 643</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/675">issue 675</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/663">issue 663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/711">issue 711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/714">issue 714</a>)</li>
</ul>
</div>
<div class="section" id="id16">
<h5>Bugfixes<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>)</li>
<li>Ignore None values in ItemLoader processors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/556">issue 556</a>)</li>
<li>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/485">issue 485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/574">issue 574</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/581">issue 581</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/584">issue 584</a>)</li>
<li>Handle errors caused by inspect.stack() failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/582">issue 582</a>)</li>
<li>Fix a reference to unexistent engine attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/593">issue 593</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/594">issue 594</a>)</li>
<li>Fix dynamic itemclass example usage of type() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/603">issue 603</a>)</li>
<li>Use lucasdemarchi/codespell to fix typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/628">issue 628</a>)</li>
<li>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>)</li>
<li>Fix XXE flaw in sitemap reader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/676">issue 676</a>)</li>
<li>Fix engine to support filtered start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/707">issue 707</a>)</li>
<li>Fix offsite middleware case on urls with no hostnames (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/745">issue 745</a>)</li>
<li>Testsuite doesn&#8217;t require PIL anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/585">issue 585</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2014-02-14">
<h4>0.22.2 (released 2014-02-14)<a class="headerlink" href="#released-2014-02-14" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix a reference to unexistent engine.slots. closes #593 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c099a">commit 13c099a</a>)</li>
<li>downloaderMW doc typo (spiderMW doc copy remnant) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit 8ae11bf</a>)</li>
<li>Correct typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1346037">commit 1346037</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-02-08">
<h4>0.22.1 (released 2014-02-08)<a class="headerlink" href="#released-2014-02-08" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>localhost666 can resolve under certain circumstances (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2ec2279">commit 2ec2279</a>)</li>
<li>test inspect.stack failure (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit cc3eda3</a>)</li>
<li>Handle cases when inspect.stack() fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit 8cb44f9</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/46d98d6">commit 46d98d6</a>)</li>
<li>Docs: 4-space indent for final spider example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13846de">commit 13846de</a>)</li>
<li>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/368a946">commit 368a946</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b566388">commit b566388</a>)</li>
<li>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c1cb418">commit c1cb418</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7e4d627">commit 7e4d627</a>)</li>
<li>Fix tests for Travis-CI build (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/76c7e20">commit 76c7e20</a>)</li>
<li>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f87b17">commit 5f87b17</a>)</li>
<li>RegexLinkExtractor: encode URL unicode value when creating Links (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0ee545">commit d0ee545</a>)</li>
<li>Updated the tutorial crawl output with latest output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8da65de">commit 8da65de</a>)</li>
<li>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/875b9ab">commit 875b9ab</a>)</li>
<li>PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f89efaf">commit f89efaf</a>)</li>
<li>Expose current crawler in the scrapy shell. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5349cec">commit 5349cec</a>)</li>
<li>Unused re import and PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/387f414">commit 387f414</a>)</li>
<li>Ignore None&#8217;s values when using the ItemLoader. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0632546">commit 0632546</a>)</li>
<li>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit cde9a8c</a>)</li>
<li>show ubuntu setup instructions as literal code (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit fb5c9c5</a>)</li>
<li>Update Ubuntu installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/70fb105">commit 70fb105</a>)</li>
<li>Merge pull request #550 from stray-leone/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit 6f70b6a</a>)</li>
<li>modify the version of scrapy ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/725900d">commit 725900d</a>)</li>
<li>fix 0.22.0 release date (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/af0219a">commit af0219a</a>)</li>
<li>fix typos in news.rst and remove (not released yet) header (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit b7f58f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2014-01-17">
<h4>0.22.0 (released 2014-01-17)<a class="headerlink" href="#released-2014-01-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id17">
<h5>Enhancements<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>[<strong>Backwards incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>)
To restore old backend set <cite>HTTPCACHE_STORAGE</cite> to <cite>scrapy.contrib.httpcache.DbmCacheStorage</cite></li>
<li>Proxy https:// urls using CONNECT method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/392">issue 392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/397">issue 397</a>)</li>
<li>Add a middleware to crawl ajax crawleable pages as defined by google (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>)</li>
<li>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/510">issue 510</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/519">issue 519</a>)</li>
<li>Selectors register EXSLT namespaces by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/472">issue 472</a>)</li>
<li>Unify item loaders similar to selectors renaming (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/461">issue 461</a>)</li>
<li>Make <cite>RFPDupeFilter</cite> class easily subclassable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/533">issue 533</a>)</li>
<li>Improve test coverage and forthcoming Python 3 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/525">issue 525</a>)</li>
<li>Promote startup info on settings and middleware to INFO level (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>)</li>
<li>Support partials in <cite>get_func_args</cite> util (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/506">issue 506</a>, issue:<cite>504</cite>)</li>
<li>Allow running indiviual tests via tox (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/503">issue 503</a>)</li>
<li>Update extensions ignored by link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/498">issue 498</a>)</li>
<li>Add middleware methods to get files/images/thumbs paths (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/490">issue 490</a>)</li>
<li>Improve offsite middleware tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/478">issue 478</a>)</li>
<li>Add a way to skip default Referer header set by RefererMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>)</li>
<li>Do not send <cite>x-gzip</cite> in default <cite>Accept-Encoding</cite> header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/469">issue 469</a>)</li>
<li>Support defining http error handling using settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/466">issue 466</a>)</li>
<li>Use modern python idioms wherever you find legacies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/497">issue 497</a>)</li>
<li>Improve and correct documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/527">issue 527</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/524">issue 524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/521">issue 521</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/517">issue 517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/512">issue 512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/505">issue 505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/502">issue 502</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/489">issue 489</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/465">issue 465</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/460">issue 460</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/425">issue 425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/536">issue 536</a>)</li>
</ul>
</div>
<div class="section" id="fixes">
<h5>Fixes<a class="headerlink" href="#fixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Update Selector class imports in CrawlSpider template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/484">issue 484</a>)</li>
<li>Fix unexistent reference to <cite>engine.slots</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/464">issue 464</a>)</li>
<li>Do not try to call <cite>body_as_unicode()</cite> on a non-TextResponse instance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/462">issue 462</a>)</li>
<li>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/523">issue 523</a>)</li>
<li>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/537">issue 537</a>)</li>
<li>Multiple fixes to memory stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/531">issue 531</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/530">issue 530</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/529">issue 529</a>)</li>
<li>Fix overriding url in <cite>FormRequest.from_response()</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/507">issue 507</a>)</li>
<li>Fix tests runner under pip 1.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/513">issue 513</a>)</li>
<li>Fix logging error when spider name is unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/479">issue 479</a>)</li>
</ul>
</div>
</div>
<div class="section" id="released-2013-12-09">
<h4>0.20.2 (released 2013-12-09)<a class="headerlink" href="#released-2013-12-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Update CrawlSpider Template with Selector changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d1457d">commit 6d1457d</a>)</li>
<li>fix method name in tutorial. closes GH-480 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4fc359">commit b4fc359</a></li>
</ul>
</div>
<div class="section" id="released-2013-11-28">
<h4>0.20.1 (released 2013-11-28)<a class="headerlink" href="#released-2013-11-28" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>include_package_data is required to build wheels from published sources (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit 5ba1ad5</a>)</li>
<li>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/419a780">commit 419a780</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-11-08">
<h4>0.20.0 (released 2013-11-08)<a class="headerlink" href="#released-2013-11-08" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id18">
<h5>Enhancements<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Selector&#8217;s API including CSS selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/395">issue 395</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/426">issue 426</a>),</li>
<li>Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)</li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> is now defined as a dict (instead of a list)</li>
<li>Sitemap spider can fetch alternate URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/360">issue 360</a>)</li>
<li><cite>Selector.remove_namespaces()</cite> now remove namespaces from element&#8217;s attributes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/416">issue 416</a>)</li>
<li>Paved the road for Python 3.3+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/435">issue 435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/436">issue 436</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/431">issue 431</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/452">issue 452</a>)</li>
<li>New item exporter using native python types with nesting support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>)</li>
<li>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit b43b5f575</a>)</li>
<li>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/327">issue 327</a>)</li>
<li>New FilesPipeline with functionality factored out from ImagesPipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/370">issue 370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/409">issue 409</a>)</li>
<li>Recommend Pillow instead of PIL for image handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/317">issue 317</a>)</li>
<li>Added debian packages for Ubuntu quantal and raring (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86230c0">commit 86230c0</a>)</li>
<li>Mock server (used for tests) can listen for HTTPS requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/410">issue 410</a>)</li>
<li>Remove multi spider support from multiple core components
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/422">issue 422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/421">issue 421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/420">issue 420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/419">issue 419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/423">issue 423</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>)</li>
<li>Travis-CI now tests Scrapy changes against development versions of <cite>w3lib</cite> and <cite>queuelib</cite> python packages.</li>
<li>Add pypy 2.1 to continuous integration tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit ecfa7431</a>)</li>
<li>Pylinted, pep8 and removed old-style exceptions from source (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/430">issue 430</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/432">issue 432</a>)</li>
<li>Use importlib for parametric imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/445">issue 445</a>)</li>
<li>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/372">issue 372</a>)</li>
<li>Bugfix crawling shutdown on SIGINT (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/450">issue 450</a>)</li>
<li>Do not submit <cite>reset</cite> type inputs in FormRequest.from_response (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b326b87">commit b326b87</a>)</li>
<li>Do not silence download errors when request errback raises an exception (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/684cfc0">commit 684cfc0</a>)</li>
</ul>
</div>
<div class="section" id="id19">
<h5>Bugfixes<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix tests under Django 1.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit b6bed44c</a>)</li>
<li>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</li>
<li>Fix inconsistencies among Twisted releases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/406">issue 406</a>)</li>
<li>Fix scrapy shell bugs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/407">issue 407</a>)</li>
<li>Fix invalid variable name in setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/429">issue 429</a>)</li>
<li>Fix tutorial references (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/387">issue 387</a>)</li>
<li>Improve request-response docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/391">issue 391</a>)</li>
<li>Improve best practices docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/399">issue 399</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/400">issue 400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/401">issue 401</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/402">issue 402</a>)</li>
<li>Improve django integration docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/404">issue 404</a>)</li>
<li>Document <cite>bindaddress</cite> request meta (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit 37c24e01d7</a>)</li>
<li>Improve <cite>Request</cite> class documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/226">issue 226</a>)</li>
</ul>
</div>
<div class="section" id="other">
<h5>Other<a class="headerlink" href="#other" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Dropped Python 2.6 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/448">issue 448</a>)</li>
<li>Add <a class="reference external" href="https://github.com/SimonSapin/cssselect">cssselect</a> python package as install dependency</li>
<li>Drop libxml2 and multi selector&#8217;s backend support, <a class="reference external" href="http://lxml.de/">lxml</a> is required from now on.</li>
<li>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</li>
<li>Running test suite now requires <cite>mock</cite> python library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/390">issue 390</a>)</li>
</ul>
</div>
<div class="section" id="thanks">
<h5>Thanks<a class="headerlink" href="#thanks" title="Permalink to this headline">¶</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>69 Daniel Graña &lt;dangra@...&gt;
37 Pablo Hoffman &lt;pablo@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
 9 Alex Cepoi &lt;alex.cepoi@...&gt;
 9 alexanderlukanin13 &lt;alexander.lukanin.13@...&gt;
 8 Rolando Espinoza La fuente &lt;darkrho@...&gt;
 8 Lukasz Biedrycki &lt;lukasz.biedrycki@...&gt;
 6 Nicolas Ramirez &lt;nramirez.uy@...&gt;
 3 Paul Tremberth &lt;paul.tremberth@...&gt;
 2 Martin Olveyra &lt;molveyra@...&gt;
 2 Stefan &lt;misc@...&gt;
 2 Rolando Espinoza &lt;darkrho@...&gt;
 2 Loren Davie &lt;loren@...&gt;
 2 irgmedeiros &lt;irgmedeiros@...&gt;
 1 Stefan Koch &lt;taikano@...&gt;
 1 Stefan &lt;cct@...&gt;
 1 scraperdragon &lt;dragon@...&gt;
 1 Kumara Tharmalingam &lt;ktharmal@...&gt;
 1 Francesco Piccinno &lt;stack.box@...&gt;
 1 Marcos Campal &lt;duendex@...&gt;
 1 Dragon Dave &lt;dragon@...&gt;
 1 Capi Etheriel &lt;barraponto@...&gt;
 1 cacovsky &lt;amarquesferraz@...&gt;
 1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="released-2013-10-10">
<h4>0.18.4 (released 2013-10-10)<a class="headerlink" href="#released-2013-10-10" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>IPython refuses to update the namespace. fix #396 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit 3d32c4f</a>)</li>
<li>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b1d8919">commit b1d8919</a>)</li>
<li>Fix start_requests laziness and early hangs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/89faf52">commit 89faf52</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-10-03">
<h4>0.18.3 (released 2013-10-03)<a class="headerlink" href="#released-2013-10-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix regression on lazy evaluation of start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/12693a5">commit 12693a5</a>)</li>
<li>forms: do not submit reset inputs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e429f63">commit e429f63</a>)</li>
<li>increase unittest timeouts to decrease travis false positive failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/912202e">commit 912202e</a>)</li>
<li>backport master fixes to json exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit cfc2d46</a>)</li>
<li>Fix permission and set umask before generating sdist tarball (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/06149e0">commit 06149e0</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-09-03">
<h4>0.18.2 (released 2013-09-03)<a class="headerlink" href="#released-2013-09-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Backport <cite>scrapy check</cite> command fixes and backward compatible multi
crawler process(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/339">issue 339</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-27">
<h4>0.18.1 (released 2013-08-27)<a class="headerlink" href="#released-2013-08-27" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>remove extra import added by cherry picked changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d20304e">commit d20304e</a>)</li>
<li>fix crawling tests under twisted pre 11.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1994f38">commit 1994f38</a>)</li>
<li>py26 can not format zero length fields {} (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/abf756f">commit abf756f</a>)</li>
<li>test PotentiaDataLoss errors on unbound responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b15470d">commit b15470d</a>)</li>
<li>Treat responses without content-length or Transfer-Encoding as good responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4bf324">commit c4bf324</a>)</li>
<li>do no include ResponseFailed if http11 handler is not enabled (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cbe684">commit 6cbe684</a>)</li>
<li>New HTTP client wraps connection losts in ResponseFailed exception. fix #373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1a20bba">commit 1a20bba</a>)</li>
<li>limit travis-ci build matrix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit 3b01bb8</a>)</li>
<li>Merge pull request #375 from peterarenot/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa766d7">commit fa766d7</a>)</li>
<li>Fixed so it refers to the correct folder (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3283809">commit 3283809</a>)</li>
<li>added quantal &amp; raring to support ubuntu releases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1411923">commit 1411923</a>)</li>
<li>fix retry middleware which didn&#8217;t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit bb35ed0</a>)</li>
<li>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de3e451">commit de3e451</a>)</li>
<li>minor updates to 0.18 release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit c45e5f1</a>)</li>
<li>fix contributters list format (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0b60031">commit 0b60031</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-08-09">
<h4>0.18.0 (released 2013-08-09)<a class="headerlink" href="#released-2013-08-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Lot of improvements to testsuite run using Tox, including a way to test on pypi</li>
<li>Handle GET parameters for AJAX crawleable urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit 3fe2a32</a>)</li>
<li>Use lxml recover option to parse sitemaps (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/347">issue 347</a>)</li>
<li>Bugfix cookie merging by hostname and not by netloc (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/352">issue 352</a>)</li>
<li>Support disabling <cite>HttpCompressionMiddleware</cite> using a flag setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/359">issue 359</a>)</li>
<li>Support xml namespaces using <cite>iternodes</cite> parser in <cite>XMLFeedSpider</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/12">issue 12</a>)</li>
<li>Support <cite>dont_cache</cite> request meta flag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/19">issue 19</a>)</li>
<li>Bugfix <cite>scrapy.utils.gz.gunzip</cite> broken by changes in python 2.7.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4dc76e">commit 4dc76e</a>)</li>
<li>Bugfix url encoding on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/24">issue 24</a>)</li>
<li>Bugfix <cite>TakeFirst</cite> processor shouldn&#8217;t discard zero (0) value (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/59">issue 59</a>)</li>
<li>Support nested items in xml exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/66">issue 66</a>)</li>
<li>Improve cookies handling performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/77">issue 77</a>)</li>
<li>Log dupe filtered requests once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/105">issue 105</a>)</li>
<li>Split redirection middleware into status and meta based middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>)</li>
<li>Use HTTP1.1 as default downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/109">issue 109</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/318">issue 318</a>)</li>
<li>Support xpath form selection on <cite>FormRequest.from_response</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>)</li>
<li>Bugfix unicode decoding error on <cite>SgmlLinkExtractor</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/199">issue 199</a>)</li>
<li>Bugfix signal dispatching on pypi interpreter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/205">issue 205</a>)</li>
<li>Improve request delay and concurrency handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/206">issue 206</a>)</li>
<li>Add RFC2616 cache policy to <cite>HttpCacheMiddleware</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/212">issue 212</a>)</li>
<li>Allow customization of messages logged by engine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/214">issue 214</a>)</li>
<li>Multiples improvements to <cite>DjangoItem</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/217">issue 217</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/218">issue 218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/221">issue 221</a>)</li>
<li>Extend Scrapy commands using setuptools entry points (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Allow spider <cite>allowed_domains</cite> value to be set/tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/261">issue 261</a>)</li>
<li>Support <cite>settings.getdict</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>)</li>
<li>Simplify internal <cite>scrapy.core.scraper</cite> slot handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/271">issue 271</a>)</li>
<li>Added <cite>Item.copy</cite> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/290">issue 290</a>)</li>
<li>Collect idle downloader slots (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/297">issue 297</a>)</li>
<li>Add <cite>ftp://</cite> scheme downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/329">issue 329</a>)</li>
<li>Added downloader benchmark webserver and spider tools <a class="reference internal" href="index.html#benchmarking"><span>Benchmarking</span></a></li>
<li>Moved persistent (on disk) queues to a separate project (<a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a>) which scrapy now depends on</li>
<li>Add scrapy commands using external libraries (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Added <code class="docutils literal"><span class="pre">--pdb</span></code> option to <code class="docutils literal"><span class="pre">scrapy</span></code> command line tool</li>
<li>Added <code class="xref py py-meth docutils literal"><span class="pre">XPathSelector.remove_namespaces()</span></code> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class="reference internal" href="index.html#topics-selectors"><span>Selectors</span></a>.</li>
<li>Several improvements to spider contracts</li>
<li>New default middleware named MetaRefreshMiddldeware that handles meta-refresh html tag redirections,</li>
<li>MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62</li>
<li>added from_crawler method to spiders</li>
<li>added system tests with mock server</li>
<li>more improvements to Mac OS compatibility (thanks Alex Cepoi)</li>
<li>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</li>
<li>support custom download slots</li>
<li>added &#8211;spider option to &#8220;shell&#8221; command.</li>
<li>log overridden settings when scrapy starts</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>130 Pablo Hoffman &lt;pablo@...&gt;
 97 Daniel Graña &lt;dangra@...&gt;
 20 Nicolás Ramírez &lt;nramirez.uy@...&gt;
 13 Mikhail Korobov &lt;kmike84@...&gt;
 12 Pedro Faustino &lt;pedrobandim@...&gt;
 11 Steven Almeroth &lt;sroth77@...&gt;
  5 Rolando Espinoza La fuente &lt;darkrho@...&gt;
  4 Michal Danilak &lt;mimino.coder@...&gt;
  4 Alex Cepoi &lt;alex.cepoi@...&gt;
  4 Alexandr N Zamaraev (aka tonal) &lt;tonal@...&gt;
  3 paul &lt;paul.tremberth@...&gt;
  3 Martin Olveyra &lt;molveyra@...&gt;
  3 Jordi Llonch &lt;llonchj@...&gt;
  3 arijitchakraborty &lt;myself.arijit@...&gt;
  2 Shane Evans &lt;shane.evans@...&gt;
  2 joehillen &lt;joehillen@...&gt;
  2 Hart &lt;HartSimha@...&gt;
  2 Dan &lt;ellisd23@...&gt;
  1 Zuhao Wan &lt;wanzuhao@...&gt;
  1 whodatninja &lt;blake@...&gt;
  1 vkrest &lt;v.krestiannykov@...&gt;
  1 tpeng &lt;pengtaoo@...&gt;
  1 Tom Mortimer-Jones &lt;tom@...&gt;
  1 Rocio Aramberri &lt;roschegel@...&gt;
  1 Pedro &lt;pedro@...&gt;
  1 notsobad &lt;wangxiaohugg@...&gt;
  1 Natan L &lt;kuyanatan.nlao@...&gt;
  1 Mark Grey &lt;mark.grey@...&gt;
  1 Luan &lt;luanpab@...&gt;
  1 Libor Nenadál &lt;libor.nenadal@...&gt;
  1 Juan M Uys &lt;opyate@...&gt;
  1 Jonas Brunsgaard &lt;jonas.brunsgaard@...&gt;
  1 Ilya Baryshev &lt;baryshev@...&gt;
  1 Hasnain Lakhani &lt;m.hasnain.lakhani@...&gt;
  1 Emanuel Schorsch &lt;emschorsch@...&gt;
  1 Chris Tilden &lt;chris.tilden@...&gt;
  1 Capi Etheriel &lt;barraponto@...&gt;
  1 cacovsky &lt;amarquesferraz@...&gt;
  1 Berend Iwema &lt;berend@...&gt;
</pre></div>
</div>
</div>
<div class="section" id="released-2013-05-30">
<h4>0.16.5 (released 2013-05-30)<a class="headerlink" href="#released-2013-05-30" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>obey request method when scrapy deploy is redirected to a new endpoint (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit 8c4fcee</a>)</li>
<li>fix inaccurate downloader middleware documentation. refs #280 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/40667cb">commit 40667cb</a>)</li>
<li>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit bd58bfa</a>)</li>
<li>Find form nodes in invalid html5 documents (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3d6945">commit e3d6945</a>)</li>
<li>Fix typo labeling attrs type bool instead of list (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274276">commit a274276</a>)</li>
</ul>
</div>
<div class="section" id="released-2013-01-23">
<h4>0.16.4 (released 2013-01-23)<a class="headerlink" href="#released-2013-01-23" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixes spelling errors in documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit 6d2b3aa</a>)</li>
<li>add doc about disabling an extension. refs #132 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c90de33">commit c90de33</a>)</li>
<li>Fixed error message formatting. log.err() doesn&#8217;t support cool formatting and when error occurred, the message was:    &#8220;ERROR: Error processing %(item)s&#8221; (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c16150c">commit c16150c</a>)</li>
<li>lint and improve images pipeline error logging (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/56b45fc">commit 56b45fc</a>)</li>
<li>fixed doc typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/243be84">commit 243be84</a>)</li>
<li>add documentation topics: Broad Crawls &amp; Common Practies (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1fbb715">commit 1fbb715</a>)</li>
<li>fix bug in scrapy parse command when spider is not specified explicitly. closes #209 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c72e682">commit c72e682</a>)</li>
<li>Update docs/topics/commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/28eac7a">commit 28eac7a</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-12-07">
<h4>0.16.3 (released 2012-12-07)<a class="headerlink" href="#released-2012-12-07" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/487b9b5">commit 487b9b5</a>)</li>
<li>add error details when image pipeline fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8232569">commit 8232569</a>)</li>
<li>improve mac os compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit 8dcf8aa</a>)</li>
<li>setup.py: use README.rst to populate long_description (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7b5310d">commit 7b5310d</a>)</li>
<li>doc: removed obsolete references to ClientForm (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit 80f9bb6</a>)</li>
<li>correct docs for default storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2aa491b">commit 2aa491b</a>)</li>
<li>doc: removed broken proxyhub link from FAQ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit bdf61c4</a>)</li>
<li>Fixed docs typo in SpiderOpenCloseLogging example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7184094">commit 7184094</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-11-09">
<h4>0.16.2 (released 2012-11-09)<a class="headerlink" href="#released-2012-11-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>scrapy contracts: python2.6 compat (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a4a9199">commit a4a9199</a>)</li>
<li>scrapy contracts verbose option (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ec41673">commit ec41673</a>)</li>
<li>proper unittest-like output for scrapy contracts (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86635e4">commit 86635e4</a>)</li>
<li>added open_in_browser to debugging doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c9b690d">commit c9b690d</a>)</li>
<li>removed reference to global scrapy stats from settings doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/dd55067">commit dd55067</a>)</li>
<li>Fix SpiderState bug in Windows platforms (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/58998f4">commit 58998f4</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-26">
<h4>0.16.1 (released 2012-10-26)<a class="headerlink" href="#released-2012-10-26" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c780fd">commit 8c780fd</a>)</li>
<li>better backwards compatibility for scrapy.conf.settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3403089">commit 3403089</a>)</li>
<li>extended documentation on how to access crawler stats from extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit c4da0b5</a>)</li>
<li>removed .hgtags (no longer needed now that scrapy uses git) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d52c188">commit d52c188</a>)</li>
<li>fix dashes under rst headers (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit fa4f7f9</a>)</li>
<li>set release date for 0.16.0 in news (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e292246">commit e292246</a>)</li>
</ul>
</div>
<div class="section" id="released-2012-10-18">
<h4>0.16.0 (released 2012-10-18)<a class="headerlink" href="#released-2012-10-18" title="Permalink to this headline">¶</a></h4>
<p>Scrapy changes:</p>
<ul class="simple">
<li>added <a class="reference internal" href="index.html#topics-contracts"><span>Spiders Contracts</span></a>, a mechanism for testing spiders in a formal/reproducible way</li>
<li>added options <code class="docutils literal"><span class="pre">-o</span></code> and <code class="docutils literal"><span class="pre">-t</span></code> to the <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command</li>
<li>documented <a class="reference internal" href="index.html#document-topics/autothrottle"><em>AutoThrottle extension</em></a> and added to extensions installed by default. You still need to enable it with <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></li>
<li>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<code class="docutils literal"><span class="pre">stats_spider_opened</span></code>, etc). Stats are much simpler now, backwards compatibility is kept on the Stats Collector API and signals.</li>
<li>added <code class="xref py py-meth docutils literal"><span class="pre">process_start_requests()</span></code> method to spider middlewares</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</li>
<li>documented <a class="reference internal" href="index.html#topics-api"><span>Core API</span></a></li>
<li><cite>lxml</cite> is now the default selectors backend instead of <cite>libxml2</cite></li>
<li>ported FormRequest.from_response() to use <a class="reference external" href="http://lxml.de/">lxml</a> instead of <a class="reference external" href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a></li>
<li>removed modules: <code class="docutils literal"><span class="pre">scrapy.xlib.BeautifulSoup</span></code> and <code class="docutils literal"><span class="pre">scrapy.xlib.ClientForm</span></code></li>
<li>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10ed28b">commit 10ed28b</a>)</li>
<li>StackTraceDump extension: also dump trackref live references (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit fe2ce93</a>)</li>
<li>nested items now fully supported in JSON and JSONLines exporters</li>
<li>added <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> Request meta key to support multiple cookie sessions per spider</li>
<li>decoupled encoding detection code to <a class="reference external" href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>, and ported Scrapy code to use that module</li>
<li>dropped support for Python 2.5. See <a class="reference external" href="https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a></li>
<li>dropped support for Twisted 2.5</li>
<li>added <a class="reference internal" href="index.html#std:setting-REFERER_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">REFERER_ENABLED</span></code></a> setting, to control referer middleware</li>
<li>changed default user agent to: <code class="docutils literal"><span class="pre">Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)</span></code></li>
<li>removed (undocumented) <code class="docutils literal"><span class="pre">HTMLImageLinkExtractor</span></code> class from <code class="docutils literal"><span class="pre">scrapy.contrib.linkextractors.image</span></code></li>
<li>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</li>
<li><code class="docutils literal"><span class="pre">USER_AGENT</span></code> spider attribute will no longer work, use <code class="docutils literal"><span class="pre">user_agent</span></code> attribute instead</li>
<li><code class="docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code> spider attribute will no longer work, use <code class="docutils literal"><span class="pre">download_timeout</span></code> attribute instead</li>
<li>removed <code class="docutils literal"><span class="pre">ENCODING_ALIASES</span></code> setting, as encoding auto-detection has been moved to the <a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> library</li>
<li>promoted <a class="reference internal" href="index.html#topics-djangoitem"><span>DjangoItem</span></a> to main contrib</li>
<li>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/164">issue 164</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit dcef7b0</a>)</li>
<li>downloader handlers (<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> setting) now receive settings as the first argument of the constructor</li>
<li>replaced memory usage acounting with (more portable) <a class="reference external" href="https://docs.python.org/2/library/resource.html">resource</a> module, removed <code class="docutils literal"><span class="pre">scrapy.utils.memory</span></code> module</li>
<li>removed signal: <code class="docutils literal"><span class="pre">scrapy.mail.mail_sent</span></code></li>
<li>removed <code class="docutils literal"><span class="pre">TRACK_REFS</span></code> setting, now <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span>trackrefs</span></a> is always enabled</li>
<li>DBM is now the default storage backend for HTTP cache middleware</li>
<li>number of log messages (per level) are now tracked through Scrapy stats (stat name: <code class="docutils literal"><span class="pre">log_count/LEVEL</span></code>)</li>
<li>number received responses are now tracked through Scrapy stats (stat name: <code class="docutils literal"><span class="pre">response_received_count</span></code>)</li>
<li>removed <code class="docutils literal"><span class="pre">scrapy.log.started</span></code> attribute</li>
</ul>
</div>
<div class="section" id="id20">
<h4>0.14.4<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>added precise to supported ubuntu distros (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7e46df">commit b7e46df</a>)</li>
<li>fixed bug in json-rpc webservice reported in <a class="reference external" href="https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported &#8216;run&#8217; command from extras/scrapy-ws.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/340fbdb">commit 340fbdb</a>)</li>
<li>meta tag attributes for content-type http equiv can be in any order. #123 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0cb68af">commit 0cb68af</a>)</li>
<li>replace &#8220;import Image&#8221; by more standard &#8220;from PIL import Image&#8221;. closes #88 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4d17048">commit 4d17048</a>)</li>
<li>return trial status as bin/runtests.sh exit value. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit b7b2e7f</a>)</li>
</ul>
</div>
<div class="section" id="id21">
<h4>0.14.3<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>forgot to include pydispatch license. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit fd85f9c</a>)</li>
<li>include egg files used by testsuite in source distribution. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c897793">commit c897793</a>)</li>
<li>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2548dcc">commit 2548dcc</a>)</li>
<li>added note to docs/topics/firebug.rst about google directory being shut down (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/668e352">commit 668e352</a>)</li>
<li>dont discard slot when empty, just save in another dict in order to recycle if needed again. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e9f607">commit 8e9f607</a>)</li>
<li>do not fail handling unicode xpaths in libxml2 backed selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b830e95">commit b830e95</a>)</li>
<li>fixed minor mistake in Request objects documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit bf3c9ee</a>)</li>
<li>fixed minor defect in link extractors documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ba14f38">commit ba14f38</a>)</li>
<li>removed some obsolete remaining code related to sqlite support in scrapy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0665175">commit 0665175</a>)</li>
</ul>
</div>
<div class="section" id="id22">
<h4>0.14.2<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>move buffer pointing to start of file before computing checksum. refs #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit 6a5bef2</a>)</li>
<li>Compute image checksum before persisting images. closes #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9817df1">commit 9817df1</a>)</li>
<li>remove leaking references in cached failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/673a120">commit 673a120</a>)</li>
<li>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/11133e9">commit 11133e9</a>)</li>
<li>fixed struct.error on http compression middleware. closes #87 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1423140">commit 1423140</a>)</li>
<li>ajax crawling wasn&#8217;t expanding for unicode urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit 0de3fb4</a>)</li>
<li>Catch start_requests iterator errors. refs #83 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/454a21d">commit 454a21d</a>)</li>
<li>Speed-up libxml2 XPathSelector (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2fbd662">commit 2fbd662</a>)</li>
<li>updated versioning doc according to recent changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a070f5">commit 0a070f5</a>)</li>
<li>scrapyd: fixed documentation link (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit 2b4e4c3</a>)</li>
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
</ul>
</div>
<div class="section" id="id23">
<h4>0.14.1<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
<li>bumped version to 0.14.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit 6cb9e1c</a>)</li>
<li>fixed reference to tutorial directory (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit 4b86bd6</a>)</li>
<li>doc: removed duplicated callback argument from Request.replace() (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit 1aeccdd</a>)</li>
<li>fixed formatting of scrapyd doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit 8bf19e6</a>)</li>
<li>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit 14a8e6e</a>)</li>
<li>added comment about why we disable ssl on boto images upload (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5223575">commit 5223575</a>)</li>
<li>SSL handshaking hangs when doing too many parallel connections to S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/63d583d">commit 63d583d</a>)</li>
<li>change tutorial to follow changes on dmoz site (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bcb3198">commit bcb3198</a>)</li>
<li>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98f3f87">commit 98f3f87</a>)</li>
<li>allow spider to set autothrottle max concurrency (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/175a4b5">commit 175a4b5</a>)</li>
</ul>
</div>
<div class="section" id="id24">
<h4>0.14<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
<div class="section" id="new-features-and-settings">
<h5>New features and settings<a class="headerlink" href="#new-features-and-settings" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">Support for <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started?csw=1">AJAX crawleable urls</a></p>
</li>
<li><p class="first">New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>)</p>
</li>
<li><p class="first">added <code class="docutils literal"><span class="pre">-o</span></code> option to <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>, a shortcut for dumping scraped items into a file (or standard output using <code class="docutils literal"><span class="pre">-</span></code>)</p>
</li>
<li><p class="first">Added support for passing custom settings to Scrapyd <code class="docutils literal"><span class="pre">schedule.json</span></code> api (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>)</p>
</li>
<li><p class="first">New <code class="docutils literal"><span class="pre">ChunkedTransferMiddleware</span></code> (enabled by default) to support <a class="reference external" href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>)</p>
</li>
<li><p class="first">Add boto 2.0 support for S3 downloader handler (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>)</p>
</li>
<li><p class="first">Added <a class="reference external" href="https://docs.python.org/2/library/marshal.html">marshal</a> to formats supported by feed exports (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>)</p>
</li>
<li><p class="first">In request errbacks, offending requests are now received in <cite>failure.request</cite> attribute (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>)</p>
</li>
<li><dl class="first docutils">
<dt>Big downloader refactoring to support per domain/ip concurrency limits (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2732">r2732</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> setting has been deprecated and replaced by:</dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS</span></code></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">check the documentation for more details</p>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Added builtin caching DNS resolver (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>)</p>
</li>
<li><p class="first">Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class="reference external" href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>)</p>
</li>
<li><p class="first">Moved spider queues to scrapyd: <cite>scrapy.spiderqueue</cite> -&gt; <cite>scrapyd.spiderqueue</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>)</p>
</li>
<li><p class="first">Moved sqlite utils to scrapyd: <cite>scrapy.utils.sqlite</cite> -&gt; <cite>scrapyd.sqlite</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>)</p>
</li>
<li><p class="first">Real support for returning iterators on <cite>start_requests()</cite> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_ENABLED</span></code></a> setting to quickly enable/disable the redirect middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_ENABLED</span></code></a> setting to quickly enable/disable the retry middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>)</p>
</li>
<li><p class="first">Added <code class="docutils literal"><span class="pre">CloseSpider</span></code> exception to manually close spiders (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>)</p>
</li>
<li><p class="first">Improved encoding detection by adding support for HTML5 meta charset declaration (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>)</p>
</li>
<li><p class="first">Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>)</p>
</li>
<li><p class="first">Added <code class="docutils literal"><span class="pre">SitemapSpider</span></code> (see documentation in Spiders page) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>)</p>
</li>
<li><p class="first">Added <code class="docutils literal"><span class="pre">LogStats</span></code> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>)</p>
</li>
<li><p class="first">Make handling of gzipped responses more robust (#319, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <cite>IOError</cite>.</p>
</li>
<li><p class="first">Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>)</p>
</li>
<li><p class="first">Added new command to edit spiders: <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">edit</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>) and <cite>-e</cite> flag to <cite>genspider</cite> command that uses it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>)</p>
</li>
<li><p class="first">Changed default representation of items to pretty-printed dicts. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:signal-spider_error"><code class="xref std std-signal docutils literal"><span class="pre">spider_error</span></code></a> signal (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>)</p>
</li>
<li><p class="first">Added <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_ENABLED</span></code></a> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>)</p>
</li>
<li><p class="first">Stats are now dumped to Scrapy log (default value of <a class="reference internal" href="index.html#std:setting-STATS_DUMP"><code class="xref std std-setting docutils literal"><span class="pre">STATS_DUMP</span></code></a> setting has been changed to <cite>True</cite>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</p>
</li>
<li><p class="first">Added support for dynamically adjusting download delay and maximum concurrent requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>)</p>
</li>
<li><p class="first">Added new DBM HTTP cache storage backend (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>)</p>
</li>
<li><p class="first">Added <code class="docutils literal"><span class="pre">listjobs.json</span></code> API to Scrapyd (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>)</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">CsvItemExporter</span></code>: added <code class="docutils literal"><span class="pre">join_multivalued</span></code> parameter (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>)</p>
</li>
<li><p class="first">Added namespace support to <code class="docutils literal"><span class="pre">xmliter_lxml</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>)</p>
</li>
<li><p class="first">Improved cookies middleware by making <cite>COOKIES_DEBUG</cite> nicer and documenting it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>)</p>
</li>
<li><p class="first">Several improvements to Scrapyd and Link extractors</p>
</li>
</ul>
</div>
<div class="section" id="code-rearranged-and-removed">
<h5>Code rearranged and removed<a class="headerlink" href="#code-rearranged-and-removed" title="Permalink to this headline">¶</a></h5>
<ul>
<li><dl class="first docutils">
<dt>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2630">r2630</a>)</dt>
<dd><ul class="first last simple">
<li>original item_scraped signal was removed</li>
<li>original item_passed signal was renamed to item_scraped</li>
<li>old log lines <code class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> were removed</li>
<li>old log lines <code class="docutils literal"><span class="pre">Passed</span> <span class="pre">Item...</span></code> were renamed to <code class="docutils literal"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> lines and downgraded to <code class="docutils literal"><span class="pre">DEBUG</span></code> level</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> (several functions from <code class="docutils literal"><span class="pre">scrapy.utils.{http,markup,multipart,response,url}</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2584">r2584</a>)</li>
<li><a class="reference external" href="https://github.com/scrapy/scrapely">scrapely</a> (was <code class="docutils literal"><span class="pre">scrapy.contrib.ibl</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2586">r2586</a>)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed unused function: <cite>scrapy.utils.request.request_info()</cite> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>)</p>
</li>
<li><p class="first">Removed googledir project from <cite>examples/googledir</cite>. There&#8217;s now a new example project called <cite>dirbot</cite> available on github: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p>
</li>
<li><p class="first">Removed support for default field values in Scrapy items (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>)</p>
</li>
<li><p class="first">Removed experimental crawlspider v2 (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>)</p>
</li>
<li><p class="first">Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (<cite>DUPEFILTER_CLASS</cite> setting) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>)</p>
</li>
<li><p class="first">Removed support for passing urls to <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></code> command (use <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">parse</span></code> instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed deprecated Execution Queue (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p>
</li>
<li><p class="first">Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>)</p>
</li>
<li><p class="first">removed <code class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></code> setting (use scrapyd maxproc instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>)</p>
</li>
<li><p class="first">Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>)</p>
</li>
<li><p class="first">Renamed setting <code class="docutils literal"><span class="pre">CLOSESPIDER_ITEMPASSED</span></code> to <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>). Backwards compatibility kept.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id25">
<h4>0.12<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features-and-improvements">
<h5>New features and improvements<a class="headerlink" href="#new-features-and-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Passed item is now sent in the <code class="docutils literal"><span class="pre">item</span></code> argument of the <code class="xref std std-signal docutils literal"><span class="pre">item_passed</span></code> (#273)</li>
<li>Added verbose option to <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span></code> command, useful for bug reports (#298)</li>
<li>HTTP cache now stored by default in the project data dir (#279)</li>
<li>Added project data storage directory (#276, #277)</li>
<li>Documented file structure of Scrapy projects (see command-line tool doc)</li>
<li>New lxml backend for XPath selectors (#147)</li>
<li>Per-spider settings (#245)</li>
<li>Support exit codes to signal errors in Scrapy commands (#248)</li>
<li>Added <code class="docutils literal"><span class="pre">-c</span></code> argument to <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span></code> command</li>
<li>Made <code class="docutils literal"><span class="pre">libxml2</span></code> optional (#260)</li>
<li>New <code class="docutils literal"><span class="pre">deploy</span></code> command (#261)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a> setting (#253)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a> setting (#254)</li>
</ul>
</div>
<div class="section" id="scrapyd-changes">
<h5>Scrapyd changes<a class="headerlink" href="#scrapyd-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapyd now uses one process per spider</li>
<li>It stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)</li>
<li>A minimal web ui was added, available at <a class="reference external" href="http://localhost:6800">http://localhost:6800</a> by default</li>
<li>There is now a <cite>scrapy server</cite> command to start a Scrapyd server of the current project</li>
</ul>
</div>
<div class="section" id="changes-to-settings">
<h5>Changes to settings<a class="headerlink" href="#changes-to-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>added <cite>HTTPCACHE_ENABLED</cite> setting (False by default) to enable HTTP cache middleware</li>
<li>changed <cite>HTTPCACHE_EXPIRATION_SECS</cite> semantics: now zero means &#8220;never expire&#8221;.</li>
</ul>
</div>
<div class="section" id="deprecated-obsoleted-functionality">
<h5>Deprecated/obsoleted functionality<a class="headerlink" href="#deprecated-obsoleted-functionality" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Deprecated <code class="docutils literal"><span class="pre">runserver</span></code> command in favor of <code class="docutils literal"><span class="pre">server</span></code> command which starts a Scrapyd server. See also: Scrapyd changes</li>
<li>Deprecated <code class="docutils literal"><span class="pre">queue</span></code> command in favor of using Scrapyd <code class="docutils literal"><span class="pre">schedule.json</span></code> API. See also: Scrapyd changes</li>
<li>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</li>
</ul>
</div>
</div>
<div class="section" id="id26">
<h4>0.10<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id27">
<h5>New features and improvements<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Scrapy service called <code class="docutils literal"><span class="pre">scrapyd</span></code> for deploying Scrapy crawlers in production (#218) (documentation available)</li>
<li>Simplified Images pipeline usage which doesn&#8217;t require subclassing your own images pipeline now (#217)</li>
<li>Scrapy shell now shows the Scrapy log by default (#206)</li>
<li>Refactored execution queue in a common base code and pluggable backends called &#8220;spider queues&#8221; (#220)</li>
<li>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</li>
<li>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</li>
<li>Feed exporters with pluggable backends (#197) (documentation available)</li>
<li>Deferred signals (#193)</li>
<li>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</li>
<li>Support for overriding default request headers per spider (#181)</li>
<li>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</li>
<li>Splitted Debian package into two packages - the library and the service (#187)</li>
<li>Scrapy log refactoring (#188)</li>
<li>New extension for keeping persistent spider contexts among different runs (#203)</li>
<li>Added <cite>dont_redirect</cite> request.meta key for avoiding redirects (#233)</li>
<li>Added <cite>dont_retry</cite> request.meta key for avoiding retries (#234)</li>
</ul>
</div>
<div class="section" id="command-line-tool-changes">
<h5>Command-line tool changes<a class="headerlink" href="#command-line-tool-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New <cite>scrapy</cite> command which replaces the old <cite>scrapy-ctl.py</cite> (#199)
- there is only one global <cite>scrapy</cite> command now, instead of one <cite>scrapy-ctl.py</cite> per project
- Added <cite>scrapy.bat</cite> script for running more conveniently from Windows</li>
<li>Added bash completion to command-line tool (#210)</li>
<li>Renamed command <cite>start</cite> to <cite>runserver</cite> (#209)</li>
</ul>
</div>
<div class="section" id="api-changes">
<h5>API changes<a class="headerlink" href="#api-changes" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first"><code class="docutils literal"><span class="pre">url</span></code> and <code class="docutils literal"><span class="pre">body</span></code> attributes of Request objects are now read-only (#230)</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">Request.copy()</span></code> and <code class="docutils literal"><span class="pre">Request.replace()</span></code> now also copies their <code class="docutils literal"><span class="pre">callback</span></code> and <code class="docutils literal"><span class="pre">errback</span></code> attributes (#231)</p>
</li>
<li><p class="first">Removed <code class="docutils literal"><span class="pre">UrlFilterMiddleware</span></code> from <code class="docutils literal"><span class="pre">scrapy.contrib</span></code> (already disabled by default)</p>
</li>
<li><p class="first">Offsite middelware doesn&#8217;t filter out any request coming from a spider that doesn&#8217;t have a allowed_domains attribute (#225)</p>
</li>
<li><p class="first">Removed Spider Manager <code class="docutils literal"><span class="pre">load()</span></code> method. Now spiders are loaded in the constructor itself.</p>
</li>
<li><dl class="first docutils">
<dt>Changes to Scrapy Manager (now called &#8220;Crawler&#8221;):</dt>
<dd><ul class="first last simple">
<li><code class="docutils literal"><span class="pre">scrapy.core.manager.ScrapyManager</span></code> class renamed to <code class="docutils literal"><span class="pre">scrapy.crawler.Crawler</span></code></li>
<li><code class="docutils literal"><span class="pre">scrapy.core.manager.scrapymanager</span></code> singleton moved to <code class="docutils literal"><span class="pre">scrapy.project.crawler</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Moved module: <code class="docutils literal"><span class="pre">scrapy.contrib.spidermanager</span></code> to <code class="docutils literal"><span class="pre">scrapy.spidermanager</span></code></p>
</li>
<li><p class="first">Spider Manager singleton moved from <code class="docutils literal"><span class="pre">scrapy.spider.spiders</span></code> to the <code class="docutils literal"><span class="pre">spiders`</span> <span class="pre">attribute</span> <span class="pre">of</span> <span class="pre">``scrapy.project.crawler</span></code> singleton.</p>
</li>
<li><dl class="first docutils">
<dt>moved Stats Collector classes: (#204)</dt>
<dd><ul class="first last simple">
<li><code class="docutils literal"><span class="pre">scrapy.stats.collector.StatsCollector</span></code> to <code class="docutils literal"><span class="pre">scrapy.statscol.StatsCollector</span></code></li>
<li><code class="docutils literal"><span class="pre">scrapy.stats.collector.SimpledbStatsCollector</span></code> to <code class="docutils literal"><span class="pre">scrapy.contrib.statscol.SimpledbStatsCollector</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">default per-command settings are now specified in the <code class="docutils literal"><span class="pre">default_settings</span></code> attribute of command object class (#201)</p>
</li>
<li><dl class="first docutils">
<dt>changed arguments of Item pipeline <code class="docutils literal"><span class="pre">process_item()</span></code> method from <code class="docutils literal"><span class="pre">(spider,</span> <span class="pre">item)</span></code> to <code class="docutils literal"><span class="pre">(item,</span> <span class="pre">spider)</span></code></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <code class="docutils literal"><span class="pre">scrapy.core.signals</span></code> module to <code class="docutils literal"><span class="pre">scrapy.signals</span></code></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <code class="docutils literal"><span class="pre">scrapy.core.exceptions</span></code> module to <code class="docutils literal"><span class="pre">scrapy.exceptions</span></code></dt>
<dd><ul class="first last simple">
<li>backwards compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">added <code class="docutils literal"><span class="pre">handles_request()</span></code> class method to <code class="docutils literal"><span class="pre">BaseSpider</span></code></p>
</li>
<li><p class="first">dropped <code class="docutils literal"><span class="pre">scrapy.log.exc()</span></code> function (use <code class="docutils literal"><span class="pre">scrapy.log.err()</span></code> instead)</p>
</li>
<li><p class="first">dropped <code class="docutils literal"><span class="pre">component</span></code> argument of <code class="docutils literal"><span class="pre">scrapy.log.msg()</span></code> function</p>
</li>
<li><p class="first">dropped <code class="docutils literal"><span class="pre">scrapy.log.log_level</span></code> attribute</p>
</li>
<li><p class="first">Added <code class="docutils literal"><span class="pre">from_settings()</span></code> class methods to Spider Manager, and Item Pipeline Manager</p>
</li>
</ul>
</div>
<div class="section" id="id28">
<h5>Changes to settings<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added <code class="docutils literal"><span class="pre">HTTPCACHE_IGNORE_SCHEMES</span></code> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</li>
<li>Added <code class="docutils literal"><span class="pre">SPIDER_QUEUE_CLASS</span></code> setting which defines the spider queue to use (#220)</li>
<li>Added <code class="docutils literal"><span class="pre">KEEP_ALIVE</span></code> setting (#220)</li>
<li>Removed <code class="docutils literal"><span class="pre">SERVICE_QUEUE</span></code> setting (#220)</li>
<li>Removed <code class="docutils literal"><span class="pre">COMMANDS_SETTINGS_MODULE</span></code> setting (#201)</li>
<li>Renamed <code class="docutils literal"><span class="pre">REQUEST_HANDLERS</span></code> to <code class="docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code> and make download handlers classes (instead of functions)</li>
</ul>
</div>
</div>
<div class="section" id="id29">
<h4>0.9<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id30">
<h5>New features and improvements<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added SMTP-AUTH support to scrapy.mail</li>
<li>New settings added: <code class="docutils literal"><span class="pre">MAIL_USER</span></code>, <code class="docutils literal"><span class="pre">MAIL_PASS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a> | #149)</li>
<li>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a> | #167)</li>
<li>Support for running Scrapy as a service, for production systems (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a> | #168)</li>
<li>Added wrapper induction library (documentation only available in source code for now). (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>)</li>
<li>Simplified and improved response encoding support (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>)</li>
<li>Added <code class="docutils literal"><span class="pre">LOG_ENCODING</span></code> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>, documentation available)</li>
<li>Added <code class="docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code> setting (enabled by default) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>, doc available)</li>
<li><code class="docutils literal"><span class="pre">MailSender</span></code> is no longer IO-blocking (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a> | #146)</li>
<li>Linkextractors and new Crawlspider now handle relative base tag urls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a> | #148)</li>
<li>Several improvements to Item Loaders and processors (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>)</li>
<li>Added support for adding variables to telnet console (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a> | #165)</li>
<li>Support for requests without callbacks (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a> | #166)</li>
</ul>
</div>
<div class="section" id="id31">
<h5>API changes<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Change <code class="docutils literal"><span class="pre">Spider.domain_name</span></code> to <code class="docutils literal"><span class="pre">Spider.name</span></code> (SEP-012, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>)</li>
<li><code class="docutils literal"><span class="pre">Response.encoding</span></code> is now the detected encoding (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>)</li>
<li><code class="docutils literal"><span class="pre">HttpErrorMiddleware</span></code> now returns None or raises an exception (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a> | #157)</li>
<li><code class="docutils literal"><span class="pre">scrapy.command</span></code> modules relocation (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>)</li>
<li>Added <code class="docutils literal"><span class="pre">ExecutionQueue</span></code> for feeding spiders to scrape (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>)</li>
<li>Removed <code class="docutils literal"><span class="pre">ExecutionEngine</span></code> singleton (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Ported <code class="docutils literal"><span class="pre">S3ImagesStore</span></code> (images pipeline) to use boto and threads (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>)</li>
<li>Moved module: <code class="docutils literal"><span class="pre">scrapy.management.telnet</span></code> to <code class="docutils literal"><span class="pre">scrapy.telnet</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>)</li>
</ul>
</div>
<div class="section" id="changes-to-default-settings">
<h5>Changes to default settings<a class="headerlink" href="#changes-to-default-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Changed default <code class="docutils literal"><span class="pre">SCHEDULER_ORDER</span></code> to <code class="docutils literal"><span class="pre">DFO</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>)</li>
</ul>
</div>
</div>
<div class="section" id="id32">
<h4>0.8<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features">
<h5>New features<a class="headerlink" href="#new-features" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added DEFAULT_RESPONSE_ENCODING setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>)</li>
<li>Added <code class="docutils literal"><span class="pre">dont_click</span></code> argument to <code class="docutils literal"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>)</li>
<li>Added <code class="docutils literal"><span class="pre">clickdata</span></code> argument to <code class="docutils literal"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>)</li>
<li>Added support for HTTP proxies (<code class="docutils literal"><span class="pre">HttpProxyMiddleware</span></code>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>)</li>
<li>Offsite spider middleware now logs messages when filtering out requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>)</li>
</ul>
</div>
<div class="section" id="backwards-incompatible-changes">
<h5>Backwards-incompatible changes<a class="headerlink" href="#backwards-incompatible-changes" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first">Changed <code class="docutils literal"><span class="pre">scrapy.utils.response.get_meta_refresh()</span></code> signature (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>)</p>
</li>
<li><p class="first">Removed deprecated <code class="docutils literal"><span class="pre">scrapy.item.ScrapedItem</span></code> class - use <code class="docutils literal"><span class="pre">scrapy.item.Item</span> <span class="pre">instead</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>)</p>
</li>
<li><p class="first">Removed deprecated <code class="docutils literal"><span class="pre">scrapy.xpath</span></code> module - use <code class="docutils literal"><span class="pre">scrapy.selector</span></code> instead. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>)</p>
</li>
<li><p class="first">Removed deprecated <code class="docutils literal"><span class="pre">core.signals.domain_open</span></code> signal - use <code class="docutils literal"><span class="pre">core.signals.domain_opened</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</p>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">log.msg()</span></code> now receives a <code class="docutils literal"><span class="pre">spider</span></code> argument (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</dt>
<dd><ul class="first last simple">
<li>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <code class="docutils literal"><span class="pre">spider</span></code> argument and pass spider references. If you really want to pass a string, use the <code class="docutils literal"><span class="pre">component</span></code> argument instead.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Changed core signals <code class="docutils literal"><span class="pre">domain_opened</span></code>, <code class="docutils literal"><span class="pre">domain_closed</span></code>, <code class="docutils literal"><span class="pre">domain_idle</span></code></p>
</li>
<li><dl class="first docutils">
<dt>Changed Item pipeline to use spiders instead of domains</dt>
<dd><ul class="first last simple">
<li>The <code class="docutils literal"><span class="pre">domain</span></code> argument of  <code class="docutils literal"><span class="pre">process_item()</span></code> item pipeline method was changed to  <code class="docutils literal"><span class="pre">spider</span></code>, the new signature is: <code class="docutils literal"><span class="pre">process_item(spider,</span> <span class="pre">item)</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1827">r1827</a> | #105)</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal"><span class="pre">domain</span></code>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Changed Stats API to use spiders instead of domains (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1849">r1849</a> | #113)</dt>
<dd><ul class="first last simple">
<li><code class="docutils literal"><span class="pre">StatsCollector</span></code> was changed to receive spider references (instead of domains) in its methods (<code class="docutils literal"><span class="pre">set_value</span></code>, <code class="docutils literal"><span class="pre">inc_value</span></code>, etc).</li>
<li>added <code class="docutils literal"><span class="pre">StatsCollector.iter_spider_stats()</span></code> method</li>
<li>removed <code class="docutils literal"><span class="pre">StatsCollector.list_domains()</span></code> method</li>
<li>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here&#8217;s a summary of the changes:</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal"><span class="pre">domain</span></code>. <code class="docutils literal"><span class="pre">spider_stats</span></code> contains exactly the same data as <code class="docutils literal"><span class="pre">domain_stats</span></code>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal"><span class="pre">CloseDomain</span></code> extension moved to <code class="docutils literal"><span class="pre">scrapy.contrib.closespider.CloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1833">r1833</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>Its settings were also renamed:</dt>
<dd><ul class="first last simple">
<li><code class="docutils literal"><span class="pre">CLOSEDOMAIN_TIMEOUT</span></code> to <code class="docutils literal"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></li>
<li><code class="docutils literal"><span class="pre">CLOSEDOMAIN_ITEMCOUNT</span></code> to <code class="docutils literal"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Removed deprecated <code class="docutils literal"><span class="pre">SCRAPYSETTINGS_MODULE</span></code> environment variable - use <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>)</p>
</li>
<li><p class="first">Renamed setting: <code class="docutils literal"><span class="pre">REQUESTS_PER_DOMAIN</span></code> to <code class="docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>)</p>
</li>
<li><p class="first">Renamed setting: <code class="docutils literal"><span class="pre">CONCURRENT_DOMAINS</span></code> to <code class="docutils literal"><span class="pre">CONCURRENT_SPIDERS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>)</p>
</li>
<li><p class="first">Refactored HTTP Cache middleware</p>
</li>
<li><p class="first">HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a> )</p>
</li>
<li><p class="first">Renamed exception: <code class="docutils literal"><span class="pre">DontCloseDomain</span></code> to <code class="docutils literal"><span class="pre">DontCloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a> | #120)</p>
</li>
<li><p class="first">Renamed extension: <code class="docutils literal"><span class="pre">DelayedCloseDomain</span></code> to <code class="docutils literal"><span class="pre">SpiderCloseDelay</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a> | #121)</p>
</li>
<li><p class="first">Removed obsolete <code class="docutils literal"><span class="pre">scrapy.utils.markup.remove_escape_chars</span></code> function - use <code class="docutils literal"><span class="pre">scrapy.utils.markup.replace_escape_chars</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>)</p>
</li>
</ul>
</div>
</div>
<div class="section" id="id33">
<h4>0.7<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h4>
<p>First release of Scrapy.</p>
</div>
</div>
<span id="document-contributing"></span><div class="section" id="contributing-to-scrapy">
<span id="topics-contributing"></span><h3>Contributing to Scrapy<a class="headerlink" href="#contributing-to-scrapy" title="Permalink to this headline">¶</a></h3>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Double check you are reading the most recent version of this document at
<a class="reference external" href="http://doc.scrapy.org/en/master/contributing.html">http://doc.scrapy.org/en/master/contributing.html</a></p>
</div>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul class="simple">
<li>Blog about Scrapy. Tell the world how you&#8217;re using Scrapy. This will help
newcomers with more examples and the Scrapy project to increase its
visibility.</li>
<li>Report bugs and request features in the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>, trying to follow
the guidelines detailed in <a class="reference internal" href="#reporting-bugs">Reporting bugs</a> below.</li>
<li>Submit patches for new functionality and/or bug fixes. Please read
<a class="reference internal" href="#writing-patches">Writing patches</a> and <a class="reference internal" href="#submitting-patches">Submitting patches</a> below for details on how to
write and submit a patch.</li>
<li>Join the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a> mailing list and share your ideas on how to
improve Scrapy. We&#8217;re always open to suggestions.</li>
</ul>
<div class="section" id="reporting-bugs">
<h4>Reporting bugs<a class="headerlink" href="#reporting-bugs" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security&#37;&#52;&#48;googlegroups&#46;com">scrapy-security<span>&#64;</span>googlegroups<span>&#46;</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
</div>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when reporting a new bug.</p>
<ul class="simple">
<li>check the <a class="reference internal" href="index.html#faq"><span>FAQ</span></a> first to see if your issue is addressed in a
well-known question</li>
<li>check the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">open issues</a> to see if it has already been reported. If it has,
don&#8217;t dismiss the report but check the ticket history and comments, you may
find additional useful information to contribute.</li>
<li>search the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a> list to see if it has been discussed there, or
if you&#8217;re not sure if what you&#8217;re seeing is a bug. You can also ask in the
<cite>#scrapy</cite> IRC channel.</li>
<li>write <strong>complete, reproducible, specific bug reports</strong>. The smaller the test
case, the better. Remember that other developers won&#8217;t have your project to
reproduce the bug, so please include all relevant files required to reproduce
it. See for example StackOverflow&#8217;s guide on creating a
<a class="reference external" href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable example</a> exhibiting the issue.</li>
<li>include the output of <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">-v</span></code> so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.</li>
</ul>
</div>
<div class="section" id="writing-patches">
<h4>Writing patches<a class="headerlink" href="#writing-patches" title="Permalink to this headline">¶</a></h4>
<p>The better written a patch is, the higher chance that it&#8217;ll get accepted and
the sooner that will be merged.</p>
<p>Well-written patches should:</p>
<ul class="simple">
<li>contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you&#8217;re doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.</li>
<li>pass all unit-tests. See <a class="reference internal" href="#running-tests">Running tests</a> below.</li>
<li>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a class="reference internal" href="#writing-tests">Writing tests</a> below.</li>
<li>if you&#8217;re adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See <a class="reference internal" href="#documentation-policies">Documentation policies</a>
below.</li>
</ul>
</div>
<div class="section" id="submitting-patches">
<h4>Submitting patches<a class="headerlink" href="#submitting-patches" title="Permalink to this headline">¶</a></h4>
<p>The best way to submit a patch is to issue a <a class="reference external" href="https://help.github.com/send-pull-requests/">pull request</a> on GitHub,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it&#8217;s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it&#8217;s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on GitHub. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can send an email to
<a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a> to discuss your idea first.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="https://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits than functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
</div>
<div class="section" id="coding-style">
<h4>Coding style<a class="headerlink" href="#coding-style" title="Permalink to this headline">¶</a></h4>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<ul class="simple">
<li>Unless otherwise specified, follow <span class="target" id="index-1"></span><a class="pep reference external" href="https://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a>.</li>
<li>It&#8217;s OK to use lines longer than 80 chars if it improves the code
readability.</li>
<li>Don&#8217;t put your name in the code you contribute. Our policy is to keep
the contributor&#8217;s name in the <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/AUTHORS">AUTHORS</a> file distributed with Scrapy.</li>
</ul>
</div>
<div class="section" id="scrapy-contrib">
<h4>Scrapy Contrib<a class="headerlink" href="#scrapy-contrib" title="Permalink to this headline">¶</a></h4>
<p>Scrapy contrib shares a similar rationale as Django contrib, which is explained
in <a class="reference external" href="https://jacobian.org/writing/what-is-django-contrib/">this post</a>. If you
are working on a new functionality, please follow that rationale to decide
whether it should be a Scrapy contrib. If unsure, you can ask in
<a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a>.</p>
</div>
<div class="section" id="documentation-policies">
<h4>Documentation policies<a class="headerlink" href="#documentation-policies" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>Don&#8217;t</strong> use docstrings for documenting classes, or methods which are
already documented in the official (sphinx) documentation. For example, the
<code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></code> method should be documented in the sphinx
documentation, not its docstring.</li>
<li><strong>Do</strong> use docstrings for documenting functions not present in the official
(sphinx) documentation, such as functions from <code class="docutils literal"><span class="pre">scrapy.utils</span></code> package and
its sub-modules.</li>
</ul>
</div>
<div class="section" id="tests">
<h4>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h4>
<p>Tests are implemented using the <a class="reference external" href="https://twistedmatrix.com/documents/current/core/development/policy/test-standard.html">Twisted unit-testing framework</a>, running
tests requires <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a>.</p>
<div class="section" id="running-tests">
<h5>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this headline">¶</a></h5>
<p>Make sure you have a recent enough <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> installation:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">tox</span> <span class="pre">--version</span></code></div></blockquote>
<p>If your version is older than 1.7.0, please update it first:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span> <span class="pre">tox</span></code></div></blockquote>
<p>To run all tests go to the root directory of Scrapy source code and run:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">tox</span></code></div></blockquote>
<p>To run a specific test (say <code class="docutils literal"><span class="pre">tests/test_loader.py</span></code>) use:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">tox</span> <span class="pre">--</span> <span class="pre">tests/test_loader.py</span></code></div></blockquote>
<p>To see coverage report install <a class="reference external" href="https://pypi.python.org/pypi/coverage">coverage</a> (<code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">coverage</span></code>) and run:</p>
<blockquote>
<div><code class="docutils literal"><span class="pre">coverage</span> <span class="pre">report</span></code></div></blockquote>
<p>see output of <code class="docutils literal"><span class="pre">coverage</span> <span class="pre">--help</span></code> for more options like html or xml report.</p>
</div>
<div class="section" id="writing-tests">
<h5>Writing tests<a class="headerlink" href="#writing-tests" title="Permalink to this headline">¶</a></h5>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a> directory.
Their module name typically resembles the full path of the module they&#8217;re
testing. For example, the item loaders code is in:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>scrapy.loader
</pre></div>
</div>
<p>And their unit-tests are in:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>tests/test_loader.py
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-versioning"></span><div class="section" id="versioning-and-api-stability">
<span id="versioning"></span><h3>Versioning and API Stability<a class="headerlink" href="#versioning-and-api-stability" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>Versioning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Scrapy uses the <a class="reference external" href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.</p>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul class="simple">
<li><em>A</em> is the major version. This will rarely change and will signify very
large changes.</li>
<li><em>B</em> is the release number. This will include many changes including features
and things that possibly break backwards compatibility. Even Bs will be
stable branches, and odd Bs will be development.</li>
<li><em>C</em> is the bugfix release number.</li>
</ul>
<p>For example:</p>
<ul class="simple">
<li><em>0.14.1</em> is the first bugfix release of the <em>0.14</em> series (safe to use in
production)</li>
</ul>
</div>
<div class="section" id="api-stability">
<h4>API Stability<a class="headerlink" href="#api-stability" title="Permalink to this headline">¶</a></h4>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash (<code class="docutils literal"><span class="pre">_</span></code>) are private and
should never be relied as stable.</p>
<p>Also, keep in mind that stable doesn&#8217;t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-news"><em>Release notes</em></a></dt>
<dd>See what has changed in recent Scrapy versions.</dd>
<dt><a class="reference internal" href="index.html#document-contributing"><em>Contributing to Scrapy</em></a></dt>
<dd>Learn how to contribute to the Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-versioning"><em>Versioning and API Stability</em></a></dt>
<dd>Understand Scrapy versioning and API stability.</dd>
</dl>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>441df4c8</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: master
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
          <dd><a href="/en/1.1/">1.1</a></dd>
        
          <dd><a href="/en/1.0/">1.0</a></dd>
        
          <dd><a href="/en/0.24/">0.24</a></dd>
        
          <dd><a href="/en/0.22/">0.22</a></dd>
        
          <dd><a href="/en/0.20/">0.20</a></dd>
        
          <dd><a href="/en/0.18/">0.18</a></dd>
        
          <dd><a href="/en/0.16/">0.16</a></dd>
        
          <dd><a href="/en/0.14/">0.14</a></dd>
        
          <dd><a href="/en/0.12/">0.12</a></dd>
        
          <dd><a href="/en/0.10.3/">0.10.3</a></dd>
        
          <dd><a href="/en/0.9/">0.9</a></dd>
        
          <dd><a href="/en/master/">master</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/pdf/master/">pdf</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/htmlzip/master/">htmlzip</a></dd>
        
          <dd><a href="//readthedocs.org/projects/scrapy/downloads/epub/master/">epub</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.2.0dev2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>